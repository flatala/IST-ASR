transformers==4.48.2
torch==2.6.0
pillow==11.1.0
scipy==1.15.2
accelerate==1.3.0
backoff==2.2.1
peft==0.13.2
torchvision==0.21.0
soundfile==0.13.1
ninja
packaging
ipykernel
ipywidgets
flash_attn==2.7.4.post1
# flash_attn==2.7.4.post1
# module load cuda/12 && module load nvhpc && pip install flash-attn --no-build-isolation
