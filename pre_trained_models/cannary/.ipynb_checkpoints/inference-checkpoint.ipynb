{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from nemo.collections.asr.models import EncDecMultiTaskModel\n",
    "from nemo.collections.asr.metrics.wer import word_error_rate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-27 21:09:21 mixins:200] _setup_tokenizer: detected an aggregate tokenizer\n",
      "[NeMo I 2025-03-27 21:09:21 mixins:339] Tokenizer SentencePieceTokenizer initialized with 1152 tokens\n",
      "[NeMo I 2025-03-27 21:09:21 mixins:339] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-03-27 21:09:21 mixins:339] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-03-27 21:09:21 mixins:339] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-03-27 21:09:21 mixins:339] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-03-27 21:09:21 aggregate_tokenizer:73] Aggregate vocab size: 5248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-03-27 21:09:21 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_lhotse: true\n",
      "    input_cfg: null\n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    prompt_format: canary2\n",
      "    max_tps: 25\n",
      "    max_duration: 40.0\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    use_bucketing: true\n",
      "    bucket_duration_bins:\n",
      "    - - 3.56\n",
      "      - 30\n",
      "    - - 3.56\n",
      "      - 77\n",
      "    - - 4.608\n",
      "      - 38\n",
      "    - - 4.608\n",
      "      - 88\n",
      "    - - 5.48\n",
      "      - 49\n",
      "    - - 5.48\n",
      "      - 106\n",
      "    - - 6.05\n",
      "      - 52\n",
      "    - - 6.05\n",
      "      - 109\n",
      "    - - 6.85\n",
      "      - 54\n",
      "    - - 6.85\n",
      "      - 124\n",
      "    - - 7.914\n",
      "      - 59\n",
      "    - - 7.914\n",
      "      - 137\n",
      "    - - 8.52\n",
      "      - 67\n",
      "    - - 8.52\n",
      "      - 158\n",
      "    - - 9.51\n",
      "      - 67\n",
      "    - - 9.51\n",
      "      - 153\n",
      "    - - 10.29\n",
      "      - 78\n",
      "    - - 10.29\n",
      "      - 202\n",
      "    - - 10.89\n",
      "      - 88\n",
      "    - - 10.89\n",
      "      - 197\n",
      "    - - 11.45\n",
      "      - 92\n",
      "    - - 11.45\n",
      "      - 217\n",
      "    - - 12.1\n",
      "      - 98\n",
      "    - - 12.1\n",
      "      - 222\n",
      "    - - 12.69\n",
      "      - 123\n",
      "    - - 12.69\n",
      "      - 257\n",
      "    - - 13.09\n",
      "      - 133\n",
      "    - - 13.09\n",
      "      - 245\n",
      "    - - 13.57\n",
      "      - 104\n",
      "    - - 13.57\n",
      "      - 225\n",
      "    - - 14.02\n",
      "      - 108\n",
      "    - - 14.02\n",
      "      - 238\n",
      "    - - 14.53\n",
      "      - 117\n",
      "    - - 14.53\n",
      "      - 235\n",
      "    - - 15.08\n",
      "      - 135\n",
      "    - - 15.08\n",
      "      - 253\n",
      "    - - 15.53\n",
      "      - 130\n",
      "    - - 15.53\n",
      "      - 273\n",
      "    - - 15.945\n",
      "      - 123\n",
      "    - - 15.945\n",
      "      - 270\n",
      "    - - 16.4\n",
      "      - 126\n",
      "    - - 16.4\n",
      "      - 284\n",
      "    - - 16.9\n",
      "      - 126\n",
      "    - - 16.9\n",
      "      - 294\n",
      "    - - 17.44\n",
      "      - 127\n",
      "    - - 17.44\n",
      "      - 296\n",
      "    - - 17.96\n",
      "      - 131\n",
      "    - - 17.96\n",
      "      - 290\n",
      "    - - 18.41\n",
      "      - 133\n",
      "    - - 18.41\n",
      "      - 308\n",
      "    - - 18.91\n",
      "      - 137\n",
      "    - - 18.91\n",
      "      - 320\n",
      "    - - 19.42\n",
      "      - 135\n",
      "    - - 19.42\n",
      "      - 313\n",
      "    - - 20.0\n",
      "      - 138\n",
      "    - - 20.0\n",
      "      - 311\n",
      "    - - 34.547\n",
      "      - 195\n",
      "    - - 34.547\n",
      "      - 434\n",
      "    - - 40.0\n",
      "      - 263\n",
      "    - - 40.0\n",
      "      - 381\n",
      "    bucket_batch_size:\n",
      "    - 1044\n",
      "    - 835\n",
      "    - 786\n",
      "    - 662\n",
      "    - 662\n",
      "    - 558\n",
      "    - 594\n",
      "    - 501\n",
      "    - 534\n",
      "    - 450\n",
      "    - 465\n",
      "    - 391\n",
      "    - 417\n",
      "    - 342\n",
      "    - 378\n",
      "    - 318\n",
      "    - 340\n",
      "    - 271\n",
      "    - 322\n",
      "    - 258\n",
      "    - 295\n",
      "    - 248\n",
      "    - 284\n",
      "    - 234\n",
      "    - 250\n",
      "    - 210\n",
      "    - 250\n",
      "    - 210\n",
      "    - 250\n",
      "    - 210\n",
      "    - 241\n",
      "    - 203\n",
      "    - 232\n",
      "    - 196\n",
      "    - 216\n",
      "    - 187\n",
      "    - 207\n",
      "    - 174\n",
      "    - 207\n",
      "    - 174\n",
      "    - 199\n",
      "    - 163\n",
      "    - 194\n",
      "    - 163\n",
      "    - 188\n",
      "    - 158\n",
      "    - 186\n",
      "    - 156\n",
      "    - 172\n",
      "    - 145\n",
      "    - 173\n",
      "    - 142\n",
      "    - 167\n",
      "    - 140\n",
      "    - 160\n",
      "    - 135\n",
      "    - 86\n",
      "    - 74\n",
      "    - 70\n",
      "    - 64\n",
      "    bucket_buffer_size: 40000\n",
      "    shuffle_buffer_size: 10000\n",
      "    concurrent_bucketing: false\n",
      "    augmentor: null\n",
      "    \n",
      "[NeMo W 2025-03-27 21:09:21 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_lhotse: true\n",
      "    prompt_format: canary2\n",
      "    manifest_filepath: /data/ASR/librispeech/test_other.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    tarred_audio_filepaths: null\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-27 21:09:21 features:305] PADDING: 0\n",
      "[NeMo I 2025-03-27 21:09:24 save_restore_connector:275] Model EncDecMultiTaskModel was successfully restored from /scratch/flatala/pre_trained_models/cannary/canary-180m-flash/canary-180m-flash.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16538 [00:00<?, ?it/s][NeMo W 2025-03-27 21:09:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.13it/s]\u001b[A\n",
      "  0%|          | 1/16538 [00:00<1:01:47,  4.46it/s][NeMo W 2025-03-27 21:09:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.21it/s]\n",
      "  0%|          | 2/16538 [00:00<46:51,  5.88it/s]  [NeMo W 2025-03-27 21:09:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.86it/s]\n",
      "  0%|          | 3/16538 [00:00<42:37,  6.46it/s][NeMo W 2025-03-27 21:09:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.28it/s]\n",
      "  0%|          | 4/16538 [00:00<40:09,  6.86it/s][NeMo W 2025-03-27 21:09:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.93it/s]\n",
      "  0%|          | 5/16538 [00:00<39:13,  7.02it/s][NeMo W 2025-03-27 21:09:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.66it/s]\n",
      "  0%|          | 6/16538 [00:00<38:56,  7.08it/s][NeMo W 2025-03-27 21:09:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.04it/s]\u001b[A\n",
      "  0%|          | 7/16538 [00:01<46:18,  5.95it/s][NeMo W 2025-03-27 21:09:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.73it/s]\n",
      "  0%|          | 8/16538 [00:01<44:01,  6.26it/s][NeMo W 2025-03-27 21:09:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.65it/s]\u001b[A\n",
      "  0%|          | 9/16538 [00:01<50:57,  5.41it/s][NeMo W 2025-03-27 21:09:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.01it/s]\n",
      "  0%|          | 10/16538 [00:01<47:33,  5.79it/s][NeMo W 2025-03-27 21:09:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.72it/s]\u001b[A\n",
      "  0%|          | 11/16538 [00:01<53:06,  5.19it/s][NeMo W 2025-03-27 21:09:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.98it/s]\u001b[A\n",
      "  0%|          | 12/16538 [00:02<50:59,  5.40it/s][NeMo W 2025-03-27 21:09:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.68it/s]\n",
      "  0%|          | 13/16538 [00:02<47:16,  5.83it/s][NeMo W 2025-03-27 21:09:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.79it/s]\n",
      "  0%|          | 14/16538 [00:02<45:00,  6.12it/s][NeMo W 2025-03-27 21:09:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.87it/s]\n",
      "  0%|          | 15/16538 [00:02<43:06,  6.39it/s][NeMo W 2025-03-27 21:09:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.31it/s]\u001b[A\n",
      "  0%|          | 16/16538 [00:02<43:28,  6.33it/s][NeMo W 2025-03-27 21:09:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.78it/s]\n",
      "  0%|          | 17/16538 [00:02<41:58,  6.56it/s][NeMo W 2025-03-27 21:09:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.22it/s]\n",
      "  0%|          | 18/16538 [00:02<41:27,  6.64it/s][NeMo W 2025-03-27 21:09:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.02it/s]\n",
      "  0%|          | 19/16538 [00:03<40:15,  6.84it/s][NeMo W 2025-03-27 21:09:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.67it/s]\n",
      "  0%|          | 20/16538 [00:03<39:45,  6.92it/s][NeMo W 2025-03-27 21:09:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.39it/s]\n",
      "  0%|          | 21/16538 [00:03<39:54,  6.90it/s][NeMo W 2025-03-27 21:09:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.91it/s]\u001b[A\n",
      "  0%|          | 22/16538 [00:03<40:25,  6.81it/s][NeMo W 2025-03-27 21:09:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.97it/s]\u001b[A\n",
      "  0%|          | 23/16538 [00:03<46:59,  5.86it/s][NeMo W 2025-03-27 21:09:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.44it/s]\n",
      "  0%|          | 24/16538 [00:03<45:44,  6.02it/s][NeMo W 2025-03-27 21:09:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.97it/s]\u001b[A\n",
      "  0%|          | 25/16538 [00:04<48:03,  5.73it/s][NeMo W 2025-03-27 21:09:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.97it/s]\u001b[A\n",
      "  0%|          | 26/16538 [00:04<49:37,  5.54it/s][NeMo W 2025-03-27 21:09:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.61it/s]\u001b[A\n",
      "  0%|          | 27/16538 [00:04<47:49,  5.75it/s][NeMo W 2025-03-27 21:09:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.35it/s]\n",
      "  0%|          | 28/16538 [00:04<45:30,  6.05it/s][NeMo W 2025-03-27 21:09:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.17it/s]\n",
      "  0%|          | 29/16538 [00:04<44:08,  6.23it/s][NeMo W 2025-03-27 21:09:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.27it/s]\n",
      "  0%|          | 30/16538 [00:04<43:03,  6.39it/s][NeMo W 2025-03-27 21:09:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.54it/s]\n",
      "  0%|          | 31/16538 [00:05<41:58,  6.55it/s][NeMo W 2025-03-27 21:09:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.58it/s]\n",
      "  0%|          | 32/16538 [00:05<40:59,  6.71it/s][NeMo W 2025-03-27 21:09:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  1.74it/s]\u001b[A\n",
      "  0%|          | 33/16538 [00:05<1:18:00,  3.53it/s][NeMo W 2025-03-27 21:09:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.60it/s]\n",
      "  0%|          | 34/16538 [00:05<1:06:18,  4.15it/s][NeMo W 2025-03-27 21:09:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.05it/s]\n",
      "  0%|          | 35/16538 [00:06<57:28,  4.79it/s]  [NeMo W 2025-03-27 21:09:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.66it/s]\n",
      "  0%|          | 36/16538 [00:06<51:46,  5.31it/s][NeMo W 2025-03-27 21:09:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.07it/s]\u001b[A\n",
      "  0%|          | 37/16538 [00:06<50:03,  5.49it/s][NeMo W 2025-03-27 21:09:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.28it/s]\n",
      "  0%|          | 38/16538 [00:06<45:57,  5.98it/s][NeMo W 2025-03-27 21:09:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.95it/s]\u001b[A\n",
      "  0%|          | 39/16538 [00:06<48:00,  5.73it/s][NeMo W 2025-03-27 21:09:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.87it/s]\u001b[A\n",
      "  0%|          | 40/16538 [00:06<45:59,  5.98it/s][NeMo W 2025-03-27 21:09:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.09it/s]\n",
      "  0%|          | 41/16538 [00:06<43:18,  6.35it/s][NeMo W 2025-03-27 21:09:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.05it/s]\n",
      "  0%|          | 42/16538 [00:07<41:22,  6.64it/s][NeMo W 2025-03-27 21:09:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.70it/s]\u001b[A\n",
      "  0%|          | 43/16538 [00:07<43:15,  6.35it/s][NeMo W 2025-03-27 21:09:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.85it/s]\u001b[A\n",
      "  0%|          | 44/16538 [00:07<46:35,  5.90it/s][NeMo W 2025-03-27 21:09:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.73it/s]\n",
      "  0%|          | 45/16538 [00:07<44:03,  6.24it/s][NeMo W 2025-03-27 21:09:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.26it/s]\n",
      "  0%|          | 46/16538 [00:07<41:39,  6.60it/s][NeMo W 2025-03-27 21:09:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.88it/s]\n",
      "  0%|          | 47/16538 [00:07<40:23,  6.80it/s][NeMo W 2025-03-27 21:09:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.91it/s]\u001b[A\n",
      "  0%|          | 48/16538 [00:08<40:42,  6.75it/s][NeMo W 2025-03-27 21:09:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.68it/s]\u001b[A\n",
      "  0%|          | 49/16538 [00:08<44:59,  6.11it/s][NeMo W 2025-03-27 21:09:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.23it/s]\n",
      "  0%|          | 50/16538 [00:08<42:23,  6.48it/s][NeMo W 2025-03-27 21:09:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.76it/s]\u001b[A\n",
      "  0%|          | 51/16538 [00:08<43:48,  6.27it/s][NeMo W 2025-03-27 21:09:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.71it/s]\n",
      "  0%|          | 52/16538 [00:08<42:05,  6.53it/s][NeMo W 2025-03-27 21:09:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.02it/s]\n",
      "  0%|          | 53/16538 [00:08<40:31,  6.78it/s][NeMo W 2025-03-27 21:09:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.97it/s]\u001b[A\n",
      "  0%|          | 54/16538 [00:08<42:07,  6.52it/s][NeMo W 2025-03-27 21:09:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.28it/s]\n",
      "  0%|          | 55/16538 [00:09<40:23,  6.80it/s][NeMo W 2025-03-27 21:09:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.25it/s]\n",
      "  0%|          | 56/16538 [00:09<39:06,  7.03it/s][NeMo W 2025-03-27 21:09:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.75it/s]\n",
      "  0%|          | 57/16538 [00:09<38:42,  7.10it/s][NeMo W 2025-03-27 21:09:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.72it/s]\n",
      "  0%|          | 58/16538 [00:09<38:29,  7.14it/s][NeMo W 2025-03-27 21:09:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.25it/s]\n",
      "  0%|          | 59/16538 [00:09<37:46,  7.27it/s][NeMo W 2025-03-27 21:09:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.13it/s]\u001b[A\n",
      "  0%|          | 60/16538 [00:09<44:25,  6.18it/s][NeMo W 2025-03-27 21:09:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.52it/s]\u001b[A\n",
      "  0%|          | 61/16538 [00:10<45:40,  6.01it/s][NeMo W 2025-03-27 21:09:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.81it/s]\u001b[A\n",
      "  0%|          | 62/16538 [00:10<46:03,  5.96it/s][NeMo W 2025-03-27 21:09:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.08it/s]\n",
      "  0%|          | 63/16538 [00:10<43:14,  6.35it/s][NeMo W 2025-03-27 21:09:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.68it/s]\n",
      "  0%|          | 64/16538 [00:10<41:44,  6.58it/s][NeMo W 2025-03-27 21:09:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.83it/s]\n",
      "  0%|          | 65/16538 [00:10<40:28,  6.78it/s][NeMo W 2025-03-27 21:09:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.34it/s]\n",
      "  0%|          | 66/16538 [00:10<39:07,  7.02it/s][NeMo W 2025-03-27 21:09:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.05it/s]\n",
      "  0%|          | 67/16538 [00:10<38:26,  7.14it/s][NeMo W 2025-03-27 21:09:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.64it/s]\u001b[A\n",
      "  0%|          | 68/16538 [00:11<41:12,  6.66it/s][NeMo W 2025-03-27 21:09:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.01it/s]\n",
      "  0%|          | 69/16538 [00:11<40:00,  6.86it/s][NeMo W 2025-03-27 21:09:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.82it/s]\u001b[A\n",
      "  0%|          | 70/16538 [00:11<44:06,  6.22it/s][NeMo W 2025-03-27 21:09:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.76it/s]\n",
      "  0%|          | 71/16538 [00:11<42:19,  6.49it/s][NeMo W 2025-03-27 21:09:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.53it/s]\u001b[A\n",
      "  0%|          | 72/16538 [00:11<44:07,  6.22it/s][NeMo W 2025-03-27 21:09:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.70it/s]\u001b[A\n",
      "  0%|          | 73/16538 [00:11<43:37,  6.29it/s][NeMo W 2025-03-27 21:09:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.18it/s]\u001b[A\n",
      "  0%|          | 74/16538 [00:12<43:55,  6.25it/s][NeMo W 2025-03-27 21:09:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.63it/s]\n",
      "  0%|          | 75/16538 [00:12<42:12,  6.50it/s][NeMo W 2025-03-27 21:09:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.36it/s]\u001b[A\n",
      "  0%|          | 76/16538 [00:12<46:56,  5.85it/s][NeMo W 2025-03-27 21:09:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.97it/s]\n",
      "  0%|          | 77/16538 [00:12<44:10,  6.21it/s][NeMo W 2025-03-27 21:09:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.86it/s]\n",
      "  0%|          | 78/16538 [00:12<42:07,  6.51it/s][NeMo W 2025-03-27 21:09:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.05it/s]\n",
      "  0%|          | 79/16538 [00:12<40:33,  6.76it/s][NeMo W 2025-03-27 21:09:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  2.94it/s]\u001b[A\n",
      "  0%|          | 80/16538 [00:13<58:17,  4.71it/s][NeMo W 2025-03-27 21:09:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.88it/s]\u001b[A\n",
      "  0%|          | 81/16538 [00:13<53:10,  5.16it/s][NeMo W 2025-03-27 21:09:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.24it/s]\u001b[A\n",
      "  0%|          | 82/16538 [00:13<52:25,  5.23it/s][NeMo W 2025-03-27 21:09:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.82it/s]\u001b[A\n",
      "  1%|          | 83/16538 [00:13<49:21,  5.56it/s][NeMo W 2025-03-27 21:09:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.27it/s]\n",
      "  1%|          | 84/16538 [00:13<45:19,  6.05it/s][NeMo W 2025-03-27 21:09:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.04it/s]\u001b[A\n",
      "  1%|          | 85/16538 [00:13<47:14,  5.80it/s][NeMo W 2025-03-27 21:09:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.05it/s]\n",
      "  1%|          | 86/16538 [00:14<44:08,  6.21it/s][NeMo W 2025-03-27 21:09:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.46it/s]\n",
      "  1%|          | 87/16538 [00:14<42:33,  6.44it/s][NeMo W 2025-03-27 21:09:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.46it/s]\u001b[A\n",
      "  1%|          | 88/16538 [00:14<46:55,  5.84it/s][NeMo W 2025-03-27 21:09:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.76it/s]\u001b[A\n",
      "  1%|          | 89/16538 [00:14<45:23,  6.04it/s][NeMo W 2025-03-27 21:09:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.54it/s]\n",
      "  1%|          | 90/16538 [00:14<43:17,  6.33it/s][NeMo W 2025-03-27 21:09:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.05it/s]\u001b[A\n",
      "  1%|          | 91/16538 [00:14<48:33,  5.65it/s][NeMo W 2025-03-27 21:09:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.21it/s]\n",
      "  1%|          | 92/16538 [00:15<46:01,  5.96it/s][NeMo W 2025-03-27 21:09:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.29it/s]\n",
      "  1%|          | 93/16538 [00:15<44:06,  6.21it/s][NeMo W 2025-03-27 21:09:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.11it/s]\n",
      "  1%|          | 94/16538 [00:15<43:13,  6.34it/s][NeMo W 2025-03-27 21:09:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.97it/s]\n",
      "  1%|          | 95/16538 [00:15<41:23,  6.62it/s][NeMo W 2025-03-27 21:09:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.12it/s]\u001b[A\n",
      "  1%|          | 96/16538 [00:15<42:33,  6.44it/s][NeMo W 2025-03-27 21:09:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  2.41it/s]\u001b[A\n",
      "  1%|          | 97/16538 [00:16<1:05:55,  4.16it/s][NeMo W 2025-03-27 21:09:40 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.90it/s]\u001b[A\n",
      "  1%|          | 98/16538 [00:16<58:58,  4.65it/s]  [NeMo W 2025-03-27 21:09:40 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.10it/s]\n",
      "  1%|          | 99/16538 [00:16<53:29,  5.12it/s][NeMo W 2025-03-27 21:09:40 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  3.86it/s]\u001b[A\n",
      "  1%|          | 100/16538 [00:16<1:00:45,  4.51it/s][NeMo W 2025-03-27 21:09:40 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.87it/s]\n",
      "  1%|          | 101/16538 [00:16<53:44,  5.10it/s]  [NeMo W 2025-03-27 21:09:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.16it/s]\u001b[A\n",
      "  1%|          | 102/16538 [00:17<52:51,  5.18it/s][NeMo W 2025-03-27 21:09:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.75it/s]\n",
      "  1%|          | 103/16538 [00:17<48:19,  5.67it/s][NeMo W 2025-03-27 21:09:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.20it/s]\u001b[A\n",
      "  1%|          | 104/16538 [00:17<47:09,  5.81it/s][NeMo W 2025-03-27 21:09:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.12it/s]\u001b[A\n",
      "  1%|          | 105/16538 [00:17<51:03,  5.36it/s][NeMo W 2025-03-27 21:09:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.26it/s]\u001b[A\n",
      "  1%|          | 106/16538 [00:17<53:19,  5.14it/s][NeMo W 2025-03-27 21:09:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  2.27it/s]\u001b[A\n",
      "  1%|          | 107/16538 [00:18<1:15:31,  3.63it/s][NeMo W 2025-03-27 21:09:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.35it/s]\u001b[A\n",
      "  1%|          | 108/16538 [00:18<1:10:30,  3.88it/s][NeMo W 2025-03-27 21:09:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.25it/s]\u001b[A\n",
      "  1%|          | 109/16538 [00:18<1:02:41,  4.37it/s][NeMo W 2025-03-27 21:09:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.30it/s]\u001b[A\n",
      "  1%|          | 110/16538 [00:18<1:01:20,  4.46it/s][NeMo W 2025-03-27 21:09:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.49it/s]\u001b[A\n",
      "  1%|          | 111/16538 [00:18<55:50,  4.90it/s]  [NeMo W 2025-03-27 21:09:43 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.13it/s]\u001b[A\n",
      "  1%|          | 112/16538 [00:19<54:23,  5.03it/s][NeMo W 2025-03-27 21:09:43 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.06it/s]\n",
      "  1%|          | 113/16538 [00:19<49:08,  5.57it/s][NeMo W 2025-03-27 21:09:43 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.78it/s]\u001b[A\n",
      "  1%|          | 114/16538 [00:19<47:13,  5.80it/s][NeMo W 2025-03-27 21:09:43 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.28it/s]\n",
      "  1%|          | 115/16538 [00:19<45:02,  6.08it/s][NeMo W 2025-03-27 21:09:43 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.41it/s]\u001b[A\n",
      "  1%|          | 116/16538 [00:19<46:20,  5.91it/s][NeMo W 2025-03-27 21:09:43 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.29it/s]\u001b[A\n",
      "  1%|          | 117/16538 [00:19<45:37,  6.00it/s][NeMo W 2025-03-27 21:09:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.23it/s]\u001b[A\n",
      "  1%|          | 118/16538 [00:20<47:01,  5.82it/s][NeMo W 2025-03-27 21:09:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.22it/s]\n",
      "  1%|          | 119/16538 [00:20<44:57,  6.09it/s][NeMo W 2025-03-27 21:09:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.00it/s]\u001b[A\n",
      "  1%|          | 120/16538 [00:20<47:03,  5.81it/s][NeMo W 2025-03-27 21:09:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.54it/s]\n",
      "  1%|          | 121/16538 [00:20<44:35,  6.14it/s][NeMo W 2025-03-27 21:09:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.93it/s]\n",
      "  1%|          | 122/16538 [00:20<42:26,  6.45it/s][NeMo W 2025-03-27 21:09:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.76it/s]\u001b[A\n",
      "  1%|          | 123/16538 [00:20<42:15,  6.47it/s][NeMo W 2025-03-27 21:09:45 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.38it/s]\n",
      "  1%|          | 124/16538 [00:21<41:18,  6.62it/s][NeMo W 2025-03-27 21:09:45 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.92it/s]\n",
      "  1%|          | 125/16538 [00:21<40:02,  6.83it/s][NeMo W 2025-03-27 21:09:45 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.79it/s]\u001b[A\n",
      "  1%|          | 126/16538 [00:21<40:36,  6.74it/s][NeMo W 2025-03-27 21:09:45 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.49it/s]\n",
      "  1%|          | 127/16538 [00:21<40:02,  6.83it/s][NeMo W 2025-03-27 21:09:45 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.87it/s]\u001b[A\n",
      "  1%|          | 128/16538 [00:21<40:26,  6.76it/s][NeMo W 2025-03-27 21:09:45 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.89it/s]\n",
      "  1%|          | 129/16538 [00:21<39:27,  6.93it/s][NeMo W 2025-03-27 21:09:45 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.74it/s]\u001b[A\n",
      "  1%|          | 130/16538 [00:21<43:54,  6.23it/s][NeMo W 2025-03-27 21:09:46 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.73it/s]\u001b[A\n",
      "  1%|          | 131/16538 [00:22<50:00,  5.47it/s][NeMo W 2025-03-27 21:09:46 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.39it/s]\n",
      "  1%|          | 132/16538 [00:22<46:55,  5.83it/s][NeMo W 2025-03-27 21:09:46 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.99it/s]\n",
      "  1%|          | 133/16538 [00:22<44:11,  6.19it/s][NeMo W 2025-03-27 21:09:46 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.82it/s]\n",
      "  1%|          | 134/16538 [00:22<42:14,  6.47it/s][NeMo W 2025-03-27 21:09:46 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.05it/s]\u001b[A\n",
      "  1%|          | 135/16538 [00:22<43:06,  6.34it/s][NeMo W 2025-03-27 21:09:46 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.72it/s]\u001b[A\n",
      "  1%|          | 136/16538 [00:22<42:45,  6.39it/s][NeMo W 2025-03-27 21:09:47 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.99it/s]\u001b[A\n",
      "  1%|          | 137/16538 [00:23<43:35,  6.27it/s][NeMo W 2025-03-27 21:09:47 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.08it/s]\n",
      "  1%|          | 138/16538 [00:23<41:26,  6.60it/s][NeMo W 2025-03-27 21:09:47 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.64it/s]\u001b[A\n",
      "  1%|          | 139/16538 [00:23<45:28,  6.01it/s][NeMo W 2025-03-27 21:09:47 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  3.69it/s]\u001b[A\n",
      "  1%|          | 140/16538 [00:23<56:04,  4.87it/s][NeMo W 2025-03-27 21:09:47 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.17it/s]\n",
      "  1%|          | 141/16538 [00:23<50:10,  5.45it/s][NeMo W 2025-03-27 21:09:48 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.54it/s]\u001b[A\n",
      "  1%|          | 142/16538 [00:24<51:50,  5.27it/s][NeMo W 2025-03-27 21:09:48 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.38it/s]\u001b[A\n",
      "  1%|          | 143/16538 [00:24<51:04,  5.35it/s][NeMo W 2025-03-27 21:09:48 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.51it/s]\u001b[A\n",
      "  1%|          | 144/16538 [00:24<48:37,  5.62it/s][NeMo W 2025-03-27 21:09:48 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.01it/s]\u001b[A\n",
      "  1%|          | 145/16538 [00:24<47:37,  5.74it/s][NeMo W 2025-03-27 21:09:48 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.17it/s]\u001b[A\n",
      "  1%|          | 146/16538 [00:24<46:40,  5.85it/s][NeMo W 2025-03-27 21:09:48 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.89it/s]\u001b[A\n",
      "  1%|          | 147/16538 [00:24<46:35,  5.86it/s][NeMo W 2025-03-27 21:09:49 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.75it/s]\n",
      "  1%|          | 148/16538 [00:25<43:54,  6.22it/s][NeMo W 2025-03-27 21:09:49 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.57it/s]\u001b[A\n",
      "  1%|          | 149/16538 [00:25<43:29,  6.28it/s][NeMo W 2025-03-27 21:09:49 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.43it/s]\u001b[A\n",
      "  1%|          | 150/16538 [00:25<45:05,  6.06it/s][NeMo W 2025-03-27 21:09:49 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.78it/s]\u001b[A\n",
      "  1%|          | 151/16538 [00:25<47:43,  5.72it/s][NeMo W 2025-03-27 21:09:49 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.60it/s]\u001b[A\n",
      "  1%|          | 152/16538 [00:25<49:58,  5.47it/s][NeMo W 2025-03-27 21:09:49 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.85it/s]\u001b[A\n",
      "  1%|          | 153/16538 [00:25<47:26,  5.76it/s][NeMo W 2025-03-27 21:09:50 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.33it/s]\u001b[A\n",
      "  1%|          | 154/16538 [00:26<46:16,  5.90it/s][NeMo W 2025-03-27 21:09:50 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.58it/s]\u001b[A\n",
      "  1%|          | 155/16538 [00:26<45:04,  6.06it/s][NeMo W 2025-03-27 21:09:50 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.11it/s]\u001b[A\n",
      "  1%|          | 156/16538 [00:26<46:51,  5.83it/s][NeMo W 2025-03-27 21:09:50 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.68it/s]\u001b[A\n",
      "  1%|          | 157/16538 [00:26<46:59,  5.81it/s][NeMo W 2025-03-27 21:09:50 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.90it/s]\u001b[A\n",
      "  1%|          | 158/16538 [00:26<51:37,  5.29it/s][NeMo W 2025-03-27 21:09:50 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.18it/s]\n",
      "  1%|          | 159/16538 [00:26<47:07,  5.79it/s][NeMo W 2025-03-27 21:09:51 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.95it/s]\u001b[A\n",
      "  1%|          | 160/16538 [00:27<46:39,  5.85it/s][NeMo W 2025-03-27 21:09:51 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.43it/s]\u001b[A\n",
      "  1%|          | 161/16538 [00:27<45:37,  5.98it/s][NeMo W 2025-03-27 21:09:51 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.28it/s]\n",
      "  1%|          | 162/16538 [00:27<43:57,  6.21it/s][NeMo W 2025-03-27 21:09:51 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.68it/s]\n",
      "  1%|          | 163/16538 [00:27<42:12,  6.46it/s][NeMo W 2025-03-27 21:09:51 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.10it/s]\n",
      "  1%|          | 164/16538 [00:27<40:29,  6.74it/s][NeMo W 2025-03-27 21:09:51 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.86it/s]\n",
      "  1%|          | 165/16538 [00:27<39:35,  6.89it/s][NeMo W 2025-03-27 21:09:52 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.47it/s]\u001b[A\n",
      "  1%|          | 166/16538 [00:27<40:43,  6.70it/s][NeMo W 2025-03-27 21:09:52 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.36it/s]\n",
      "  1%|          | 167/16538 [00:28<40:14,  6.78it/s][NeMo W 2025-03-27 21:09:52 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.29it/s]\u001b[A\n",
      "  1%|          | 168/16538 [00:28<41:20,  6.60it/s][NeMo W 2025-03-27 21:09:52 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.55it/s]\n",
      "  1%|          | 169/16538 [00:28<40:30,  6.74it/s][NeMo W 2025-03-27 21:09:52 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.83it/s]\u001b[A\n",
      "  1%|          | 170/16538 [00:28<40:42,  6.70it/s][NeMo W 2025-03-27 21:09:52 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.97it/s]\n",
      "  1%|          | 171/16538 [00:28<39:30,  6.91it/s][NeMo W 2025-03-27 21:09:52 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.79it/s]\u001b[A\n",
      "  1%|          | 172/16538 [00:28<42:51,  6.36it/s][NeMo W 2025-03-27 21:09:53 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.03it/s]\u001b[A\n",
      "  1%|          | 173/16538 [00:29<43:41,  6.24it/s][NeMo W 2025-03-27 21:09:53 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.49it/s]\n",
      "  1%|          | 174/16538 [00:29<42:19,  6.44it/s][NeMo W 2025-03-27 21:09:53 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.43it/s]\u001b[A\n",
      "  1%|          | 175/16538 [00:29<44:18,  6.16it/s][NeMo W 2025-03-27 21:09:53 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.31it/s]\u001b[A\n",
      "  1%|          | 176/16538 [00:29<48:21,  5.64it/s][NeMo W 2025-03-27 21:09:53 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.95it/s]\n",
      "  1%|          | 177/16538 [00:29<44:56,  6.07it/s][NeMo W 2025-03-27 21:09:53 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.47it/s]\u001b[A\n",
      "  1%|          | 178/16538 [00:29<44:20,  6.15it/s][NeMo W 2025-03-27 21:09:54 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.30it/s]\u001b[A\n",
      "  1%|          | 179/16538 [00:30<45:57,  5.93it/s][NeMo W 2025-03-27 21:09:54 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.88it/s]\n",
      "  1%|          | 180/16538 [00:30<43:17,  6.30it/s][NeMo W 2025-03-27 21:09:54 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.21it/s]\n",
      "  1%|          | 181/16538 [00:30<42:09,  6.47it/s][NeMo W 2025-03-27 21:09:54 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.29it/s]\u001b[A\n",
      "  1%|          | 182/16538 [00:30<44:25,  6.14it/s][NeMo W 2025-03-27 21:09:54 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.79it/s]\u001b[A\n",
      "  1%|          | 183/16538 [00:30<43:27,  6.27it/s][NeMo W 2025-03-27 21:09:54 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.88it/s]\n",
      "  1%|          | 184/16538 [00:30<41:34,  6.56it/s][NeMo W 2025-03-27 21:09:55 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.19it/s]\n",
      "  1%|          | 185/16538 [00:30<41:01,  6.64it/s][NeMo W 2025-03-27 21:09:55 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.93it/s]\u001b[A\n",
      "  1%|          | 186/16538 [00:31<41:02,  6.64it/s][NeMo W 2025-03-27 21:09:55 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.93it/s]\n",
      "  1%|          | 187/16538 [00:31<39:49,  6.84it/s][NeMo W 2025-03-27 21:09:55 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.92it/s]\n",
      "  1%|          | 188/16538 [00:31<38:56,  7.00it/s][NeMo W 2025-03-27 21:09:55 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.20it/s]\n",
      "  1%|          | 189/16538 [00:31<39:07,  6.97it/s][NeMo W 2025-03-27 21:09:55 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.18it/s]\n",
      "  1%|          | 190/16538 [00:31<39:20,  6.93it/s][NeMo W 2025-03-27 21:09:55 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.94it/s]\u001b[A\n",
      "  1%|          | 191/16538 [00:31<43:12,  6.31it/s][NeMo W 2025-03-27 21:09:56 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.45it/s]\u001b[A\n",
      "  1%|          | 192/16538 [00:32<44:49,  6.08it/s][NeMo W 2025-03-27 21:09:56 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.89it/s]\u001b[A\n",
      "  1%|          | 193/16538 [00:32<50:06,  5.44it/s][NeMo W 2025-03-27 21:09:56 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.51it/s]\n",
      "  1%|          | 194/16538 [00:32<46:51,  5.81it/s][NeMo W 2025-03-27 21:09:56 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  8.00it/s]\u001b[A\n",
      "  1%|          | 195/16538 [00:32<44:54,  6.07it/s][NeMo W 2025-03-27 21:09:56 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.95it/s]\n",
      "  1%|          | 196/16538 [00:32<42:29,  6.41it/s][NeMo W 2025-03-27 21:09:56 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.39it/s]\n",
      "  1%|          | 197/16538 [00:32<41:22,  6.58it/s][NeMo W 2025-03-27 21:09:57 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.46it/s]\u001b[A\n",
      "  1%|          | 198/16538 [00:33<45:51,  5.94it/s][NeMo W 2025-03-27 21:09:57 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.25it/s]\n",
      "  1%|          | 199/16538 [00:33<43:59,  6.19it/s][NeMo W 2025-03-27 21:09:57 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.91it/s]\u001b[A\n",
      "  1%|          | 200/16538 [00:33<46:35,  5.84it/s][NeMo W 2025-03-27 21:09:57 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.15it/s]\n",
      "  1%|          | 201/16538 [00:33<44:49,  6.07it/s][NeMo W 2025-03-27 21:09:57 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.38it/s]\n",
      "  1%|          | 202/16538 [00:33<43:05,  6.32it/s][NeMo W 2025-03-27 21:09:57 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.17it/s]\u001b[A\n",
      "  1%|          | 203/16538 [00:33<47:52,  5.69it/s][NeMo W 2025-03-27 21:09:58 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.27it/s]\n",
      "  1%|          | 204/16538 [00:34<45:24,  6.00it/s][NeMo W 2025-03-27 21:09:58 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.35it/s]\n",
      "  1%|          | 205/16538 [00:34<43:34,  6.25it/s][NeMo W 2025-03-27 21:09:58 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.13it/s]\n",
      "  1%|          | 206/16538 [00:34<41:25,  6.57it/s][NeMo W 2025-03-27 21:09:58 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.27it/s]\u001b[A\n",
      "  1%|         | 207/16538 [00:34<43:56,  6.20it/s][NeMo W 2025-03-27 21:09:58 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.44it/s]\n",
      "  1%|         | 208/16538 [00:34<42:21,  6.42it/s][NeMo W 2025-03-27 21:09:58 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.44it/s]\u001b[A\n",
      "  1%|         | 209/16538 [00:34<46:33,  5.84it/s][NeMo W 2025-03-27 21:09:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.09it/s]\n",
      "  1%|         | 210/16538 [00:35<43:30,  6.25it/s][NeMo W 2025-03-27 21:09:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.74it/s]\u001b[A\n",
      "  1%|         | 211/16538 [00:35<44:32,  6.11it/s][NeMo W 2025-03-27 21:09:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.12it/s]\u001b[A\n",
      "  1%|         | 212/16538 [00:35<44:36,  6.10it/s][NeMo W 2025-03-27 21:09:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.93it/s]\n",
      "  1%|         | 213/16538 [00:35<42:17,  6.43it/s][NeMo W 2025-03-27 21:09:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.45it/s]\n",
      "  1%|         | 214/16538 [00:35<40:09,  6.77it/s][NeMo W 2025-03-27 21:09:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.28it/s]\n",
      "  1%|         | 215/16538 [00:35<39:56,  6.81it/s][NeMo W 2025-03-27 21:09:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.95it/s]\n",
      "  1%|         | 216/16538 [00:35<39:08,  6.95it/s][NeMo W 2025-03-27 21:10:00 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.98it/s]\u001b[A\n",
      "  1%|         | 217/16538 [00:36<44:02,  6.18it/s][NeMo W 2025-03-27 21:10:00 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.14it/s]\u001b[A\n",
      "  1%|         | 218/16538 [00:36<44:10,  6.16it/s][NeMo W 2025-03-27 21:10:00 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.15it/s]\n",
      "  1%|         | 219/16538 [00:36<43:13,  6.29it/s][NeMo W 2025-03-27 21:10:00 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.49it/s]\n",
      "  1%|         | 220/16538 [00:36<41:49,  6.50it/s][NeMo W 2025-03-27 21:10:00 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.15it/s]\u001b[A\n",
      "  1%|         | 221/16538 [00:36<42:42,  6.37it/s][NeMo W 2025-03-27 21:10:00 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.63it/s]\u001b[A\n",
      "  1%|         | 222/16538 [00:36<42:35,  6.38it/s][NeMo W 2025-03-27 21:10:01 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  3.09it/s]\u001b[A\n",
      "  1%|         | 223/16538 [00:37<58:19,  4.66it/s][NeMo W 2025-03-27 21:10:01 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.84it/s]\u001b[A\n",
      "  1%|         | 224/16538 [00:37<53:10,  5.11it/s][NeMo W 2025-03-27 21:10:01 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.08it/s]\n",
      "  1%|         | 225/16538 [00:37<49:17,  5.52it/s][NeMo W 2025-03-27 21:10:01 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.16it/s]\n",
      "  1%|         | 226/16538 [00:37<46:26,  5.85it/s][NeMo W 2025-03-27 21:10:01 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.99it/s]\n",
      "  1%|         | 227/16538 [00:37<43:33,  6.24it/s][NeMo W 2025-03-27 21:10:01 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.22it/s]\n",
      "  1%|         | 228/16538 [00:37<41:13,  6.59it/s][NeMo W 2025-03-27 21:10:02 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.91it/s]\n",
      "  1%|         | 229/16538 [00:38<39:56,  6.80it/s][NeMo W 2025-03-27 21:10:02 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "  1%|         | 229/16538 [00:38<45:17,  6.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     transcriptions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Clean transcription\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nemo/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nemo/lib/python3.10/site-packages/nemo/collections/asr/models/aed_multitask_models.py:525\u001b[0m, in \u001b[0;36mEncDecMultiTaskModel.transcribe\u001b[0;34m(self, audio, batch_size, return_hypotheses, num_workers, channel_selector, augmentor, verbose, timestamps, override_config, **prompt)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    520\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverride_config must be of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMultiTaskTranscriptionConfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(override_config)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m         )\n\u001b[1;32m    523\u001b[0m     trcfg \u001b[38;5;241m=\u001b[39m override_config\n\u001b[0;32m--> 525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nemo/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nemo/lib/python3.10/site-packages/nemo/collections/asr/parts/mixins/transcription.py:269\u001b[0m, in \u001b[0;36mTranscriptionMixin.transcribe\u001b[0;34m(self, audio, batch_size, return_hypotheses, num_workers, channel_selector, augmentor, verbose, timestamps, override_config, **config_kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranscribe_generator(audio, override_config\u001b[38;5;241m=\u001b[39mtranscribe_cfg)\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m processed_outputs \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    272\u001b[0m             \u001b[38;5;66;03m# Create a results of the same type as each element in processed_outputs\u001b[39;00m\n\u001b[1;32m    273\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/nemo/lib/python3.10/site-packages/nemo/collections/asr/parts/mixins/transcription.py:375\u001b[0m, in \u001b[0;36mTranscriptionMixin.transcribe_generator\u001b[0;34m(self, audio, override_config)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    373\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscribing\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m verbose):\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# Move batch to device\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     test_batch \u001b[38;5;241m=\u001b[39m move_data_to_device(test_batch, transcribe_cfg\u001b[38;5;241m.\u001b[39m_internal\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# Run forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nemo/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nemo/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/nemo/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nemo/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1402\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1404\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/nemo/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nemo/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/.conda/envs/nemo/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === Silence warnings and NeMo logs ===\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('nemo').setLevel(logging.ERROR)\n",
    "\n",
    "# === Paths ===\n",
    "model_path = 'canary-180m-flash/canary-180m-flash.nemo'\n",
    "csv_path = '../../TORGO_CLEANED.csv'\n",
    "results_dir = 'results'\n",
    "checkpoint_path = os.path.join(results_dir, 'checkpoint.csv')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# === Load model ===\n",
    "model = EncDecMultiTaskModel.restore_from(restore_path=model_path)\n",
    "model.eval()\n",
    "\n",
    "# === Load dataset ===\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# === Load checkpoint if available ===\n",
    "if os.path.exists(checkpoint_path):\n",
    "    df_checkpoint = pd.read_csv(checkpoint_path)\n",
    "    processed_paths = set(df_checkpoint['wav'])\n",
    "    transcriptions = df_checkpoint['prediction'].tolist()\n",
    "    references = df_checkpoint['reference'].tolist()\n",
    "    speakers = df_checkpoint['spk_id'].tolist()\n",
    "else:\n",
    "    df_checkpoint = pd.DataFrame(columns=['wav', 'spk_id', 'prediction', 'reference'])\n",
    "    processed_paths = set()\n",
    "    transcriptions = []\n",
    "    references = []\n",
    "    speakers = []\n",
    "\n",
    "# === Inference with checkpointing ===\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if row['wav'] in processed_paths:\n",
    "        continue\n",
    "\n",
    "    audio_path = os.path.join('../../', row['wav'])\n",
    "    ref = str(row['wrd']).lower()\n",
    "    spk = row['spk_id']\n",
    "\n",
    "    if not os.path.exists(audio_path):\n",
    "        pred = ''\n",
    "    else:\n",
    "        result = model.transcribe([audio_path])[0]\n",
    "\n",
    "        # Clean transcription\n",
    "        pred = re.sub(r'\\[[^\\]]*\\]', '', result)\n",
    "        pred = pred.lower()\n",
    "        pred = re.sub(r\"[^a-z0-9\\s']\", '', pred)\n",
    "        pred = pred.strip()\n",
    "\n",
    "    # Store results\n",
    "    transcriptions.append(pred)\n",
    "    references.append(ref)\n",
    "    speakers.append(spk)\n",
    "    df_checkpoint = df_checkpoint.append({\n",
    "        'wav': row['wav'],\n",
    "        'spk_id': spk,\n",
    "        'prediction': pred,\n",
    "        'reference': ref\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    # Save checkpoint\n",
    "    df_checkpoint.to_csv(checkpoint_path, index=False)\n",
    "\n",
    "# === Merge results with original metadata ===\n",
    "df_result = df.merge(df_checkpoint, on=['wav', 'spk_id'])\n",
    "df_result = df_result[df_result['prediction'].str.strip() != '']\n",
    "df_result = df_result[['ID', 'duration', 'wav', 'spk_id', 'prediction', 'reference']]\n",
    "df_result.to_csv(os.path.join(results_dir, 'transcription_results.csv'), index=False)\n",
    "\n",
    "# === Compute WERs ===\n",
    "wer_per_spk = (\n",
    "    df_result.groupby('spk_id')\n",
    "    .apply(lambda g: pd.Series({\n",
    "        'wer': word_error_rate(\n",
    "            hypotheses=g['prediction'].tolist(),\n",
    "            references=g['reference'].tolist()\n",
    "        )\n",
    "    }))\n",
    "    .reset_index()\n",
    ")\n",
    "wer_per_spk.to_csv(os.path.join(results_dir, 'wer_per_speaker.csv'), index=False)\n",
    "\n",
    "total_wer = word_error_rate(\n",
    "    hypotheses=df_result['prediction'].tolist(),\n",
    "    references=df_result['reference'].tolist()\n",
    ")\n",
    "\n",
    "# === Save WER summary ===\n",
    "summary_path = os.path.join(results_dir, 'wer_summary.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(f'Total WER: {total_wer:.4f}\\n\\n')\n",
    "    f.write('WER per speaker:\\n')\n",
    "    for _, row in wer_per_spk.iterrows():\n",
    "        f.write(f'{row[\"spk_id\"]}: {row[\"wer\"]:.4f}\\n')\n",
    "\n",
    "print(f\"Total WER: {total_wer:.4f}\")\n",
    "print(f\"Results saved in: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
