{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from nemo.collections.asr.models import EncDecMultiTaskModel\n",
    "from nemo.collections.asr.metrics.wer import word_error_rate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-27 21:12:21 mixins:200] _setup_tokenizer: detected an aggregate tokenizer\n",
      "[NeMo I 2025-03-27 21:12:21 mixins:339] Tokenizer SentencePieceTokenizer initialized with 1152 tokens\n",
      "[NeMo I 2025-03-27 21:12:21 mixins:339] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-03-27 21:12:21 mixins:339] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-03-27 21:12:21 mixins:339] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-03-27 21:12:21 mixins:339] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-03-27 21:12:21 aggregate_tokenizer:73] Aggregate vocab size: 5248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-03-27 21:12:21 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_lhotse: true\n",
      "    input_cfg: null\n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    prompt_format: canary2\n",
      "    max_tps: 25\n",
      "    max_duration: 40.0\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    use_bucketing: true\n",
      "    bucket_duration_bins:\n",
      "    - - 3.56\n",
      "      - 30\n",
      "    - - 3.56\n",
      "      - 77\n",
      "    - - 4.608\n",
      "      - 38\n",
      "    - - 4.608\n",
      "      - 88\n",
      "    - - 5.48\n",
      "      - 49\n",
      "    - - 5.48\n",
      "      - 106\n",
      "    - - 6.05\n",
      "      - 52\n",
      "    - - 6.05\n",
      "      - 109\n",
      "    - - 6.85\n",
      "      - 54\n",
      "    - - 6.85\n",
      "      - 124\n",
      "    - - 7.914\n",
      "      - 59\n",
      "    - - 7.914\n",
      "      - 137\n",
      "    - - 8.52\n",
      "      - 67\n",
      "    - - 8.52\n",
      "      - 158\n",
      "    - - 9.51\n",
      "      - 67\n",
      "    - - 9.51\n",
      "      - 153\n",
      "    - - 10.29\n",
      "      - 78\n",
      "    - - 10.29\n",
      "      - 202\n",
      "    - - 10.89\n",
      "      - 88\n",
      "    - - 10.89\n",
      "      - 197\n",
      "    - - 11.45\n",
      "      - 92\n",
      "    - - 11.45\n",
      "      - 217\n",
      "    - - 12.1\n",
      "      - 98\n",
      "    - - 12.1\n",
      "      - 222\n",
      "    - - 12.69\n",
      "      - 123\n",
      "    - - 12.69\n",
      "      - 257\n",
      "    - - 13.09\n",
      "      - 133\n",
      "    - - 13.09\n",
      "      - 245\n",
      "    - - 13.57\n",
      "      - 104\n",
      "    - - 13.57\n",
      "      - 225\n",
      "    - - 14.02\n",
      "      - 108\n",
      "    - - 14.02\n",
      "      - 238\n",
      "    - - 14.53\n",
      "      - 117\n",
      "    - - 14.53\n",
      "      - 235\n",
      "    - - 15.08\n",
      "      - 135\n",
      "    - - 15.08\n",
      "      - 253\n",
      "    - - 15.53\n",
      "      - 130\n",
      "    - - 15.53\n",
      "      - 273\n",
      "    - - 15.945\n",
      "      - 123\n",
      "    - - 15.945\n",
      "      - 270\n",
      "    - - 16.4\n",
      "      - 126\n",
      "    - - 16.4\n",
      "      - 284\n",
      "    - - 16.9\n",
      "      - 126\n",
      "    - - 16.9\n",
      "      - 294\n",
      "    - - 17.44\n",
      "      - 127\n",
      "    - - 17.44\n",
      "      - 296\n",
      "    - - 17.96\n",
      "      - 131\n",
      "    - - 17.96\n",
      "      - 290\n",
      "    - - 18.41\n",
      "      - 133\n",
      "    - - 18.41\n",
      "      - 308\n",
      "    - - 18.91\n",
      "      - 137\n",
      "    - - 18.91\n",
      "      - 320\n",
      "    - - 19.42\n",
      "      - 135\n",
      "    - - 19.42\n",
      "      - 313\n",
      "    - - 20.0\n",
      "      - 138\n",
      "    - - 20.0\n",
      "      - 311\n",
      "    - - 34.547\n",
      "      - 195\n",
      "    - - 34.547\n",
      "      - 434\n",
      "    - - 40.0\n",
      "      - 263\n",
      "    - - 40.0\n",
      "      - 381\n",
      "    bucket_batch_size:\n",
      "    - 1044\n",
      "    - 835\n",
      "    - 786\n",
      "    - 662\n",
      "    - 662\n",
      "    - 558\n",
      "    - 594\n",
      "    - 501\n",
      "    - 534\n",
      "    - 450\n",
      "    - 465\n",
      "    - 391\n",
      "    - 417\n",
      "    - 342\n",
      "    - 378\n",
      "    - 318\n",
      "    - 340\n",
      "    - 271\n",
      "    - 322\n",
      "    - 258\n",
      "    - 295\n",
      "    - 248\n",
      "    - 284\n",
      "    - 234\n",
      "    - 250\n",
      "    - 210\n",
      "    - 250\n",
      "    - 210\n",
      "    - 250\n",
      "    - 210\n",
      "    - 241\n",
      "    - 203\n",
      "    - 232\n",
      "    - 196\n",
      "    - 216\n",
      "    - 187\n",
      "    - 207\n",
      "    - 174\n",
      "    - 207\n",
      "    - 174\n",
      "    - 199\n",
      "    - 163\n",
      "    - 194\n",
      "    - 163\n",
      "    - 188\n",
      "    - 158\n",
      "    - 186\n",
      "    - 156\n",
      "    - 172\n",
      "    - 145\n",
      "    - 173\n",
      "    - 142\n",
      "    - 167\n",
      "    - 140\n",
      "    - 160\n",
      "    - 135\n",
      "    - 86\n",
      "    - 74\n",
      "    - 70\n",
      "    - 64\n",
      "    bucket_buffer_size: 40000\n",
      "    shuffle_buffer_size: 10000\n",
      "    concurrent_bucketing: false\n",
      "    augmentor: null\n",
      "    \n",
      "[NeMo W 2025-03-27 21:12:21 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_lhotse: true\n",
      "    prompt_format: canary2\n",
      "    manifest_filepath: /data/ASR/librispeech/test_other.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    tarred_audio_filepaths: null\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-27 21:12:21 features:305] PADDING: 0\n",
      "[NeMo I 2025-03-27 21:12:24 save_restore_connector:275] Model EncDecMultiTaskModel was successfully restored from /scratch/flatala/pre_trained_models/cannary/canary-180m-flash/canary-180m-flash.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16538 [00:00<?, ?it/s][NeMo W 2025-03-27 21:12:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.70it/s]\n",
      "  0%|          | 1/16538 [00:00<43:46,  6.30it/s][NeMo W 2025-03-27 21:12:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.14it/s]\n",
      "  0%|          | 2/16538 [00:00<40:35,  6.79it/s][NeMo W 2025-03-27 21:12:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.08it/s]\n",
      "  0%|          | 3/16538 [00:00<39:12,  7.03it/s][NeMo W 2025-03-27 21:12:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.05it/s]\n",
      "  0%|          | 4/16538 [00:00<38:33,  7.15it/s][NeMo W 2025-03-27 21:12:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.87it/s]\n",
      "  0%|          | 5/16538 [00:00<38:35,  7.14it/s][NeMo W 2025-03-27 21:12:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.29it/s]\n",
      "  0%|          | 6/16538 [00:00<37:59,  7.25it/s][NeMo W 2025-03-27 21:12:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.12it/s]\u001b[A\n",
      "  0%|          | 7/16538 [00:01<42:40,  6.46it/s][NeMo W 2025-03-27 21:12:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.07it/s]\n",
      "  0%|          | 8/16538 [00:01<41:11,  6.69it/s][NeMo W 2025-03-27 21:12:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.78it/s]\u001b[A\n",
      "  0%|          | 9/16538 [00:01<48:34,  5.67it/s][NeMo W 2025-03-27 21:12:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.44it/s]\n",
      "  0%|          | 10/16538 [00:01<44:47,  6.15it/s][NeMo W 2025-03-27 21:12:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.72it/s]\u001b[A\n",
      "  0%|          | 11/16538 [00:01<47:59,  5.74it/s][NeMo W 2025-03-27 21:12:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.28it/s]\u001b[A\n",
      "  0%|          | 12/16538 [00:01<47:53,  5.75it/s][NeMo W 2025-03-27 21:12:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.02it/s]\n",
      "  0%|          | 13/16538 [00:02<44:47,  6.15it/s][NeMo W 2025-03-27 21:12:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.29it/s]\n",
      "  0%|          | 14/16538 [00:02<42:33,  6.47it/s][NeMo W 2025-03-27 21:12:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.89it/s]\n",
      "  0%|          | 15/16538 [00:02<41:15,  6.68it/s][NeMo W 2025-03-27 21:12:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.29it/s]\n",
      "  0%|          | 16/16538 [00:02<39:53,  6.90it/s][NeMo W 2025-03-27 21:12:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.11it/s]\n",
      "  0%|          | 17/16538 [00:02<40:20,  6.83it/s][NeMo W 2025-03-27 21:12:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.98it/s]\n",
      "  0%|          | 18/16538 [00:02<39:37,  6.95it/s][NeMo W 2025-03-27 21:12:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.49it/s]\n",
      "  0%|          | 19/16538 [00:02<38:36,  7.13it/s][NeMo W 2025-03-27 21:12:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.20it/s]\n",
      "  0%|          | 20/16538 [00:03<38:08,  7.22it/s][NeMo W 2025-03-27 21:12:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.00it/s]\n",
      "  0%|          | 21/16538 [00:03<38:05,  7.23it/s][NeMo W 2025-03-27 21:12:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.05it/s]\n",
      "  0%|          | 22/16538 [00:03<38:14,  7.20it/s][NeMo W 2025-03-27 21:12:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.37it/s]\u001b[A\n",
      "  0%|          | 23/16538 [00:03<44:29,  6.19it/s][NeMo W 2025-03-27 21:12:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.99it/s]\n",
      "  0%|          | 24/16538 [00:03<42:29,  6.48it/s][NeMo W 2025-03-27 21:12:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.21it/s]\u001b[A\n",
      "  0%|          | 25/16538 [00:03<45:10,  6.09it/s][NeMo W 2025-03-27 21:12:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.47it/s]\u001b[A\n",
      "  0%|          | 26/16538 [00:04<46:30,  5.92it/s][NeMo W 2025-03-27 21:12:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.60it/s]\n",
      "  0%|          | 27/16538 [00:04<44:17,  6.21it/s][NeMo W 2025-03-27 21:12:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.24it/s]\n",
      "  0%|          | 28/16538 [00:04<42:14,  6.52it/s][NeMo W 2025-03-27 21:12:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.28it/s]\n",
      "  0%|          | 29/16538 [00:04<40:37,  6.77it/s][NeMo W 2025-03-27 21:12:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.21it/s]\n",
      "  0%|          | 30/16538 [00:04<39:31,  6.96it/s][NeMo W 2025-03-27 21:12:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.09it/s]\n",
      "  0%|          | 31/16538 [00:04<38:51,  7.08it/s][NeMo W 2025-03-27 21:12:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.11it/s]\n",
      "  0%|          | 32/16538 [00:04<38:24,  7.16it/s][NeMo W 2025-03-27 21:12:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.12it/s]\n",
      "  0%|          | 33/16538 [00:04<38:05,  7.22it/s][NeMo W 2025-03-27 21:12:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.37it/s]\n",
      "  0%|          | 34/16538 [00:05<37:39,  7.30it/s][NeMo W 2025-03-27 21:12:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.80it/s]\n",
      "  0%|          | 35/16538 [00:05<37:54,  7.26it/s][NeMo W 2025-03-27 21:12:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.35it/s]\n",
      "  0%|          | 36/16538 [00:05<38:40,  7.11it/s][NeMo W 2025-03-27 21:12:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.11it/s]\u001b[A\n",
      "  0%|          | 37/16538 [00:05<40:46,  6.74it/s][NeMo W 2025-03-27 21:12:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.25it/s]\n",
      "  0%|          | 38/16538 [00:05<39:41,  6.93it/s][NeMo W 2025-03-27 21:12:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.02it/s]\u001b[A\n",
      "  0%|          | 39/16538 [00:05<43:37,  6.30it/s][NeMo W 2025-03-27 21:12:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.22it/s]\n",
      "  0%|          | 40/16538 [00:06<41:38,  6.60it/s][NeMo W 2025-03-27 21:12:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.66it/s]\n",
      "  0%|          | 41/16538 [00:06<40:49,  6.73it/s][NeMo W 2025-03-27 21:12:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.27it/s]\n",
      "  0%|          | 42/16538 [00:06<39:41,  6.93it/s][NeMo W 2025-03-27 21:12:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.57it/s]\u001b[A\n",
      "  0%|          | 43/16538 [00:06<42:37,  6.45it/s][NeMo W 2025-03-27 21:12:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.09it/s]\u001b[A\n",
      "  0%|          | 44/16538 [00:06<45:29,  6.04it/s][NeMo W 2025-03-27 21:12:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.84it/s]\n",
      "  0%|          | 45/16538 [00:06<43:20,  6.34it/s][NeMo W 2025-03-27 21:12:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.35it/s]\n",
      "  0%|          | 46/16538 [00:06<41:18,  6.65it/s][NeMo W 2025-03-27 21:12:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.34it/s]\n",
      "  0%|          | 47/16538 [00:07<40:55,  6.72it/s][NeMo W 2025-03-27 21:12:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.40it/s]\n",
      "  0%|          | 48/16538 [00:07<40:50,  6.73it/s][NeMo W 2025-03-27 21:12:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.68it/s]\u001b[A\n",
      "  0%|          | 49/16538 [00:07<45:27,  6.05it/s][NeMo W 2025-03-27 21:12:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.10it/s]\n",
      "  0%|          | 50/16538 [00:07<43:05,  6.38it/s][NeMo W 2025-03-27 21:12:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.66it/s]\u001b[A\n",
      "  0%|          | 51/16538 [00:07<44:40,  6.15it/s][NeMo W 2025-03-27 21:12:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.77it/s]\n",
      "  0%|          | 52/16538 [00:07<42:51,  6.41it/s][NeMo W 2025-03-27 21:12:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.09it/s]\n",
      "  0%|          | 53/16538 [00:08<41:14,  6.66it/s][NeMo W 2025-03-27 21:12:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.93it/s]\u001b[A\n",
      "  0%|          | 54/16538 [00:08<42:54,  6.40it/s][NeMo W 2025-03-27 21:12:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.02it/s]\n",
      "  0%|          | 55/16538 [00:08<41:18,  6.65it/s][NeMo W 2025-03-27 21:12:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.34it/s]\n",
      "  0%|          | 56/16538 [00:08<39:52,  6.89it/s][NeMo W 2025-03-27 21:12:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.83it/s]\n",
      "  0%|          | 57/16538 [00:08<39:34,  6.94it/s][NeMo W 2025-03-27 21:12:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.10it/s]\n",
      "  0%|          | 58/16538 [00:08<38:56,  7.05it/s][NeMo W 2025-03-27 21:12:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.29it/s]\n",
      "  0%|          | 59/16538 [00:08<38:25,  7.15it/s][NeMo W 2025-03-27 21:12:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.27it/s]\u001b[A\n",
      "  0%|          | 60/16538 [00:09<44:36,  6.16it/s][NeMo W 2025-03-27 21:12:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.55it/s]\u001b[A\n",
      "  0%|          | 61/16538 [00:09<45:56,  5.98it/s][NeMo W 2025-03-27 21:12:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.81it/s]\u001b[A\n",
      "  0%|          | 62/16538 [00:09<46:28,  5.91it/s][NeMo W 2025-03-27 21:12:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.05it/s]\n",
      "  0%|          | 63/16538 [00:09<43:52,  6.26it/s][NeMo W 2025-03-27 21:12:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.19it/s]\n",
      "  0%|          | 64/16538 [00:09<41:55,  6.55it/s][NeMo W 2025-03-27 21:12:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.64it/s]\n",
      "  0%|          | 65/16538 [00:09<41:04,  6.68it/s][NeMo W 2025-03-27 21:12:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.24it/s]\n",
      "  0%|          | 66/16538 [00:09<39:48,  6.90it/s][NeMo W 2025-03-27 21:12:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.22it/s]\n",
      "  0%|          | 67/16538 [00:10<38:57,  7.05it/s][NeMo W 2025-03-27 21:12:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.62it/s]\u001b[A\n",
      "  0%|          | 68/16538 [00:10<41:53,  6.55it/s][NeMo W 2025-03-27 21:12:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.87it/s]\n",
      "  0%|          | 69/16538 [00:10<40:51,  6.72it/s][NeMo W 2025-03-27 21:12:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.55it/s]\u001b[A\n",
      "  0%|          | 70/16538 [00:10<43:21,  6.33it/s][NeMo W 2025-03-27 21:12:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.10it/s]\n",
      "  0%|          | 71/16538 [00:10<41:36,  6.60it/s][NeMo W 2025-03-27 21:12:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.62it/s]\u001b[A\n",
      "  0%|          | 72/16538 [00:10<43:43,  6.28it/s][NeMo W 2025-03-27 21:12:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.70it/s]\n",
      "  0%|          | 73/16538 [00:11<42:34,  6.45it/s][NeMo W 2025-03-27 21:12:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.24it/s]\n",
      "  0%|          | 74/16538 [00:11<42:04,  6.52it/s][NeMo W 2025-03-27 21:12:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.36it/s]\n",
      "  0%|          | 75/16538 [00:11<40:27,  6.78it/s][NeMo W 2025-03-27 21:12:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.36it/s]\u001b[A\n",
      "  0%|          | 76/16538 [00:11<46:12,  5.94it/s][NeMo W 2025-03-27 21:12:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.41it/s]\u001b[A\n",
      "  0%|          | 77/16538 [00:11<53:24,  5.14it/s][NeMo W 2025-03-27 21:12:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.81it/s]\u001b[A\n",
      "  0%|          | 78/16538 [00:12<56:48,  4.83it/s][NeMo W 2025-03-27 21:12:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.48it/s]\u001b[A\n",
      "  0%|          | 79/16538 [00:12<52:54,  5.19it/s][NeMo W 2025-03-27 21:12:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.97it/s]\n",
      "  0%|          | 80/16538 [00:12<48:21,  5.67it/s][NeMo W 2025-03-27 21:12:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.88it/s]\n",
      "  0%|          | 81/16538 [00:12<45:29,  6.03it/s][NeMo W 2025-03-27 21:12:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.15it/s]\n",
      "  0%|          | 82/16538 [00:12<43:23,  6.32it/s][NeMo W 2025-03-27 21:12:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.74it/s]\n",
      "  1%|          | 83/16538 [00:12<42:07,  6.51it/s][NeMo W 2025-03-27 21:12:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.14it/s]\n",
      "  1%|          | 84/16538 [00:12<40:46,  6.73it/s][NeMo W 2025-03-27 21:12:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.65it/s]\u001b[A\n",
      "  1%|          | 85/16538 [00:13<43:05,  6.36it/s][NeMo W 2025-03-27 21:12:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.30it/s]\n",
      "  1%|          | 86/16538 [00:13<41:10,  6.66it/s][NeMo W 2025-03-27 21:12:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.67it/s]\n",
      "  1%|          | 87/16538 [00:13<40:32,  6.76it/s][NeMo W 2025-03-27 21:12:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.77it/s]\u001b[A\n",
      "  1%|          | 88/16538 [00:13<44:52,  6.11it/s][NeMo W 2025-03-27 21:12:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.69it/s]\n",
      "  1%|          | 89/16538 [00:13<43:18,  6.33it/s][NeMo W 2025-03-27 21:12:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.93it/s]\n",
      "  1%|          | 90/16538 [00:13<46:01,  5.96it/s][NeMo W 2025-03-27 21:12:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.50it/s]\u001b[A\n",
      "  1%|          | 91/16538 [00:14<49:19,  5.56it/s][NeMo W 2025-03-27 21:12:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.11it/s]\n",
      "  1%|          | 92/16538 [00:14<45:46,  5.99it/s][NeMo W 2025-03-27 21:12:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.03it/s]\n",
      "  1%|          | 93/16538 [00:14<43:25,  6.31it/s][NeMo W 2025-03-27 21:12:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.14it/s]\n",
      "  1%|          | 94/16538 [00:14<49:05,  5.58it/s][NeMo W 2025-03-27 21:12:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.42it/s]\n",
      "  1%|          | 95/16538 [00:14<52:42,  5.20it/s][NeMo W 2025-03-27 21:12:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.56it/s]\u001b[A\n",
      "  1%|          | 96/16538 [00:15<53:48,  5.09it/s][NeMo W 2025-03-27 21:12:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.30it/s]\n",
      "  1%|          | 97/16538 [00:15<48:41,  5.63it/s][NeMo W 2025-03-27 21:12:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.98it/s]\n",
      "  1%|          | 98/16538 [00:15<45:26,  6.03it/s][NeMo W 2025-03-27 21:12:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.08it/s]\n",
      "  1%|          | 99/16538 [00:15<43:04,  6.36it/s][NeMo W 2025-03-27 21:12:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.17it/s]\u001b[A\n",
      "  1%|          | 100/16538 [00:15<45:42,  5.99it/s][NeMo W 2025-03-27 21:12:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.88it/s]\n",
      "  1%|          | 101/16538 [00:15<43:26,  6.31it/s][NeMo W 2025-03-27 21:12:40 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.49it/s]\u001b[A\n",
      "  1%|          | 102/16538 [00:15<45:16,  6.05it/s][NeMo W 2025-03-27 21:12:40 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.02it/s]\n",
      "  1%|          | 103/16538 [00:16<43:08,  6.35it/s][NeMo W 2025-03-27 21:12:40 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.93it/s]\n",
      "  1%|          | 104/16538 [00:16<48:55,  5.60it/s][NeMo W 2025-03-27 21:12:40 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.96it/s]\u001b[A\n",
      "  1%|          | 105/16538 [00:16<50:20,  5.44it/s][NeMo W 2025-03-27 21:12:40 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.69it/s]\u001b[A\n",
      "  1%|          | 106/16538 [00:16<49:42,  5.51it/s][NeMo W 2025-03-27 21:12:40 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.92it/s]\n",
      "  1%|          | 107/16538 [00:16<46:15,  5.92it/s][NeMo W 2025-03-27 21:12:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.04it/s]\u001b[A\n",
      "  1%|          | 108/16538 [00:17<48:07,  5.69it/s][NeMo W 2025-03-27 21:12:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.90it/s]\n",
      "  1%|          | 109/16538 [00:17<45:20,  6.04it/s][NeMo W 2025-03-27 21:12:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.64it/s]\u001b[A\n",
      "  1%|          | 110/16538 [00:17<46:16,  5.92it/s][NeMo W 2025-03-27 21:12:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.85it/s]\n",
      "  1%|          | 111/16538 [00:17<43:59,  6.22it/s][NeMo W 2025-03-27 21:12:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.19it/s]\u001b[A\n",
      "  1%|          | 112/16538 [00:17<46:20,  5.91it/s][NeMo W 2025-03-27 21:12:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.50it/s]\n",
      "  1%|          | 113/16538 [00:17<44:25,  6.16it/s][NeMo W 2025-03-27 21:12:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.68it/s]\u001b[A\n",
      "  1%|          | 114/16538 [00:18<43:55,  6.23it/s][NeMo W 2025-03-27 21:12:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.15it/s]\n",
      "  1%|          | 115/16538 [00:18<41:57,  6.52it/s][NeMo W 2025-03-27 21:12:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.70it/s]\u001b[A\n",
      "  1%|          | 116/16538 [00:18<43:54,  6.23it/s][NeMo W 2025-03-27 21:12:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.98it/s]\n",
      "  1%|          | 117/16538 [00:18<42:07,  6.50it/s][NeMo W 2025-03-27 21:12:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.68it/s]\u001b[A\n",
      "  1%|          | 118/16538 [00:18<43:56,  6.23it/s][NeMo W 2025-03-27 21:12:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  1.42it/s]\u001b[A\n",
      "  1%|          | 119/16538 [00:19<1:30:42,  3.02it/s][NeMo W 2025-03-27 21:12:43 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.75it/s]\u001b[A\n",
      "  1%|          | 120/16538 [00:19<1:17:58,  3.51it/s][NeMo W 2025-03-27 21:12:43 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.10it/s]\n",
      "  1%|          | 121/16538 [00:19<1:05:51,  4.15it/s][NeMo W 2025-03-27 21:12:43 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.10it/s]\n",
      "  1%|          | 122/16538 [00:19<57:21,  4.77it/s]  [NeMo W 2025-03-27 21:12:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.96it/s]\n",
      "  1%|          | 123/16538 [00:19<51:33,  5.31it/s][NeMo W 2025-03-27 21:12:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.75it/s]\n",
      "  1%|          | 124/16538 [00:20<47:38,  5.74it/s][NeMo W 2025-03-27 21:12:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.97it/s]\n",
      "  1%|          | 125/16538 [00:20<44:42,  6.12it/s][NeMo W 2025-03-27 21:12:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.96it/s]\n",
      "  1%|          | 126/16538 [00:20<42:38,  6.42it/s][NeMo W 2025-03-27 21:12:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.75it/s]\n",
      "  1%|          | 127/16538 [00:20<41:29,  6.59it/s][NeMo W 2025-03-27 21:12:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.20it/s]\n",
      "  1%|          | 128/16538 [00:20<40:11,  6.81it/s][NeMo W 2025-03-27 21:12:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.93it/s]\n",
      "  1%|          | 129/16538 [00:20<39:32,  6.92it/s][NeMo W 2025-03-27 21:12:44 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.41it/s]\u001b[A\n",
      "  1%|          | 130/16538 [00:20<44:59,  6.08it/s][NeMo W 2025-03-27 21:12:45 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.04it/s]\n",
      "  1%|          | 131/16538 [00:21<42:46,  6.39it/s][NeMo W 2025-03-27 21:12:45 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.02it/s]\n",
      "  1%|          | 132/16538 [00:21<41:15,  6.63it/s][NeMo W 2025-03-27 21:12:45 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.91it/s]\n",
      "  1%|          | 133/16538 [00:21<40:17,  6.79it/s][NeMo W 2025-03-27 21:12:45 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.98it/s]\n",
      "  1%|          | 134/16538 [00:21<39:35,  6.91it/s][NeMo W 2025-03-27 21:12:45 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.17it/s]\u001b[A\n",
      "  1%|          | 135/16538 [00:21<41:20,  6.61it/s][NeMo W 2025-03-27 21:12:45 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.47it/s]\n",
      "  1%|          | 136/16538 [00:21<39:53,  6.85it/s][NeMo W 2025-03-27 21:12:46 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.93it/s]\u001b[A\n",
      "  1%|          | 137/16538 [00:22<46:43,  5.85it/s][NeMo W 2025-03-27 21:12:46 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.86it/s]\n",
      "  1%|          | 138/16538 [00:22<44:08,  6.19it/s][NeMo W 2025-03-27 21:12:46 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.47it/s]\u001b[A\n",
      "  1%|          | 139/16538 [00:22<46:12,  5.92it/s][NeMo W 2025-03-27 21:12:46 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.90it/s]\n",
      "  1%|          | 140/16538 [00:22<43:41,  6.25it/s][NeMo W 2025-03-27 21:12:46 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.21it/s]\n",
      "  1%|          | 141/16538 [00:22<41:41,  6.56it/s][NeMo W 2025-03-27 21:12:46 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.51it/s]\u001b[A\n",
      "  1%|          | 142/16538 [00:22<43:55,  6.22it/s][NeMo W 2025-03-27 21:12:47 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.00it/s]\u001b[A\n",
      "  1%|          | 143/16538 [00:23<44:37,  6.12it/s][NeMo W 2025-03-27 21:12:47 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.71it/s]\n",
      "  1%|          | 144/16538 [00:23<42:49,  6.38it/s][NeMo W 2025-03-27 21:12:47 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.95it/s]\u001b[A\n",
      "  1%|          | 145/16538 [00:23<44:02,  6.20it/s][NeMo W 2025-03-27 21:12:47 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.52it/s]\n",
      "  1%|          | 146/16538 [00:23<42:47,  6.39it/s][NeMo W 2025-03-27 21:12:47 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.77it/s]\u001b[A\n",
      "  1%|          | 147/16538 [00:23<42:48,  6.38it/s][NeMo W 2025-03-27 21:12:47 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.16it/s]\n",
      "  1%|          | 148/16538 [00:23<41:02,  6.66it/s][NeMo W 2025-03-27 21:12:47 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.75it/s]\n",
      "  1%|          | 149/16538 [00:23<40:20,  6.77it/s][NeMo W 2025-03-27 21:12:48 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.82it/s]\u001b[A\n",
      "  1%|          | 150/16538 [00:24<42:23,  6.44it/s][NeMo W 2025-03-27 21:12:48 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.32it/s]\n",
      "  1%|          | 151/16538 [00:24<40:37,  6.72it/s][NeMo W 2025-03-27 21:12:48 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.43it/s]\n",
      "  1%|          | 152/16538 [00:24<41:01,  6.66it/s][NeMo W 2025-03-27 21:12:48 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.66it/s]\n",
      "  1%|          | 153/16538 [00:24<40:23,  6.76it/s][NeMo W 2025-03-27 21:12:48 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.71it/s]\n",
      "  1%|          | 154/16538 [00:24<39:55,  6.84it/s][NeMo W 2025-03-27 21:12:48 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.81it/s]\n",
      "  1%|          | 155/16538 [00:24<39:27,  6.92it/s][NeMo W 2025-03-27 21:12:49 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.24it/s]\u001b[A\n",
      "  1%|          | 156/16538 [00:24<43:17,  6.31it/s][NeMo W 2025-03-27 21:12:49 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.87it/s]\n",
      "  1%|          | 157/16538 [00:25<41:46,  6.54it/s][NeMo W 2025-03-27 21:12:49 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.05it/s]\u001b[A\n",
      "  1%|          | 158/16538 [00:25<45:07,  6.05it/s][NeMo W 2025-03-27 21:12:49 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.02it/s]\n",
      "  1%|          | 159/16538 [00:25<42:52,  6.37it/s][NeMo W 2025-03-27 21:12:49 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.92it/s]\n",
      "  1%|          | 160/16538 [00:25<41:19,  6.61it/s][NeMo W 2025-03-27 21:12:49 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.87it/s]\n",
      "  1%|          | 161/16538 [00:25<40:31,  6.74it/s][NeMo W 2025-03-27 21:12:49 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.00it/s]\n",
      "  1%|          | 162/16538 [00:25<39:39,  6.88it/s][NeMo W 2025-03-27 21:12:50 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.26it/s]\n",
      "  1%|          | 163/16538 [00:26<38:57,  7.01it/s][NeMo W 2025-03-27 21:12:50 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.97it/s]\n",
      "  1%|          | 164/16538 [00:26<39:03,  6.99it/s][NeMo W 2025-03-27 21:12:50 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.12it/s]\n",
      "  1%|          | 165/16538 [00:26<38:29,  7.09it/s][NeMo W 2025-03-27 21:12:50 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.14it/s]\n",
      "  1%|          | 166/16538 [00:26<38:06,  7.16it/s][NeMo W 2025-03-27 21:12:50 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  1.68it/s]\u001b[A\n",
      "  1%|          | 167/16538 [00:27<1:35:49,  2.85it/s][NeMo W 2025-03-27 21:12:51 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.79it/s]\n",
      "  1%|          | 168/16538 [00:27<1:18:57,  3.46it/s][NeMo W 2025-03-27 21:12:51 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.08it/s]\n",
      "  1%|          | 169/16538 [00:27<1:06:29,  4.10it/s][NeMo W 2025-03-27 21:12:51 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.91it/s]\n",
      "  1%|          | 170/16538 [00:27<58:37,  4.65it/s]  [NeMo W 2025-03-27 21:12:51 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.27it/s]\n",
      "  1%|          | 171/16538 [00:27<52:04,  5.24it/s][NeMo W 2025-03-27 21:12:52 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.21it/s]\u001b[A\n",
      "  1%|          | 172/16538 [00:28<51:48,  5.27it/s][NeMo W 2025-03-27 21:12:52 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.15it/s]\n",
      "  1%|          | 173/16538 [00:28<47:20,  5.76it/s][NeMo W 2025-03-27 21:12:52 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.74it/s]\n",
      "  1%|          | 174/16538 [00:28<44:39,  6.11it/s][NeMo W 2025-03-27 21:12:52 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.82it/s]\u001b[A\n",
      "  1%|          | 175/16538 [00:28<45:32,  5.99it/s][NeMo W 2025-03-27 21:12:52 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.52it/s]\u001b[A\n",
      "  1%|          | 176/16538 [00:28<48:50,  5.58it/s][NeMo W 2025-03-27 21:12:52 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.36it/s]\n",
      "  1%|          | 177/16538 [00:28<46:08,  5.91it/s][NeMo W 2025-03-27 21:12:53 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.92it/s]\n",
      "  1%|          | 178/16538 [00:28<43:39,  6.24it/s][NeMo W 2025-03-27 21:12:53 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.19it/s]\n",
      "  1%|          | 179/16538 [00:29<41:37,  6.55it/s][NeMo W 2025-03-27 21:12:53 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.72it/s]\n",
      "  1%|          | 180/16538 [00:29<40:40,  6.70it/s][NeMo W 2025-03-27 21:12:53 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.38it/s]\n",
      "  1%|          | 181/16538 [00:29<40:23,  6.75it/s][NeMo W 2025-03-27 21:12:53 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.01it/s]\u001b[A\n",
      "  1%|          | 182/16538 [00:29<42:13,  6.46it/s][NeMo W 2025-03-27 21:12:53 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.57it/s]\n",
      "  1%|          | 183/16538 [00:29<41:37,  6.55it/s][NeMo W 2025-03-27 21:12:53 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.85it/s]\n",
      "  1%|          | 184/16538 [00:29<40:39,  6.70it/s][NeMo W 2025-03-27 21:12:54 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.50it/s]\u001b[A\n",
      "  1%|          | 185/16538 [00:30<43:16,  6.30it/s][NeMo W 2025-03-27 21:12:54 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  2.67it/s]\u001b[A\n",
      "  1%|          | 186/16538 [00:30<1:03:07,  4.32it/s][NeMo W 2025-03-27 21:12:54 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.02it/s]\n",
      "  1%|          | 187/16538 [00:30<55:35,  4.90it/s]  [NeMo W 2025-03-27 21:12:54 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  2.14it/s]\u001b[A\n",
      "  1%|          | 188/16538 [00:31<1:24:23,  3.23it/s][NeMo W 2025-03-27 21:12:55 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.81it/s]\u001b[A\n",
      "  1%|          | 189/16538 [00:31<1:18:18,  3.48it/s][NeMo W 2025-03-27 21:12:55 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.32it/s]\u001b[A\n",
      "  1%|          | 190/16538 [00:31<1:12:22,  3.76it/s][NeMo W 2025-03-27 21:12:55 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.86it/s]\u001b[A\n",
      "  1%|          | 191/16538 [00:31<1:09:47,  3.90it/s][NeMo W 2025-03-27 21:12:56 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.33it/s]\u001b[A\n",
      "  1%|          | 192/16538 [00:32<1:03:57,  4.26it/s][NeMo W 2025-03-27 21:12:56 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.87it/s]\u001b[A\n",
      "  1%|          | 193/16538 [00:32<1:00:50,  4.48it/s][NeMo W 2025-03-27 21:12:56 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.63it/s]\n",
      "  1%|          | 194/16538 [00:32<54:15,  5.02it/s]  [NeMo W 2025-03-27 21:12:56 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.49it/s]\n",
      "  1%|          | 195/16538 [00:32<49:46,  5.47it/s][NeMo W 2025-03-27 21:12:56 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.91it/s]\n",
      "  1%|          | 196/16538 [00:32<46:10,  5.90it/s][NeMo W 2025-03-27 21:12:56 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.28it/s]\n",
      "  1%|          | 197/16538 [00:32<44:38,  6.10it/s][NeMo W 2025-03-27 21:12:56 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.20it/s]\u001b[A\n",
      "  1%|          | 198/16538 [00:32<46:34,  5.85it/s][NeMo W 2025-03-27 21:12:57 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.97it/s]\n",
      "  1%|          | 199/16538 [00:33<43:54,  6.20it/s][NeMo W 2025-03-27 21:12:57 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.31it/s]\u001b[A\n",
      "  1%|          | 200/16538 [00:33<45:50,  5.94it/s][NeMo W 2025-03-27 21:12:57 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.45it/s]\n",
      "  1%|          | 201/16538 [00:33<44:06,  6.17it/s][NeMo W 2025-03-27 21:12:57 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.00it/s]\n",
      "  1%|          | 202/16538 [00:33<42:18,  6.43it/s][NeMo W 2025-03-27 21:12:57 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.14it/s]\u001b[A\n",
      "  1%|          | 203/16538 [00:33<47:42,  5.71it/s][NeMo W 2025-03-27 21:12:58 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.66it/s]\n",
      "  1%|          | 204/16538 [00:33<45:05,  6.04it/s][NeMo W 2025-03-27 21:12:58 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.33it/s]\n",
      "  1%|          | 205/16538 [00:34<43:32,  6.25it/s][NeMo W 2025-03-27 21:12:58 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.17it/s]\n",
      "  1%|          | 206/16538 [00:34<41:33,  6.55it/s][NeMo W 2025-03-27 21:12:58 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.26it/s]\u001b[A\n",
      "  1%|▏         | 207/16538 [00:34<44:18,  6.14it/s][NeMo W 2025-03-27 21:12:58 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.77it/s]\n",
      "  1%|▏         | 208/16538 [00:34<42:35,  6.39it/s][NeMo W 2025-03-27 21:12:58 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.94it/s]\u001b[A\n",
      "  1%|▏         | 209/16538 [00:34<45:40,  5.96it/s][NeMo W 2025-03-27 21:12:58 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.20it/s]\n",
      "  1%|▏         | 210/16538 [00:34<43:03,  6.32it/s][NeMo W 2025-03-27 21:12:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.03it/s]\n",
      "  1%|▏         | 211/16538 [00:35<41:23,  6.57it/s][NeMo W 2025-03-27 21:12:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.11it/s]\u001b[A\n",
      "  1%|▏         | 212/16538 [00:35<42:38,  6.38it/s][NeMo W 2025-03-27 21:12:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.23it/s]\n",
      "  1%|▏         | 213/16538 [00:35<40:56,  6.65it/s][NeMo W 2025-03-27 21:12:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.08it/s]\n",
      "  1%|▏         | 214/16538 [00:35<39:47,  6.84it/s][NeMo W 2025-03-27 21:12:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.33it/s]\n",
      "  1%|▏         | 215/16538 [00:35<38:46,  7.02it/s][NeMo W 2025-03-27 21:12:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.52it/s]\n",
      "  1%|▏         | 216/16538 [00:35<38:54,  6.99it/s][NeMo W 2025-03-27 21:12:59 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.24it/s]\u001b[A\n",
      "  1%|▏         | 217/16538 [00:35<42:32,  6.39it/s][NeMo W 2025-03-27 21:13:00 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.61it/s]\n",
      "  1%|▏         | 218/16538 [00:36<41:33,  6.55it/s][NeMo W 2025-03-27 21:13:00 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.04it/s]\n",
      "  1%|▏         | 219/16538 [00:36<40:22,  6.74it/s][NeMo W 2025-03-27 21:13:00 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.88it/s]\n",
      "  1%|▏         | 220/16538 [00:36<39:38,  6.86it/s][NeMo W 2025-03-27 21:13:00 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.36it/s]\n",
      "  1%|▏         | 221/16538 [00:36<39:39,  6.86it/s][NeMo W 2025-03-27 21:13:00 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.18it/s]\n",
      "  1%|▏         | 222/16538 [00:36<40:00,  6.80it/s][NeMo W 2025-03-27 21:13:00 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.97it/s]\n",
      "  1%|▏         | 223/16538 [00:36<39:15,  6.93it/s][NeMo W 2025-03-27 21:13:00 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.98it/s]\n",
      "  1%|▏         | 224/16538 [00:36<38:45,  7.01it/s][NeMo W 2025-03-27 21:13:01 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.78it/s]\n",
      "  1%|▏         | 225/16538 [00:37<38:47,  7.01it/s][NeMo W 2025-03-27 21:13:01 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.79it/s]\n",
      "  1%|▏         | 226/16538 [00:37<38:36,  7.04it/s][NeMo W 2025-03-27 21:13:01 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.46it/s]\n",
      "  1%|▏         | 227/16538 [00:37<38:56,  6.98it/s][NeMo W 2025-03-27 21:13:01 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.94it/s]\n",
      "  1%|▏         | 228/16538 [00:37<38:38,  7.04it/s][NeMo W 2025-03-27 21:13:01 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.08it/s]\n",
      "  1%|▏         | 229/16538 [00:37<38:11,  7.12it/s][NeMo W 2025-03-27 21:13:01 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.18it/s]\n",
      "  1%|▏         | 230/16538 [00:37<38:51,  6.99it/s][NeMo W 2025-03-27 21:13:01 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.62it/s]\n",
      "  1%|▏         | 231/16538 [00:37<38:53,  6.99it/s][NeMo W 2025-03-27 21:13:02 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.84it/s]\u001b[A\n",
      "  1%|▏         | 232/16538 [00:38<41:18,  6.58it/s][NeMo W 2025-03-27 21:13:02 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.99it/s]\u001b[A\n",
      "  1%|▏         | 233/16538 [00:38<42:52,  6.34it/s][NeMo W 2025-03-27 21:13:02 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.49it/s]\n",
      "  1%|▏         | 234/16538 [00:38<41:50,  6.49it/s][NeMo W 2025-03-27 21:13:02 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.32it/s]\n",
      "  1%|▏         | 235/16538 [00:38<41:16,  6.58it/s][NeMo W 2025-03-27 21:13:02 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.48it/s]\u001b[A\n",
      "  1%|▏         | 236/16538 [00:38<45:58,  5.91it/s][NeMo W 2025-03-27 21:13:02 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  1.90it/s]\u001b[A\n",
      "  1%|▏         | 237/16538 [00:39<1:17:20,  3.51it/s][NeMo W 2025-03-27 21:13:03 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.54it/s]\n",
      "  1%|▏         | 238/16538 [00:39<1:05:57,  4.12it/s][NeMo W 2025-03-27 21:13:03 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.02it/s]\u001b[A\n",
      "  1%|▏         | 239/16538 [00:39<1:01:51,  4.39it/s][NeMo W 2025-03-27 21:13:03 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.23it/s]\u001b[A\n",
      "  1%|▏         | 240/16538 [00:39<58:36,  4.63it/s]  [NeMo W 2025-03-27 21:13:04 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.81it/s]\u001b[A\n",
      "  1%|▏         | 241/16538 [00:40<54:09,  5.01it/s][NeMo W 2025-03-27 21:13:04 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.68it/s]\n",
      "  1%|▏         | 242/16538 [00:40<49:25,  5.50it/s][NeMo W 2025-03-27 21:13:04 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.61it/s]\u001b[A\n",
      "  1%|▏         | 243/16538 [00:40<54:29,  4.98it/s][NeMo W 2025-03-27 21:13:04 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.11it/s]\n",
      "  1%|▏         | 244/16538 [00:40<49:21,  5.50it/s][NeMo W 2025-03-27 21:13:04 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.18it/s]\u001b[A\n",
      "  1%|▏         | 245/16538 [00:40<49:55,  5.44it/s][NeMo W 2025-03-27 21:13:04 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  3.87it/s]\u001b[A\n",
      "  1%|▏         | 246/16538 [00:41<58:11,  4.67it/s][NeMo W 2025-03-27 21:13:05 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.50it/s]\u001b[A\n",
      "  1%|▏         | 247/16538 [00:41<57:44,  4.70it/s][NeMo W 2025-03-27 21:13:05 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.32it/s]\u001b[A\n",
      "  1%|▏         | 248/16538 [00:41<53:44,  5.05it/s][NeMo W 2025-03-27 21:13:05 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.08it/s]\u001b[A\n",
      "  2%|▏         | 249/16538 [00:41<53:12,  5.10it/s][NeMo W 2025-03-27 21:13:05 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.81it/s]\u001b[A\n",
      "  2%|▏         | 250/16538 [00:41<49:50,  5.45it/s][NeMo W 2025-03-27 21:13:05 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.74it/s]\n",
      "  2%|▏         | 251/16538 [00:41<46:26,  5.85it/s][NeMo W 2025-03-27 21:13:06 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.63it/s]\u001b[A\n",
      "  2%|▏         | 252/16538 [00:42<49:06,  5.53it/s][NeMo W 2025-03-27 21:13:06 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.23it/s]\u001b[A\n",
      "  2%|▏         | 253/16538 [00:42<52:04,  5.21it/s][NeMo W 2025-03-27 21:13:06 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.62it/s]\u001b[A\n",
      "  2%|▏         | 254/16538 [00:42<50:51,  5.34it/s][NeMo W 2025-03-27 21:13:06 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.59it/s]\u001b[A\n",
      "  2%|▏         | 255/16538 [00:42<55:41,  4.87it/s][NeMo W 2025-03-27 21:13:06 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.62it/s]\n",
      "  2%|▏         | 256/16538 [00:42<50:34,  5.37it/s][NeMo W 2025-03-27 21:13:07 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.44it/s]\u001b[A\n",
      "  2%|▏         | 257/16538 [00:43<48:31,  5.59it/s][NeMo W 2025-03-27 21:13:07 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.08it/s]\u001b[A\n",
      "  2%|▏         | 258/16538 [00:43<47:37,  5.70it/s][NeMo W 2025-03-27 21:13:07 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.46it/s]\u001b[A\n",
      "  2%|▏         | 259/16538 [00:43<46:22,  5.85it/s][NeMo W 2025-03-27 21:13:07 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  1.11it/s]\u001b[A\n",
      "  2%|▏         | 260/16538 [00:44<1:48:06,  2.51it/s][NeMo W 2025-03-27 21:13:08 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  3.32it/s]\u001b[A\n",
      "  2%|▏         | 261/16538 [00:44<1:42:31,  2.65it/s][NeMo W 2025-03-27 21:13:08 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.58it/s]\u001b[A\n",
      "  2%|▏         | 262/16538 [00:44<1:24:40,  3.20it/s][NeMo W 2025-03-27 21:13:08 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.23it/s]\u001b[A\n",
      "  2%|▏         | 263/16538 [00:45<1:20:40,  3.36it/s][NeMo W 2025-03-27 21:13:09 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.71it/s]\u001b[A\n",
      "  2%|▏         | 264/16538 [00:45<1:15:56,  3.57it/s][NeMo W 2025-03-27 21:13:09 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  1.89it/s]\u001b[A\n",
      "  2%|▏         | 265/16538 [00:45<1:38:18,  2.76it/s][NeMo W 2025-03-27 21:13:10 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.26it/s]\u001b[A\n",
      "  2%|▏         | 266/16538 [00:46<1:30:06,  3.01it/s][NeMo W 2025-03-27 21:13:10 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.88it/s]\u001b[A\n",
      "  2%|▏         | 267/16538 [00:46<1:19:08,  3.43it/s][NeMo W 2025-03-27 21:13:10 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.34it/s]\n",
      "  2%|▏         | 268/16538 [00:46<1:07:18,  4.03it/s][NeMo W 2025-03-27 21:13:10 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.76it/s]\u001b[A\n",
      "  2%|▏         | 269/16538 [00:46<59:58,  4.52it/s]  [NeMo W 2025-03-27 21:13:10 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.46it/s]\n",
      "  2%|▏         | 270/16538 [00:46<53:50,  5.04it/s][NeMo W 2025-03-27 21:13:10 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.90it/s]\u001b[A\n",
      "  2%|▏         | 271/16538 [00:46<56:32,  4.80it/s][NeMo W 2025-03-27 21:13:11 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.62it/s]\u001b[A\n",
      "  2%|▏         | 272/16538 [00:47<52:26,  5.17it/s][NeMo W 2025-03-27 21:13:11 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.39it/s]\u001b[A\n",
      "  2%|▏         | 273/16538 [00:47<50:26,  5.37it/s][NeMo W 2025-03-27 21:13:11 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.40it/s]\u001b[A\n",
      "  2%|▏         | 274/16538 [00:47<50:14,  5.40it/s][NeMo W 2025-03-27 21:13:11 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  3.77it/s]\u001b[A\n",
      "  2%|▏         | 275/16538 [00:47<58:52,  4.60it/s][NeMo W 2025-03-27 21:13:11 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.07it/s]\n",
      "  2%|▏         | 276/16538 [00:47<52:25,  5.17it/s][NeMo W 2025-03-27 21:13:12 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.79it/s]\u001b[A\n",
      "  2%|▏         | 277/16538 [00:48<50:55,  5.32it/s][NeMo W 2025-03-27 21:13:12 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.33it/s]\n",
      "  2%|▏         | 278/16538 [00:48<47:35,  5.69it/s][NeMo W 2025-03-27 21:13:12 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.58it/s]\u001b[A\n",
      "  2%|▏         | 279/16538 [00:48<47:51,  5.66it/s][NeMo W 2025-03-27 21:13:12 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.39it/s]\u001b[A\n",
      "  2%|▏         | 280/16538 [00:48<50:51,  5.33it/s][NeMo W 2025-03-27 21:13:12 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.89it/s]\n",
      "  2%|▏         | 281/16538 [00:48<46:55,  5.77it/s][NeMo W 2025-03-27 21:13:12 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.23it/s]\n",
      "  2%|▏         | 282/16538 [00:48<44:53,  6.03it/s][NeMo W 2025-03-27 21:13:13 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.06it/s]\n",
      "  2%|▏         | 283/16538 [00:49<42:33,  6.37it/s][NeMo W 2025-03-27 21:13:13 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.28it/s]\n",
      "  2%|▏         | 284/16538 [00:49<41:47,  6.48it/s][NeMo W 2025-03-27 21:13:13 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.37it/s]\u001b[A\n",
      "  2%|▏         | 285/16538 [00:49<46:36,  5.81it/s][NeMo W 2025-03-27 21:13:13 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.20it/s]\u001b[A\n",
      "  2%|▏         | 286/16538 [00:49<47:54,  5.65it/s][NeMo W 2025-03-27 21:13:13 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.24it/s]\n",
      "  2%|▏         | 287/16538 [00:49<45:41,  5.93it/s][NeMo W 2025-03-27 21:13:13 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.19it/s]\u001b[A\n",
      "  2%|▏         | 288/16538 [00:49<50:11,  5.40it/s][NeMo W 2025-03-27 21:13:14 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.10it/s]\n",
      "  2%|▏         | 289/16538 [00:50<47:23,  5.72it/s][NeMo W 2025-03-27 21:13:14 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.31it/s]\n",
      "  2%|▏         | 290/16538 [00:50<45:10,  5.99it/s][NeMo W 2025-03-27 21:13:14 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.03it/s]\u001b[A\n",
      "  2%|▏         | 291/16538 [00:50<47:31,  5.70it/s][NeMo W 2025-03-27 21:13:14 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.91it/s]\u001b[A\n",
      "  2%|▏         | 292/16538 [00:50<45:51,  5.90it/s][NeMo W 2025-03-27 21:13:14 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.02it/s]\n",
      "  2%|▏         | 293/16538 [00:50<43:19,  6.25it/s][NeMo W 2025-03-27 21:13:14 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.17it/s]\n",
      "  2%|▏         | 294/16538 [00:50<43:42,  6.19it/s][NeMo W 2025-03-27 21:13:15 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.29it/s]\u001b[A\n",
      "  2%|▏         | 295/16538 [00:51<44:08,  6.13it/s][NeMo W 2025-03-27 21:13:15 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.53it/s]\u001b[A\n",
      "  2%|▏         | 296/16538 [00:51<45:38,  5.93it/s][NeMo W 2025-03-27 21:13:15 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.41it/s]\u001b[A\n",
      "  2%|▏         | 297/16538 [00:51<46:49,  5.78it/s][NeMo W 2025-03-27 21:13:15 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.77it/s]\n",
      "  2%|▏         | 298/16538 [00:51<44:13,  6.12it/s][NeMo W 2025-03-27 21:13:15 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.92it/s]\n",
      "  2%|▏         | 299/16538 [00:51<42:17,  6.40it/s][NeMo W 2025-03-27 21:13:15 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.62it/s]\u001b[A\n",
      "  2%|▏         | 300/16538 [00:51<44:05,  6.14it/s][NeMo W 2025-03-27 21:13:16 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.13it/s]\u001b[A\n",
      "  2%|▏         | 301/16538 [00:52<46:17,  5.85it/s][NeMo W 2025-03-27 21:13:16 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.09it/s]\n",
      "  2%|▏         | 302/16538 [00:52<44:45,  6.05it/s][NeMo W 2025-03-27 21:13:16 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.60it/s]\u001b[A\n",
      "  2%|▏         | 303/16538 [00:52<44:36,  6.07it/s][NeMo W 2025-03-27 21:13:16 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.72it/s]\n",
      "  2%|▏         | 304/16538 [00:52<42:44,  6.33it/s][NeMo W 2025-03-27 21:13:16 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.08it/s]\n",
      "  2%|▏         | 305/16538 [00:52<42:18,  6.40it/s][NeMo W 2025-03-27 21:13:16 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.47it/s]\u001b[A\n",
      "  2%|▏         | 306/16538 [00:52<44:23,  6.09it/s][NeMo W 2025-03-27 21:13:17 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.76it/s]\n",
      "  2%|▏         | 307/16538 [00:53<42:31,  6.36it/s][NeMo W 2025-03-27 21:13:17 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.16it/s]\u001b[A\n",
      "  2%|▏         | 308/16538 [00:53<49:26,  5.47it/s][NeMo W 2025-03-27 21:13:17 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.62it/s]\u001b[A\n",
      "  2%|▏         | 309/16538 [00:53<51:18,  5.27it/s][NeMo W 2025-03-27 21:13:17 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.16it/s]\n",
      "  2%|▏         | 310/16538 [00:53<48:05,  5.62it/s][NeMo W 2025-03-27 21:13:17 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.71it/s]\u001b[A\n",
      "  2%|▏         | 311/16538 [00:53<46:40,  5.80it/s][NeMo W 2025-03-27 21:13:17 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.40it/s]\n",
      "  2%|▏         | 312/16538 [00:53<44:31,  6.07it/s][NeMo W 2025-03-27 21:13:18 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.42it/s]\n",
      "  2%|▏         | 313/16538 [00:54<43:02,  6.28it/s][NeMo W 2025-03-27 21:13:18 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.21it/s]\n",
      "  2%|▏         | 314/16538 [00:54<42:12,  6.41it/s][NeMo W 2025-03-27 21:13:18 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.01it/s]\u001b[A\n",
      "  2%|▏         | 315/16538 [00:54<43:18,  6.24it/s][NeMo W 2025-03-27 21:13:18 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.47it/s]\u001b[A\n",
      "  2%|▏         | 316/16538 [00:54<43:23,  6.23it/s][NeMo W 2025-03-27 21:13:18 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.54it/s]\u001b[A\n",
      "  2%|▏         | 317/16538 [00:54<43:21,  6.24it/s][NeMo W 2025-03-27 21:13:18 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.75it/s]\u001b[A\n",
      "  2%|▏         | 318/16538 [00:54<46:43,  5.79it/s][NeMo W 2025-03-27 21:13:19 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.43it/s]\n",
      "  2%|▏         | 319/16538 [00:55<44:29,  6.08it/s][NeMo W 2025-03-27 21:13:19 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.85it/s]\u001b[A\n",
      "  2%|▏         | 320/16538 [00:55<45:08,  5.99it/s][NeMo W 2025-03-27 21:13:19 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.70it/s]\u001b[A\n",
      "  2%|▏         | 321/16538 [00:55<48:00,  5.63it/s][NeMo W 2025-03-27 21:13:19 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  3.90it/s]\u001b[A\n",
      "  2%|▏         | 322/16538 [00:55<56:34,  4.78it/s][NeMo W 2025-03-27 21:13:19 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.95it/s]\u001b[A\n",
      "  2%|▏         | 323/16538 [00:55<58:21,  4.63it/s][NeMo W 2025-03-27 21:13:20 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.27it/s]\n",
      "  2%|▏         | 324/16538 [00:56<52:57,  5.10it/s][NeMo W 2025-03-27 21:13:20 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.77it/s]\n",
      "  2%|▏         | 325/16538 [00:56<48:32,  5.57it/s][NeMo W 2025-03-27 21:13:20 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.28it/s]\u001b[A\n",
      "  2%|▏         | 326/16538 [00:56<51:30,  5.25it/s][NeMo W 2025-03-27 21:13:20 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.33it/s]\u001b[A\n",
      "  2%|▏         | 327/16538 [00:56<49:25,  5.47it/s][NeMo W 2025-03-27 21:13:20 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.47it/s]\n",
      "  2%|▏         | 328/16538 [00:56<46:21,  5.83it/s][NeMo W 2025-03-27 21:13:20 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.68it/s]\u001b[A\n",
      "  2%|▏         | 329/16538 [00:56<49:07,  5.50it/s][NeMo W 2025-03-27 21:13:21 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.24it/s]\n",
      "  2%|▏         | 330/16538 [00:57<46:27,  5.82it/s][NeMo W 2025-03-27 21:13:21 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.17it/s]\u001b[A\n",
      "  2%|▏         | 331/16538 [00:57<47:48,  5.65it/s][NeMo W 2025-03-27 21:13:21 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.58it/s]\n",
      "  2%|▏         | 332/16538 [00:57<45:23,  5.95it/s][NeMo W 2025-03-27 21:13:21 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.18it/s]\u001b[A\n",
      "  2%|▏         | 333/16538 [00:57<47:06,  5.73it/s][NeMo W 2025-03-27 21:13:21 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.24it/s]\n",
      "  2%|▏         | 334/16538 [00:57<45:00,  6.00it/s][NeMo W 2025-03-27 21:13:22 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.45it/s]\u001b[A\n",
      "  2%|▏         | 335/16538 [00:58<52:05,  5.18it/s][NeMo W 2025-03-27 21:13:22 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.64it/s]\n",
      "  2%|▏         | 336/16538 [00:58<48:12,  5.60it/s][NeMo W 2025-03-27 21:13:22 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.15it/s]\n",
      "  2%|▏         | 337/16538 [00:58<46:00,  5.87it/s][NeMo W 2025-03-27 21:13:22 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.38it/s]\n",
      "  2%|▏         | 338/16538 [00:58<44:08,  6.12it/s][NeMo W 2025-03-27 21:13:22 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.22it/s]\u001b[A\n",
      "  2%|▏         | 339/16538 [00:58<46:10,  5.85it/s][NeMo W 2025-03-27 21:13:22 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.53it/s]\u001b[A\n",
      "  2%|▏         | 340/16538 [00:58<45:17,  5.96it/s][NeMo W 2025-03-27 21:13:23 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.43it/s]\u001b[A\n",
      "  2%|▏         | 341/16538 [00:59<44:55,  6.01it/s][NeMo W 2025-03-27 21:13:23 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.16it/s]\n",
      "  2%|▏         | 342/16538 [00:59<43:33,  6.20it/s][NeMo W 2025-03-27 21:13:23 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.73it/s]\u001b[A\n",
      "  2%|▏         | 343/16538 [00:59<46:51,  5.76it/s][NeMo W 2025-03-27 21:13:23 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.06it/s]\u001b[A\n",
      "  2%|▏         | 344/16538 [00:59<48:22,  5.58it/s][NeMo W 2025-03-27 21:13:23 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.40it/s]\u001b[A\n",
      "  2%|▏         | 345/16538 [00:59<47:04,  5.73it/s][NeMo W 2025-03-27 21:13:23 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.77it/s]\u001b[A\n",
      "  2%|▏         | 346/16538 [00:59<45:31,  5.93it/s][NeMo W 2025-03-27 21:13:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.37it/s]\u001b[A\n",
      "  2%|▏         | 347/16538 [01:00<49:58,  5.40it/s][NeMo W 2025-03-27 21:13:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.19it/s]\u001b[A\n",
      "  2%|▏         | 348/16538 [01:00<51:56,  5.20it/s][NeMo W 2025-03-27 21:13:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.83it/s]\u001b[A\n",
      "  2%|▏         | 349/16538 [01:00<52:27,  5.14it/s][NeMo W 2025-03-27 21:13:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.33it/s]\u001b[A\n",
      "  2%|▏         | 350/16538 [01:00<51:43,  5.22it/s][NeMo W 2025-03-27 21:13:24 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.64it/s]\n",
      "  2%|▏         | 351/16538 [01:00<47:48,  5.64it/s][NeMo W 2025-03-27 21:13:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  2.75it/s]\u001b[A\n",
      "  2%|▏         | 352/16538 [01:01<1:06:12,  4.07it/s][NeMo W 2025-03-27 21:13:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.28it/s]\u001b[A\n",
      "  2%|▏         | 353/16538 [01:01<1:01:32,  4.38it/s][NeMo W 2025-03-27 21:13:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.34it/s]\u001b[A\n",
      "  2%|▏         | 354/16538 [01:01<58:04,  4.64it/s]  [NeMo W 2025-03-27 21:13:25 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.91it/s]\u001b[A\n",
      "  2%|▏         | 355/16538 [01:01<56:40,  4.76it/s][NeMo W 2025-03-27 21:13:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.86it/s]\u001b[A\n",
      "  2%|▏         | 356/16538 [01:02<58:28,  4.61it/s][NeMo W 2025-03-27 21:13:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.01it/s]\u001b[A\n",
      "  2%|▏         | 357/16538 [01:02<58:03,  4.65it/s][NeMo W 2025-03-27 21:13:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.21it/s]\u001b[A\n",
      "  2%|▏         | 358/16538 [01:02<58:45,  4.59it/s][NeMo W 2025-03-27 21:13:26 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  2.28it/s]\u001b[A\n",
      "  2%|▏         | 359/16538 [01:02<1:18:45,  3.42it/s][NeMo W 2025-03-27 21:13:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.41it/s]\u001b[A\n",
      "  2%|▏         | 360/16538 [01:03<1:12:16,  3.73it/s][NeMo W 2025-03-27 21:13:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.10it/s]\u001b[A\n",
      "  2%|▏         | 361/16538 [01:03<1:08:48,  3.92it/s][NeMo W 2025-03-27 21:13:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.42it/s]\u001b[A\n",
      "  2%|▏         | 362/16538 [01:03<1:05:30,  4.12it/s][NeMo W 2025-03-27 21:13:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.72it/s]\u001b[A\n",
      "  2%|▏         | 363/16538 [01:03<1:00:02,  4.49it/s][NeMo W 2025-03-27 21:13:27 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  1.80it/s]\u001b[A\n",
      "  2%|▏         | 364/16538 [01:04<1:29:52,  3.00it/s][NeMo W 2025-03-27 21:13:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  3.46it/s]\u001b[A\n",
      "  2%|▏         | 365/16538 [01:04<1:28:30,  3.05it/s][NeMo W 2025-03-27 21:13:28 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.70it/s]\u001b[A\n",
      "  2%|▏         | 366/16538 [01:04<1:21:21,  3.31it/s][NeMo W 2025-03-27 21:13:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  2.72it/s]\u001b[A\n",
      "  2%|▏         | 367/16538 [01:05<1:28:54,  3.03it/s][NeMo W 2025-03-27 21:13:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  2.84it/s]\u001b[A\n",
      "  2%|▏         | 368/16538 [01:05<1:32:58,  2.90it/s][NeMo W 2025-03-27 21:13:29 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.96it/s]\u001b[A\n",
      "  2%|▏         | 369/16538 [01:05<1:19:04,  3.41it/s][NeMo W 2025-03-27 21:13:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.25it/s]\u001b[A\n",
      "  2%|▏         | 370/16538 [01:06<1:10:28,  3.82it/s][NeMo W 2025-03-27 21:13:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  3.50it/s]\u001b[A\n",
      "  2%|▏         | 371/16538 [01:06<1:14:38,  3.61it/s][NeMo W 2025-03-27 21:13:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.77it/s]\u001b[A\n",
      "  2%|▏         | 372/16538 [01:06<1:04:49,  4.16it/s][NeMo W 2025-03-27 21:13:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.66it/s]\u001b[A\n",
      "  2%|▏         | 373/16538 [01:06<1:04:57,  4.15it/s][NeMo W 2025-03-27 21:13:30 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.94it/s]\n",
      "  2%|▏         | 374/16538 [01:06<56:49,  4.74it/s]  [NeMo W 2025-03-27 21:13:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.91it/s]\u001b[A\n",
      "  2%|▏         | 375/16538 [01:07<55:42,  4.84it/s][NeMo W 2025-03-27 21:13:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.63it/s]\u001b[A\n",
      "  2%|▏         | 376/16538 [01:07<55:35,  4.85it/s][NeMo W 2025-03-27 21:13:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.21it/s]\n",
      "  2%|▏         | 377/16538 [01:07<50:57,  5.29it/s][NeMo W 2025-03-27 21:13:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.67it/s]\n",
      "  2%|▏         | 378/16538 [01:07<47:09,  5.71it/s][NeMo W 2025-03-27 21:13:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.02it/s]\u001b[A\n",
      "  2%|▏         | 379/16538 [01:07<48:38,  5.54it/s][NeMo W 2025-03-27 21:13:31 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.90it/s]\n",
      "  2%|▏         | 380/16538 [01:07<45:19,  5.94it/s][NeMo W 2025-03-27 21:13:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.24it/s]\u001b[A\n",
      "  2%|▏         | 381/16538 [01:08<49:20,  5.46it/s][NeMo W 2025-03-27 21:13:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.37it/s]\u001b[A\n",
      "  2%|▏         | 382/16538 [01:08<51:45,  5.20it/s][NeMo W 2025-03-27 21:13:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.87it/s]\u001b[A\n",
      "  2%|▏         | 383/16538 [01:08<48:38,  5.53it/s][NeMo W 2025-03-27 21:13:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.79it/s]\n",
      "  2%|▏         | 384/16538 [01:08<45:48,  5.88it/s][NeMo W 2025-03-27 21:13:32 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.38it/s]\u001b[A\n",
      "  2%|▏         | 385/16538 [01:08<46:54,  5.74it/s][NeMo W 2025-03-27 21:13:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.45it/s]\n",
      "  2%|▏         | 386/16538 [01:08<44:42,  6.02it/s][NeMo W 2025-03-27 21:13:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.94it/s]\u001b[A\n",
      "  2%|▏         | 387/16538 [01:09<45:08,  5.96it/s][NeMo W 2025-03-27 21:13:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.44it/s]\u001b[A\n",
      "  2%|▏         | 388/16538 [01:09<46:18,  5.81it/s][NeMo W 2025-03-27 21:13:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.22it/s]\u001b[A\n",
      "  2%|▏         | 389/16538 [01:09<50:01,  5.38it/s][NeMo W 2025-03-27 21:13:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.25it/s]\n",
      "  2%|▏         | 390/16538 [01:09<46:57,  5.73it/s][NeMo W 2025-03-27 21:13:33 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.72it/s]\u001b[A\n",
      "  2%|▏         | 391/16538 [01:09<45:30,  5.91it/s][NeMo W 2025-03-27 21:13:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.03it/s]\u001b[A\n",
      "  2%|▏         | 392/16538 [01:10<45:38,  5.90it/s][NeMo W 2025-03-27 21:13:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.70it/s]\u001b[A\n",
      "  2%|▏         | 393/16538 [01:10<48:15,  5.58it/s][NeMo W 2025-03-27 21:13:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.15it/s]\u001b[A\n",
      "  2%|▏         | 394/16538 [01:10<51:35,  5.21it/s][NeMo W 2025-03-27 21:13:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.74it/s]\u001b[A\n",
      "  2%|▏         | 395/16538 [01:10<48:47,  5.51it/s][NeMo W 2025-03-27 21:13:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.60it/s]\n",
      "  2%|▏         | 396/16538 [01:10<45:42,  5.89it/s][NeMo W 2025-03-27 21:13:34 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.13it/s]\u001b[A\n",
      "  2%|▏         | 397/16538 [01:10<45:28,  5.92it/s][NeMo W 2025-03-27 21:13:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.14it/s]\u001b[A\n",
      "  2%|▏         | 398/16538 [01:11<45:27,  5.92it/s][NeMo W 2025-03-27 21:13:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.86it/s]\u001b[A\n",
      "  2%|▏         | 399/16538 [01:11<50:35,  5.32it/s][NeMo W 2025-03-27 21:13:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.99it/s]\u001b[A\n",
      "  2%|▏         | 400/16538 [01:11<49:05,  5.48it/s][NeMo W 2025-03-27 21:13:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.38it/s]\u001b[A\n",
      "  2%|▏         | 401/16538 [01:11<55:00,  4.89it/s][NeMo W 2025-03-27 21:13:35 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.30it/s]\u001b[A\n",
      "  2%|▏         | 402/16538 [01:11<53:31,  5.02it/s][NeMo W 2025-03-27 21:13:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.76it/s]\u001b[A\n",
      "  2%|▏         | 403/16538 [01:12<51:40,  5.20it/s][NeMo W 2025-03-27 21:13:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.34it/s]\u001b[A\n",
      "  2%|▏         | 404/16538 [01:12<53:25,  5.03it/s][NeMo W 2025-03-27 21:13:36 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  1.43it/s]\u001b[A\n",
      "  2%|▏         | 405/16538 [01:13<1:36:04,  2.80it/s][NeMo W 2025-03-27 21:13:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.40it/s]\n",
      "  2%|▏         | 406/16538 [01:13<1:19:10,  3.40it/s][NeMo W 2025-03-27 21:13:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.09it/s]\u001b[A\n",
      "  2%|▏         | 407/16538 [01:13<1:09:01,  3.89it/s][NeMo W 2025-03-27 21:13:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.26it/s]\u001b[A\n",
      "  2%|▏         | 408/16538 [01:13<1:01:39,  4.36it/s][NeMo W 2025-03-27 21:13:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.35it/s]\u001b[A\n",
      "  2%|▏         | 409/16538 [01:13<56:15,  4.78it/s]  [NeMo W 2025-03-27 21:13:37 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  3.39it/s]\u001b[A\n",
      "  2%|▏         | 410/16538 [01:14<1:05:24,  4.11it/s][NeMo W 2025-03-27 21:13:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.20it/s]\u001b[A\n",
      "  2%|▏         | 411/16538 [01:14<1:00:59,  4.41it/s][NeMo W 2025-03-27 21:13:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.55it/s]\u001b[A\n",
      "  2%|▏         | 412/16538 [01:14<57:11,  4.70it/s]  [NeMo W 2025-03-27 21:13:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.31it/s]\n",
      "  2%|▏         | 413/16538 [01:14<51:54,  5.18it/s][NeMo W 2025-03-27 21:13:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.61it/s]\u001b[A\n",
      "  3%|▎         | 414/16538 [01:14<50:43,  5.30it/s][NeMo W 2025-03-27 21:13:38 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.39it/s]\n",
      "  3%|▎         | 415/16538 [01:14<47:19,  5.68it/s][NeMo W 2025-03-27 21:13:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.62it/s]\n",
      "  3%|▎         | 416/16538 [01:15<44:42,  6.01it/s][NeMo W 2025-03-27 21:13:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.56it/s]\n",
      "  3%|▎         | 417/16538 [01:15<43:05,  6.24it/s][NeMo W 2025-03-27 21:13:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.65it/s]\u001b[A\n",
      "  3%|▎         | 418/16538 [01:15<46:39,  5.76it/s][NeMo W 2025-03-27 21:13:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.72it/s]\n",
      "  3%|▎         | 419/16538 [01:15<44:08,  6.09it/s][NeMo W 2025-03-27 21:13:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  5.32it/s]\u001b[A\n",
      "  3%|▎         | 420/16538 [01:15<48:12,  5.57it/s][NeMo W 2025-03-27 21:13:39 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  2.35it/s]\u001b[A\n",
      "  3%|▎         | 421/16538 [01:16<1:10:14,  3.82it/s][NeMo W 2025-03-27 21:13:40 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.99it/s]\u001b[A\n",
      "  3%|▎         | 422/16538 [01:16<1:07:30,  3.98it/s][NeMo W 2025-03-27 21:13:40 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.34it/s]\u001b[A\n",
      "  3%|▎         | 423/16538 [01:16<1:08:21,  3.93it/s][NeMo W 2025-03-27 21:13:40 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.30it/s]\n",
      "  3%|▎         | 424/16538 [01:16<1:00:06,  4.47it/s][NeMo W 2025-03-27 21:13:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.43it/s]\n",
      "  3%|▎         | 425/16538 [01:16<54:03,  4.97it/s]  [NeMo W 2025-03-27 21:13:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  4.51it/s]\u001b[A\n",
      "  3%|▎         | 426/16538 [01:17<58:00,  4.63it/s][NeMo W 2025-03-27 21:13:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.72it/s]\u001b[A\n",
      "  3%|▎         | 427/16538 [01:17<53:26,  5.02it/s][NeMo W 2025-03-27 21:13:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.53it/s]\u001b[A\n",
      "  3%|▎         | 428/16538 [01:17<50:21,  5.33it/s][NeMo W 2025-03-27 21:13:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.48it/s]\n",
      "  3%|▎         | 429/16538 [01:17<47:21,  5.67it/s][NeMo W 2025-03-27 21:13:41 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.48it/s]\u001b[A\n",
      "  3%|▎         | 430/16538 [01:17<46:04,  5.83it/s][NeMo W 2025-03-27 21:13:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.17it/s]\u001b[A\n",
      "  3%|▎         | 431/16538 [01:18<45:38,  5.88it/s][NeMo W 2025-03-27 21:13:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.75it/s]\n",
      "  3%|▎         | 432/16538 [01:18<43:19,  6.20it/s][NeMo W 2025-03-27 21:13:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.69it/s]\u001b[A\n",
      "  3%|▎         | 433/16538 [01:18<42:58,  6.25it/s][NeMo W 2025-03-27 21:13:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  8.37it/s]\n",
      "  3%|▎         | 434/16538 [01:18<41:55,  6.40it/s][NeMo W 2025-03-27 21:13:42 dataloader:230] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process, possibly impacting the training speed if your tokenizer is very large. If the impact is noticable, set pretokenize=False in dataloader config. (note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  6.02it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# === Silence warnings and NeMo logs ===\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('nemo').setLevel(logging.ERROR)\n",
    "\n",
    "# === Paths ===\n",
    "model_path = 'canary-180m-flash/canary-180m-flash.nemo'\n",
    "csv_path = '../../TORGO_CLEANED.csv'\n",
    "results_dir = 'results'\n",
    "checkpoint_path = os.path.join(results_dir, 'checkpoint.csv')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# === Load model ===\n",
    "model = EncDecMultiTaskModel.restore_from(restore_path=model_path)\n",
    "model.eval()\n",
    "\n",
    "# === Load dataset ===\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# === Load checkpoint if available ===\n",
    "if os.path.exists(checkpoint_path):\n",
    "    df_checkpoint = pd.read_csv(checkpoint_path)\n",
    "    processed_paths = set(df_checkpoint['wav'])\n",
    "    transcriptions = df_checkpoint['prediction'].tolist()\n",
    "    references = df_checkpoint['reference'].tolist()\n",
    "    speakers = df_checkpoint['spk_id'].tolist()\n",
    "else:\n",
    "    df_checkpoint = pd.DataFrame(columns=['wav', 'spk_id', 'prediction', 'reference'])\n",
    "    processed_paths = set()\n",
    "    transcriptions = []\n",
    "    references = []\n",
    "    speakers = []\n",
    "\n",
    "# === Inference with checkpointing ===\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if row['wav'] in processed_paths:\n",
    "        continue\n",
    "\n",
    "    audio_path = os.path.join('../../', row['wav'])\n",
    "    ref = str(row['wrd']).lower()\n",
    "    spk = row['spk_id']\n",
    "\n",
    "    if not os.path.exists(audio_path):\n",
    "        pred = ''\n",
    "    else:\n",
    "        result = model.transcribe([audio_path])[0]\n",
    "\n",
    "        # Clean transcription\n",
    "        pred = re.sub(r'\\[[^\\]]*\\]', '', result)\n",
    "        pred = pred.lower()\n",
    "        pred = re.sub(r\"[^a-z0-9\\s']\", '', pred)\n",
    "        pred = pred.strip()\n",
    "\n",
    "    # Store results\n",
    "    transcriptions.append(pred)\n",
    "    references.append(ref)\n",
    "    speakers.append(spk)\n",
    "    new_row = pd.DataFrame([{\n",
    "        'wav': row['wav'],\n",
    "        'spk_id': spk,\n",
    "        'prediction': pred,\n",
    "        'reference': ref\n",
    "    }])\n",
    "    df_checkpoint = pd.concat([df_checkpoint, new_row], ignore_index=True)\n",
    "\n",
    "    # Save checkpoint\n",
    "    df_checkpoint.to_csv(checkpoint_path, index=False)\n",
    "\n",
    "# === Merge results with original metadata ===\n",
    "df_result = df.merge(df_checkpoint, on=['wav', 'spk_id'])\n",
    "df_result = df_result[df_result['prediction'].str.strip() != '']\n",
    "df_result = df_result[['ID', 'duration', 'wav', 'spk_id', 'prediction', 'reference']]\n",
    "df_result.to_csv(os.path.join(results_dir, 'transcription_results.csv'), index=False)\n",
    "\n",
    "# === Compute WERs ===\n",
    "wer_per_spk = (\n",
    "    df_result.groupby('spk_id')\n",
    "    .apply(lambda g: pd.Series({\n",
    "        'wer': word_error_rate(\n",
    "            hypotheses=g['prediction'].tolist(),\n",
    "            references=g['reference'].tolist()\n",
    "        )\n",
    "    }))\n",
    "    .reset_index()\n",
    ")\n",
    "wer_per_spk.to_csv(os.path.join(results_dir, 'wer_per_speaker.csv'), index=False)\n",
    "\n",
    "total_wer = word_error_rate(\n",
    "    hypotheses=df_result['prediction'].tolist(),\n",
    "    references=df_result['reference'].tolist()\n",
    ")\n",
    "\n",
    "# === Save WER summary ===\n",
    "summary_path = os.path.join(results_dir, 'wer_summary.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(f'Total WER: {total_wer:.4f}\\n\\n')\n",
    "    f.write('WER per speaker:\\n')\n",
    "    for _, row in wer_per_spk.iterrows():\n",
    "        f.write(f'{row[\"spk_id\"]}: {row[\"wer\"]:.4f}\\n')\n",
    "\n",
    "print(f\"Total WER: {total_wer:.4f}\")\n",
    "print(f\"Results saved in: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
