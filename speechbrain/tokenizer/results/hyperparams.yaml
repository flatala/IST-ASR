# Generated 2025-03-11 from:
# /scratch/flatala/tokenizer/tokenizer.yaml
# yamllint disable
# ############################################################################
# Tokenizer: subword BPE with unigram 1K
# Training: Librispeech 960h
# Authors:  Abdel Heba 2021
# ############################################################################

output_folder: results/
dataset_folder: datasets/
# train_log: !ref <output_folder>/train_log.txt

# Data files
train_csv: !ref <dataset_folder>/train.csv
valid_csv: !ref <dataset_folder>/val.csv

####################### Training Parameters ####################################
token_type: unigram  # ["unigram", "bpe", "char"]
token_output: 1000  # index(blank/eos/bos/unk) = 0
character_coverage: 1.0
csv_read: wrd
bos_id: 1
eos_id: 2

tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
  bos_id: 1
  eos_id: 2
  model_dir: !ref <output_folder>
  vocab_size: 1000
  annotation_train: !ref <train_csv>
  annotation_read: !ref <csv_read>
  model_type: unigram            # ["unigram", "bpe", "char"]
  character_coverage: 1.0
  annotation_list_to_check: [!ref <train_csv>, !ref <valid_csv>]
