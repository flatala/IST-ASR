# Generated 2025-03-13 from:
# /scratch/flatala/speechbrain/tokenizer/tokenizer.yaml
# yamllint disable
# ############################################################################
# Tokenizer: subword BPE with unigram 1K
# Training: Librispeech 960h
# Authors:  Abdel Heba 2021
# ############################################################################

output_folder: results_2/
# train_log: !ref <output_folder>/train_log.txt

# Data files
train_csv: ../datasets/train.csv
valid_csv: ../datasets/val.csv

####################### Training Parameters ####################################
token_type: unigram  # ["unigram", "bpe", "char"]
token_output: 1000  # index(blank/eos/bos/unk) = 0
character_coverage: 1.0
csv_read: wrd
bos_id: 1
eos_id: 2

tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
  bos_id: 1
  eos_id: 2
  model_dir: results_2/
  vocab_size: 1000
  annotation_train: ../datasets/train.csv
  annotation_read: wrd
  model_type: unigram            # ["unigram", "bpe", "char"]
  character_coverage: 1.0
  annotation_list_to_check: [../datasets/train.csv, ../datasets/val.csv]
