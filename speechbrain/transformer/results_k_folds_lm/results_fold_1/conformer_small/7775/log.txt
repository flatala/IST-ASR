2025-03-20 17:23:02,685 - speechbrain.utils.quirks - INFO - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
2025-03-20 17:23:02,805 - speechbrain.utils.quirks - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
2025-03-20 17:23:02,805 - speechbrain.core - INFO - Beginning experiment!
2025-03-20 17:23:02,805 - speechbrain.core - INFO - Experiment folder: results_fold_1/conformer_small/7775
2025-03-20 17:23:15,163 - speechbrain.utils.superpowers - DEBUG - aiohappyeyeballs==2.6.1
aiohttp==3.11.13
aiosignal==1.3.2
anyio==4.8.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==3.0.0
async-lru==2.0.4
async-timeout==5.0.1
attrs==24.2.0
babel==2.17.0
backcall==0.2.0
beautifulsoup4==4.12.3
bleach==6.2.0
Brotli @ file:///home/conda/feedstock_root/build_artifacts/brotli-split_1648883617327/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2024.8.30
cffi==1.17.1
charset-normalizer==3.4.0
comm==0.2.2
contourpy==1.3.0
cycler==0.12.1
datasets==3.3.2
debugpy==1.8.13
decorator==5.1.1
defusedxml==0.7.1
dill==0.3.8
docopt==0.6.2
exceptiongroup==1.2.2
executing==2.1.0
fastjsonschema==2.21.1
filelock==3.16.1
fonttools==4.56.0
fqdn==1.5.1
frozenlist==1.5.0
fsspec==2024.10.0
h11==0.14.0
h2 @ file:///home/conda/feedstock_root/build_artifacts/h2_1738578511449/work
hpack @ file:///home/conda/feedstock_root/build_artifacts/hpack_1737618293087/work
httpcore==1.0.7
httpx==0.28.1
huggingface-hub==0.26.2
hyperframe @ file:///home/conda/feedstock_root/build_artifacts/hyperframe_1737618333194/work
HyperPyYAML==1.2.2
idna==3.10
importlib_metadata==8.5.0
importlib_resources==6.5.2
ipykernel==6.29.5
ipython==8.12.3
isoduration==20.11.0
jedi==0.19.2
Jinja2==3.1.4
joblib==1.4.2
json5==0.10.0
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
jupyter-events==0.12.0
jupyter-lsp==2.2.5
jupyter_client==8.6.3
jupyter_core==5.7.2
jupyter_server==2.15.0
jupyter_server_terminals==0.5.3
jupyterlab==4.3.5
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
kiwisolver==1.4.7
MarkupSafe==3.0.2
matplotlib==3.9.4
matplotlib-inline==0.1.7
mistune==3.0.2
mpmath==1.3.0
multidict==6.1.0
multiprocess==0.70.16
nbclient==0.10.1
nbconvert==7.16.4
nbformat==5.10.4
nest-asyncio==1.6.0
networkx==3.2.1
notebook_shim==0.2.4
numpy==2.0.2
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-cusparselt-cu12==0.6.2
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.4.127
overrides==7.7.0
packaging==24.1
pandas==2.2.3
pandocfilters==1.5.1
parso==0.8.4
pexpect==4.9.0
pickleshare==0.7.5
pillow==11.1.0
pipreqs==0.5.0
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1733344503739/work
platformdirs==4.3.6
prometheus_client==0.21.1
prompt_toolkit==3.0.48
propcache==0.3.0
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==19.0.1
pycparser==2.22
Pygments==2.18.0
pyparsing==3.2.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1733217236728/work
python-dateutil==2.9.0.post0
python-json-logger==3.2.1
pytz==2025.1
PyYAML==6.0.2
pyzmq==26.2.0
referencing==0.35.1
requests==2.32.3
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.22.3
ruamel.yaml==0.18.6
ruamel.yaml.clib==0.2.12
scikit-learn==1.5.2
scipy==1.13.1
Send2Trash==1.8.3
sentencepiece==0.2.0
six==1.17.0
sniffio==1.3.1
soundfile==0.12.1
soupsieve==2.6
sox==1.5.0
speechbrain==1.0.2
stack-data==0.6.3
sympy==1.13.1
terminado==0.18.1
threadpoolctl==3.5.0
tinycss2==1.4.0
tomli==2.2.1
torch==2.5.1
torchaudio==2.5.1
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
triton==3.1.0
types-python-dateutil==2.9.0.20241206
typing_extensions==4.12.2
typing_utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1733331286120/work
tzdata==2025.1
uri-template==1.3.0
urllib3==2.2.3
wcwidth==0.2.13
webcolors==24.11.1
webencodings==0.5.1
websocket-client==1.8.0
xxhash==3.5.0
yarg==0.1.9
yarl==1.18.3
zipp==3.21.0


2025-03-20 17:23:15,204 - speechbrain.utils.superpowers - DEBUG - 2512280


2025-03-20 17:23:15,461 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results_fold_1/conformer_small/7775/save.
2025-03-20 17:23:15,462 - speechbrain.utils.fetching - DEBUG - Fetch: Local file found, creating symlink '/scratch/flatala/speechbrain/transformer/LM_weights_13_epochs/lm.ckpt' -> '/scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt'
2025-03-20 17:23:15,463 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["lm"] = /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt
2025-03-20 17:23:15,463 - speechbrain.utils.fetching - DEBUG - Fetch: Local file found, creating symlink '/scratch/flatala/speechbrain/transformer/LM_weights_13_epochs/tokenizer.ckpt' -> '/scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/tokenizer.ckpt'
2025-03-20 17:23:15,510 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["tokenizer"] = /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/tokenizer.ckpt
2025-03-20 17:23:15,511 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2025-03-20 17:23:15,511 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): lm -> /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt
2025-03-20 17:23:15,511 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): tokenizer -> /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/tokenizer.ckpt
2025-03-20 17:23:15,954 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: positional_encoding.pe
2025-03-20 17:23:15,954 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_weight
2025-03-20 17:23:15,955 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_bias
2025-03-20 17:23:15,955 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.weight
2025-03-20 17:23:15,956 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.bias
2025-03-20 17:23:15,956 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.weight
2025-03-20 17:23:15,957 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.bias
2025-03-20 17:23:15,957 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.weight
2025-03-20 17:23:15,958 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.bias
2025-03-20 17:23:15,958 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.weight
2025-03-20 17:23:15,959 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.bias
2025-03-20 17:23:15,959 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.weight
2025-03-20 17:23:15,960 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.bias
2025-03-20 17:23:15,961 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_weight
2025-03-20 17:23:15,961 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_bias
2025-03-20 17:23:15,962 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.weight
2025-03-20 17:23:15,962 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.bias
2025-03-20 17:23:15,963 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.weight
2025-03-20 17:23:15,963 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.bias
2025-03-20 17:23:15,964 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.weight
2025-03-20 17:23:15,964 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.bias
2025-03-20 17:23:15,965 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.weight
2025-03-20 17:23:15,965 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.bias
2025-03-20 17:23:15,966 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.weight
2025-03-20 17:23:15,966 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.bias
2025-03-20 17:23:15,967 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_weight
2025-03-20 17:23:15,967 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_bias
2025-03-20 17:23:15,968 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.weight
2025-03-20 17:23:15,968 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.bias
2025-03-20 17:23:15,969 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.weight
2025-03-20 17:23:15,969 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.bias
2025-03-20 17:23:15,970 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.weight
2025-03-20 17:23:15,970 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.bias
2025-03-20 17:23:15,970 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.weight
2025-03-20 17:23:15,971 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.bias
2025-03-20 17:23:15,971 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.weight
2025-03-20 17:23:15,972 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.bias
2025-03-20 17:23:15,972 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_weight
2025-03-20 17:23:15,973 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_bias
2025-03-20 17:23:15,973 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.weight
2025-03-20 17:23:15,974 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.bias
2025-03-20 17:23:15,974 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.weight
2025-03-20 17:23:15,975 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.bias
2025-03-20 17:23:15,975 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.weight
2025-03-20 17:23:15,976 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.bias
2025-03-20 17:23:15,976 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.weight
2025-03-20 17:23:15,977 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.bias
2025-03-20 17:23:15,977 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.weight
2025-03-20 17:23:15,978 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.bias
2025-03-20 17:23:15,978 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_weight
2025-03-20 17:23:15,979 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_bias
2025-03-20 17:23:15,979 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.weight
2025-03-20 17:23:15,980 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.bias
2025-03-20 17:23:15,980 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.weight
2025-03-20 17:23:15,980 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.bias
2025-03-20 17:23:15,981 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.weight
2025-03-20 17:23:15,981 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.bias
2025-03-20 17:23:15,982 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.weight
2025-03-20 17:23:15,982 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.bias
2025-03-20 17:23:15,983 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.weight
2025-03-20 17:23:15,983 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.bias
2025-03-20 17:23:15,984 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_weight
2025-03-20 17:23:15,984 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_bias
2025-03-20 17:23:15,985 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.weight
2025-03-20 17:23:15,985 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.bias
2025-03-20 17:23:15,986 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.weight
2025-03-20 17:23:15,986 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.bias
2025-03-20 17:23:15,987 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.weight
2025-03-20 17:23:15,987 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.bias
2025-03-20 17:23:15,987 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.weight
2025-03-20 17:23:15,988 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.bias
2025-03-20 17:23:15,988 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.weight
2025-03-20 17:23:15,989 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.bias
2025-03-20 17:23:15,989 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_weight
2025-03-20 17:23:15,990 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_bias
2025-03-20 17:23:15,990 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.weight
2025-03-20 17:23:15,991 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.bias
2025-03-20 17:23:15,991 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.weight
2025-03-20 17:23:15,992 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.bias
2025-03-20 17:23:15,992 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.weight
2025-03-20 17:23:15,993 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.bias
2025-03-20 17:23:15,993 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.weight
2025-03-20 17:23:15,994 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.bias
2025-03-20 17:23:15,994 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.weight
2025-03-20 17:23:15,995 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.bias
2025-03-20 17:23:15,995 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_weight
2025-03-20 17:23:15,995 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_bias
2025-03-20 17:23:15,996 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.weight
2025-03-20 17:23:15,996 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.bias
2025-03-20 17:23:15,997 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.weight
2025-03-20 17:23:15,997 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.bias
2025-03-20 17:23:15,998 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.weight
2025-03-20 17:23:15,998 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.bias
2025-03-20 17:23:15,999 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.weight
2025-03-20 17:23:15,999 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.bias
2025-03-20 17:23:16,000 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.weight
2025-03-20 17:23:16,000 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.bias
2025-03-20 17:23:16,001 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_weight
2025-03-20 17:23:16,001 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_bias
2025-03-20 17:23:16,002 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.weight
2025-03-20 17:23:16,002 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.bias
2025-03-20 17:23:16,003 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.weight
2025-03-20 17:23:16,003 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.bias
2025-03-20 17:23:16,003 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.weight
2025-03-20 17:23:16,004 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.bias
2025-03-20 17:23:16,004 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.weight
2025-03-20 17:23:16,005 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.bias
2025-03-20 17:23:16,005 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.weight
2025-03-20 17:23:16,006 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.bias
2025-03-20 17:23:16,006 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_weight
2025-03-20 17:23:16,007 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_bias
2025-03-20 17:23:16,007 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.weight
2025-03-20 17:23:16,008 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.bias
2025-03-20 17:23:16,008 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.weight
2025-03-20 17:23:16,009 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.bias
2025-03-20 17:23:16,009 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.weight
2025-03-20 17:23:16,009 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.bias
2025-03-20 17:23:16,010 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.weight
2025-03-20 17:23:16,010 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.bias
2025-03-20 17:23:16,011 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.weight
2025-03-20 17:23:16,011 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.bias
2025-03-20 17:23:16,012 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_weight
2025-03-20 17:23:16,012 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_bias
2025-03-20 17:23:16,013 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.weight
2025-03-20 17:23:16,013 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.bias
2025-03-20 17:23:16,014 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.weight
2025-03-20 17:23:16,014 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.bias
2025-03-20 17:23:16,015 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.weight
2025-03-20 17:23:16,015 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.bias
2025-03-20 17:23:16,016 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.weight
2025-03-20 17:23:16,016 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.bias
2025-03-20 17:23:16,017 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.weight
2025-03-20 17:23:16,017 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.bias
2025-03-20 17:23:16,018 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_weight
2025-03-20 17:23:16,018 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_bias
2025-03-20 17:23:16,019 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.weight
2025-03-20 17:23:16,019 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.bias
2025-03-20 17:23:16,019 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.weight
2025-03-20 17:23:16,020 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.bias
2025-03-20 17:23:16,020 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.weight
2025-03-20 17:23:16,021 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.bias
2025-03-20 17:23:16,021 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.weight
2025-03-20 17:23:16,022 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.bias
2025-03-20 17:23:16,022 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.weight
2025-03-20 17:23:16,023 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.bias
2025-03-20 17:23:16,023 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.weight
2025-03-20 17:23:16,024 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.bias
2025-03-20 17:23:16,024 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: custom_src_module.emb.Embedding.weight
2025-03-20 17:23:16,025 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.weight
2025-03-20 17:23:16,025 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.bias
2025-03-20 17:23:16,026 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.weight
2025-03-20 17:23:16,026 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.bias
2025-03-20 17:23:16,026 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.weight
2025-03-20 17:23:16,027 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.bias
2025-03-20 17:23:16,027 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: embedding.Embedding.weight
2025-03-20 17:23:16,028 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l0
2025-03-20 17:23:16,028 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l0
2025-03-20 17:23:16,029 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l0
2025-03-20 17:23:16,029 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l0
2025-03-20 17:23:16,030 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l1
2025-03-20 17:23:16,030 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l1
2025-03-20 17:23:16,031 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l1
2025-03-20 17:23:16,031 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l1
2025-03-20 17:23:16,032 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.weight
2025-03-20 17:23:16,032 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.bias
2025-03-20 17:23:16,033 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.weight
2025-03-20 17:23:16,033 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.bias
2025-03-20 17:23:16,034 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.weight
2025-03-20 17:23:16,034 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.bias
2025-03-20 17:23:16,087 - speechbrain.core - INFO - Info: precision arg from hparam file is used
2025-03-20 17:23:16,088 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2025-03-20 17:23:16,088 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2025-03-20 17:23:16,290 - speechbrain.core - INFO - Gradscaler enabled: True. Using precision: fp16.
2025-03-20 17:23:16,292 - speechbrain.core - INFO - ASR Model Statistics:
* Total Number of Trainable Parameters: 13.3M
* Total Number of Parameters: 13.3M
* Trainable Parameters represent 100.0000% of the total size.
2025-03-20 17:23:26,977 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2025-03-20 17:23:26,978 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2025-03-20 17:26:12,738 - speechbrain.utils.train_logger - INFO - epoch: 1, lr: 3.35e-05, steps: 838, optimizer: Adam - train loss: 82.30 - valid loss: 34.57, valid ACC: 1.88e-01
2025-03-20 17:26:13,415 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-26-12+00
2025-03-20 17:26:13,423 - speechbrain.utils.epoch_loop - INFO - Going into epoch 2
2025-03-20 17:28:40,254 - speechbrain.utils.train_logger - INFO - epoch: 2, lr: 6.70e-05, steps: 1676, optimizer: Adam - train loss: 28.56 - valid loss: 25.85, valid ACC: 2.64e-01
2025-03-20 17:28:41,072 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-28-40+00
2025-03-20 17:28:41,086 - speechbrain.utils.epoch_loop - INFO - Going into epoch 3
2025-03-20 17:31:07,533 - speechbrain.utils.train_logger - INFO - epoch: 3, lr: 1.01e-04, steps: 2514, optimizer: Adam - train loss: 23.76 - valid loss: 21.24, valid ACC: 4.04e-01
2025-03-20 17:31:08,438 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-31-07+00
2025-03-20 17:31:08,469 - speechbrain.utils.epoch_loop - INFO - Going into epoch 4
2025-03-20 17:33:35,101 - speechbrain.utils.train_logger - INFO - epoch: 4, lr: 1.34e-04, steps: 3352, optimizer: Adam - train loss: 19.88 - valid loss: 17.81, valid ACC: 5.50e-01
2025-03-20 17:33:36,015 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-33-35+00
2025-03-20 17:33:36,065 - speechbrain.utils.epoch_loop - INFO - Going into epoch 5
2025-03-20 17:35:57,185 - speechbrain.utils.train_logger - INFO - epoch: 5, lr: 1.68e-04, steps: 4190, optimizer: Adam - train loss: 16.64 - valid loss: 15.12, valid ACC: 6.41e-01
2025-03-20 17:35:57,527 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-35-57+00
2025-03-20 17:35:57,574 - speechbrain.utils.epoch_loop - INFO - Going into epoch 6
2025-03-20 17:38:20,453 - speechbrain.utils.train_logger - INFO - epoch: 6, lr: 2.01e-04, steps: 5028, optimizer: Adam - train loss: 14.46 - valid loss: 13.69, valid ACC: 6.84e-01
2025-03-20 17:38:20,896 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-38-20+00
2025-03-20 17:38:20,962 - speechbrain.utils.epoch_loop - INFO - Going into epoch 7
2025-03-20 17:40:45,231 - speechbrain.utils.train_logger - INFO - epoch: 7, lr: 2.35e-04, steps: 5866, optimizer: Adam - train loss: 13.32 - valid loss: 12.86, valid ACC: 7.10e-01
2025-03-20 17:40:45,593 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-40-45+00
2025-03-20 17:40:45,675 - speechbrain.utils.epoch_loop - INFO - Going into epoch 8
2025-03-20 17:43:19,421 - speechbrain.utils.train_logger - INFO - epoch: 8, lr: 2.68e-04, steps: 6704, optimizer: Adam - train loss: 12.70 - valid loss: 12.43, valid ACC: 7.23e-01
2025-03-20 17:43:25,892 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-43-19+00
2025-03-20 17:43:27,465 - speechbrain.utils.epoch_loop - INFO - Going into epoch 9
2025-03-20 17:45:49,997 - speechbrain.utils.train_logger - INFO - epoch: 9, lr: 3.02e-04, steps: 7542, optimizer: Adam - train loss: 12.33 - valid loss: 12.30, valid ACC: 7.30e-01
2025-03-20 17:45:50,392 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-45-49+00
2025-03-20 17:45:50,520 - speechbrain.utils.epoch_loop - INFO - Going into epoch 10
2025-03-20 17:52:44,712 - speechbrain.utils.train_logger - INFO - epoch: 10, lr: 3.35e-04, steps: 8380, optimizer: Adam - train loss: 12.09 - valid loss: 12.50, valid ACC: 7.33e-01, valid WER: 1.03e+02
2025-03-20 17:52:45,409 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-52-44+00
2025-03-20 17:52:46,980 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-52-44+00
2025-03-20 17:52:47,141 - speechbrain.dataio.dataloader - DEBUG - SaveableDataLoader was requested to load a checkpoint, but the DataLoader has already been iterated. The DataLoader file will be ignored. This is normal in evaluation, when a checkpoint is loaded just to retrieve the best model.
2025-03-20 18:17:54,247 - speechbrain.utils.train_logger - INFO - Epoch loaded: 10 - test loss: 14.46, test ACC: 6.83e-01, test WER: 1.07e+02
2025-03-20 18:17:54,693 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+18-17-54+00
2025-03-20 18:17:55,117 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-40-45+00
2025-03-20 18:17:55,118 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-28-40+00
2025-03-20 18:17:55,120 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-33-35+00
2025-03-20 18:17:55,121 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-38-20+00
2025-03-20 18:17:55,122 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-26-12+00
2025-03-20 18:17:55,124 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-31-07+00
2025-03-20 18:17:55,153 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-43-19+00
2025-03-20 18:17:55,155 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-35-57+00
2025-03-20 18:17:55,159 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-45-49+00
2025-03-20 18:17:55,160 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+17-52-44+00
2025-03-20 20:53:47,197 - speechbrain.utils.quirks - INFO - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
2025-03-20 20:53:47,211 - speechbrain.utils.quirks - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
2025-03-20 20:53:47,211 - speechbrain.core - INFO - Beginning experiment!
2025-03-20 20:53:47,212 - speechbrain.core - INFO - Experiment folder: results_fold_1/conformer_small/7775
2025-03-20 20:54:09,762 - speechbrain.utils.superpowers - DEBUG - aiohappyeyeballs==2.6.1
aiohttp==3.11.13
aiosignal==1.3.2
anyio==4.8.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==3.0.0
async-lru==2.0.4
async-timeout==5.0.1
attrs==24.2.0
babel==2.17.0
backcall==0.2.0
beautifulsoup4==4.12.3
bleach==6.2.0
Brotli @ file:///home/conda/feedstock_root/build_artifacts/brotli-split_1648883617327/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2024.8.30
cffi==1.17.1
charset-normalizer==3.4.0
comm==0.2.2
contourpy==1.3.0
cycler==0.12.1
datasets==3.3.2
debugpy==1.8.13
decorator==5.1.1
defusedxml==0.7.1
dill==0.3.8
docopt==0.6.2
exceptiongroup==1.2.2
executing==2.1.0
fastjsonschema==2.21.1
filelock==3.16.1
fonttools==4.56.0
fqdn==1.5.1
frozenlist==1.5.0
fsspec==2024.10.0
h11==0.14.0
h2 @ file:///home/conda/feedstock_root/build_artifacts/h2_1738578511449/work
hpack @ file:///home/conda/feedstock_root/build_artifacts/hpack_1737618293087/work
httpcore==1.0.7
httpx==0.28.1
huggingface-hub==0.26.2
hyperframe @ file:///home/conda/feedstock_root/build_artifacts/hyperframe_1737618333194/work
HyperPyYAML==1.2.2
idna==3.10
importlib_metadata==8.5.0
importlib_resources==6.5.2
ipykernel==6.29.5
ipython==8.12.3
isoduration==20.11.0
jedi==0.19.2
Jinja2==3.1.4
joblib==1.4.2
json5==0.10.0
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
jupyter-events==0.12.0
jupyter-lsp==2.2.5
jupyter_client==8.6.3
jupyter_core==5.7.2
jupyter_server==2.15.0
jupyter_server_terminals==0.5.3
jupyterlab==4.3.5
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
kiwisolver==1.4.7
MarkupSafe==3.0.2
matplotlib==3.9.4
matplotlib-inline==0.1.7
mistune==3.0.2
mpmath==1.3.0
multidict==6.1.0
multiprocess==0.70.16
nbclient==0.10.1
nbconvert==7.16.4
nbformat==5.10.4
nest-asyncio==1.6.0
networkx==3.2.1
notebook_shim==0.2.4
numpy==2.0.2
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-cusparselt-cu12==0.6.2
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.4.127
overrides==7.7.0
packaging==24.1
pandas==2.2.3
pandocfilters==1.5.1
parso==0.8.4
pexpect==4.9.0
pickleshare==0.7.5
pillow==11.1.0
pipreqs==0.5.0
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1733344503739/work
platformdirs==4.3.6
prometheus_client==0.21.1
prompt_toolkit==3.0.48
propcache==0.3.0
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==19.0.1
pycparser==2.22
Pygments==2.18.0
pyparsing==3.2.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1733217236728/work
python-dateutil==2.9.0.post0
python-json-logger==3.2.1
pytz==2025.1
PyYAML==6.0.2
pyzmq==26.2.0
referencing==0.35.1
requests==2.32.3
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.22.3
ruamel.yaml==0.18.6
ruamel.yaml.clib==0.2.12
scikit-learn==1.5.2
scipy==1.13.1
Send2Trash==1.8.3
sentencepiece==0.2.0
six==1.17.0
sniffio==1.3.1
soundfile==0.12.1
soupsieve==2.6
sox==1.5.0
speechbrain==1.0.2
stack-data==0.6.3
sympy==1.13.1
terminado==0.18.1
threadpoolctl==3.5.0
tinycss2==1.4.0
tomli==2.2.1
torch==2.5.1
torchaudio==2.5.1
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
triton==3.1.0
types-python-dateutil==2.9.0.20241206
typing_extensions==4.12.2
typing_utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1733331286120/work
tzdata==2025.1
uri-template==1.3.0
urllib3==2.2.3
wcwidth==0.2.13
webcolors==24.11.1
webencodings==0.5.1
websocket-client==1.8.0
xxhash==3.5.0
yarg==0.1.9
yarl==1.18.3
zipp==3.21.0


2025-03-20 20:54:10,622 - speechbrain.utils.superpowers - DEBUG - 2512280


2025-03-20 20:54:11,035 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results_fold_1/conformer_small/7775/save.
2025-03-20 20:54:11,273 - speechbrain.utils.fetching - INFO - Fetch lm.ckpt: Using symlink found at '/scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt'
2025-03-20 20:54:11,274 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["lm"] = /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt
2025-03-20 20:54:11,305 - speechbrain.utils.fetching - INFO - Fetch tokenizer.ckpt: Using symlink found at '/scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/tokenizer.ckpt'
2025-03-20 20:54:11,305 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["tokenizer"] = /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/tokenizer.ckpt
2025-03-20 20:54:11,305 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2025-03-20 20:54:11,305 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): lm -> /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt
2025-03-20 20:54:11,305 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): tokenizer -> /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/tokenizer.ckpt
2025-03-20 20:54:12,552 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: positional_encoding.pe
2025-03-20 20:54:12,553 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_weight
2025-03-20 20:54:12,553 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_bias
2025-03-20 20:54:12,554 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.weight
2025-03-20 20:54:12,555 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.bias
2025-03-20 20:54:12,555 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.weight
2025-03-20 20:54:12,556 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.bias
2025-03-20 20:54:12,557 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.weight
2025-03-20 20:54:12,557 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.bias
2025-03-20 20:54:12,558 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.weight
2025-03-20 20:54:12,559 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.bias
2025-03-20 20:54:12,559 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.weight
2025-03-20 20:54:12,560 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.bias
2025-03-20 20:54:12,560 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_weight
2025-03-20 20:54:12,561 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_bias
2025-03-20 20:54:12,562 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.weight
2025-03-20 20:54:12,562 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.bias
2025-03-20 20:54:12,563 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.weight
2025-03-20 20:54:12,564 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.bias
2025-03-20 20:54:12,565 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.weight
2025-03-20 20:54:12,565 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.bias
2025-03-20 20:54:12,566 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.weight
2025-03-20 20:54:12,567 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.bias
2025-03-20 20:54:12,567 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.weight
2025-03-20 20:54:12,568 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.bias
2025-03-20 20:54:12,568 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_weight
2025-03-20 20:54:12,569 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_bias
2025-03-20 20:54:12,570 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.weight
2025-03-20 20:54:12,571 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.bias
2025-03-20 20:54:12,571 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.weight
2025-03-20 20:54:12,572 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.bias
2025-03-20 20:54:12,572 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.weight
2025-03-20 20:54:12,573 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.bias
2025-03-20 20:54:12,573 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.weight
2025-03-20 20:54:12,574 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.bias
2025-03-20 20:54:12,574 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.weight
2025-03-20 20:54:12,575 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.bias
2025-03-20 20:54:12,575 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_weight
2025-03-20 20:54:12,576 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_bias
2025-03-20 20:54:12,576 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.weight
2025-03-20 20:54:12,577 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.bias
2025-03-20 20:54:12,578 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.weight
2025-03-20 20:54:12,578 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.bias
2025-03-20 20:54:12,579 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.weight
2025-03-20 20:54:12,580 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.bias
2025-03-20 20:54:12,580 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.weight
2025-03-20 20:54:12,581 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.bias
2025-03-20 20:54:12,581 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.weight
2025-03-20 20:54:12,582 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.bias
2025-03-20 20:54:12,583 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_weight
2025-03-20 20:54:12,583 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_bias
2025-03-20 20:54:12,584 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.weight
2025-03-20 20:54:12,584 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.bias
2025-03-20 20:54:12,585 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.weight
2025-03-20 20:54:12,586 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.bias
2025-03-20 20:54:12,586 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.weight
2025-03-20 20:54:12,587 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.bias
2025-03-20 20:54:12,588 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.weight
2025-03-20 20:54:12,588 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.bias
2025-03-20 20:54:12,589 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.weight
2025-03-20 20:54:12,589 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.bias
2025-03-20 20:54:12,590 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_weight
2025-03-20 20:54:12,590 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_bias
2025-03-20 20:54:12,591 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.weight
2025-03-20 20:54:12,591 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.bias
2025-03-20 20:54:12,592 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.weight
2025-03-20 20:54:12,592 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.bias
2025-03-20 20:54:12,593 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.weight
2025-03-20 20:54:12,593 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.bias
2025-03-20 20:54:12,594 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.weight
2025-03-20 20:54:12,594 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.bias
2025-03-20 20:54:12,595 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.weight
2025-03-20 20:54:12,596 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.bias
2025-03-20 20:54:12,596 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_weight
2025-03-20 20:54:12,597 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_bias
2025-03-20 20:54:12,597 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.weight
2025-03-20 20:54:12,598 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.bias
2025-03-20 20:54:12,598 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.weight
2025-03-20 20:54:12,599 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.bias
2025-03-20 20:54:12,599 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.weight
2025-03-20 20:54:12,600 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.bias
2025-03-20 20:54:12,600 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.weight
2025-03-20 20:54:12,601 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.bias
2025-03-20 20:54:12,601 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.weight
2025-03-20 20:54:12,602 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.bias
2025-03-20 20:54:12,602 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_weight
2025-03-20 20:54:12,603 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_bias
2025-03-20 20:54:12,604 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.weight
2025-03-20 20:54:12,604 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.bias
2025-03-20 20:54:12,605 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.weight
2025-03-20 20:54:12,605 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.bias
2025-03-20 20:54:12,606 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.weight
2025-03-20 20:54:12,606 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.bias
2025-03-20 20:54:12,607 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.weight
2025-03-20 20:54:12,607 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.bias
2025-03-20 20:54:12,608 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.weight
2025-03-20 20:54:12,608 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.bias
2025-03-20 20:54:12,609 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_weight
2025-03-20 20:54:12,609 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_bias
2025-03-20 20:54:12,610 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.weight
2025-03-20 20:54:12,610 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.bias
2025-03-20 20:54:12,611 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.weight
2025-03-20 20:54:12,611 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.bias
2025-03-20 20:54:12,612 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.weight
2025-03-20 20:54:12,612 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.bias
2025-03-20 20:54:12,613 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.weight
2025-03-20 20:54:12,613 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.bias
2025-03-20 20:54:12,614 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.weight
2025-03-20 20:54:12,614 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.bias
2025-03-20 20:54:12,615 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_weight
2025-03-20 20:54:12,615 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_bias
2025-03-20 20:54:12,616 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.weight
2025-03-20 20:54:12,616 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.bias
2025-03-20 20:54:12,617 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.weight
2025-03-20 20:54:12,617 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.bias
2025-03-20 20:54:12,618 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.weight
2025-03-20 20:54:12,618 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.bias
2025-03-20 20:54:12,619 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.weight
2025-03-20 20:54:12,619 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.bias
2025-03-20 20:54:12,620 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.weight
2025-03-20 20:54:12,620 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.bias
2025-03-20 20:54:12,621 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_weight
2025-03-20 20:54:12,621 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_bias
2025-03-20 20:54:12,622 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.weight
2025-03-20 20:54:12,622 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.bias
2025-03-20 20:54:12,623 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.weight
2025-03-20 20:54:12,623 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.bias
2025-03-20 20:54:12,624 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.weight
2025-03-20 20:54:12,624 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.bias
2025-03-20 20:54:12,625 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.weight
2025-03-20 20:54:12,625 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.bias
2025-03-20 20:54:12,626 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.weight
2025-03-20 20:54:12,626 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.bias
2025-03-20 20:54:12,627 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_weight
2025-03-20 20:54:12,627 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_bias
2025-03-20 20:54:12,628 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.weight
2025-03-20 20:54:12,628 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.bias
2025-03-20 20:54:12,628 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.weight
2025-03-20 20:54:12,629 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.bias
2025-03-20 20:54:12,629 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.weight
2025-03-20 20:54:12,630 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.bias
2025-03-20 20:54:12,630 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.weight
2025-03-20 20:54:12,631 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.bias
2025-03-20 20:54:12,631 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.weight
2025-03-20 20:54:12,632 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.bias
2025-03-20 20:54:12,632 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.weight
2025-03-20 20:54:12,633 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.bias
2025-03-20 20:54:12,633 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: custom_src_module.emb.Embedding.weight
2025-03-20 20:54:12,634 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.weight
2025-03-20 20:54:12,634 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.bias
2025-03-20 20:54:12,635 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.weight
2025-03-20 20:54:12,635 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.bias
2025-03-20 20:54:12,636 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.weight
2025-03-20 20:54:12,636 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.bias
2025-03-20 20:54:12,637 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: embedding.Embedding.weight
2025-03-20 20:54:12,637 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l0
2025-03-20 20:54:12,638 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l0
2025-03-20 20:54:12,638 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l0
2025-03-20 20:54:12,639 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l0
2025-03-20 20:54:12,639 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l1
2025-03-20 20:54:12,640 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l1
2025-03-20 20:54:12,640 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l1
2025-03-20 20:54:12,641 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l1
2025-03-20 20:54:12,641 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.weight
2025-03-20 20:54:12,641 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.bias
2025-03-20 20:54:12,642 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.weight
2025-03-20 20:54:12,642 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.bias
2025-03-20 20:54:12,643 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.weight
2025-03-20 20:54:12,643 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.bias
2025-03-20 20:54:12,803 - speechbrain.core - INFO - Info: precision arg from hparam file is used
2025-03-20 20:54:12,804 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2025-03-20 20:54:12,804 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2025-03-20 20:54:13,068 - speechbrain.core - INFO - Gradscaler enabled: True. Using precision: fp16.
2025-03-20 20:54:13,069 - speechbrain.core - INFO - ASR Model Statistics:
* Total Number of Trainable Parameters: 13.3M
* Total Number of Parameters: 13.3M
* Trainable Parameters represent 100.0000% of the total size.
2025-03-20 20:54:22,829 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+18-17-54+00
2025-03-20 20:54:29,627 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+18-17-54+00
2025-03-20 21:03:20,127 - speechbrain.utils.quirks - INFO - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
2025-03-20 21:03:20,141 - speechbrain.utils.quirks - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
2025-03-20 21:03:20,141 - speechbrain.core - INFO - Beginning experiment!
2025-03-20 21:03:20,141 - speechbrain.core - INFO - Experiment folder: results_fold_1/conformer_small/7775
2025-03-20 21:03:24,774 - speechbrain.utils.superpowers - DEBUG - aiohappyeyeballs==2.6.1
aiohttp==3.11.13
aiosignal==1.3.2
anyio==4.8.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==3.0.0
async-lru==2.0.4
async-timeout==5.0.1
attrs==24.2.0
babel==2.17.0
backcall==0.2.0
beautifulsoup4==4.12.3
bleach==6.2.0
Brotli @ file:///home/conda/feedstock_root/build_artifacts/brotli-split_1648883617327/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2024.8.30
cffi==1.17.1
charset-normalizer==3.4.0
comm==0.2.2
contourpy==1.3.0
cycler==0.12.1
datasets==3.3.2
debugpy==1.8.13
decorator==5.1.1
defusedxml==0.7.1
dill==0.3.8
docopt==0.6.2
exceptiongroup==1.2.2
executing==2.1.0
fastjsonschema==2.21.1
filelock==3.16.1
fonttools==4.56.0
fqdn==1.5.1
frozenlist==1.5.0
fsspec==2024.10.0
h11==0.14.0
h2 @ file:///home/conda/feedstock_root/build_artifacts/h2_1738578511449/work
hpack @ file:///home/conda/feedstock_root/build_artifacts/hpack_1737618293087/work
httpcore==1.0.7
httpx==0.28.1
huggingface-hub==0.26.2
hyperframe @ file:///home/conda/feedstock_root/build_artifacts/hyperframe_1737618333194/work
HyperPyYAML==1.2.2
idna==3.10
importlib_metadata==8.5.0
importlib_resources==6.5.2
ipykernel==6.29.5
ipython==8.12.3
isoduration==20.11.0
jedi==0.19.2
Jinja2==3.1.4
joblib==1.4.2
json5==0.10.0
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
jupyter-events==0.12.0
jupyter-lsp==2.2.5
jupyter_client==8.6.3
jupyter_core==5.7.2
jupyter_server==2.15.0
jupyter_server_terminals==0.5.3
jupyterlab==4.3.5
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
kiwisolver==1.4.7
MarkupSafe==3.0.2
matplotlib==3.9.4
matplotlib-inline==0.1.7
mistune==3.0.2
mpmath==1.3.0
multidict==6.1.0
multiprocess==0.70.16
nbclient==0.10.1
nbconvert==7.16.4
nbformat==5.10.4
nest-asyncio==1.6.0
networkx==3.2.1
notebook_shim==0.2.4
numpy==2.0.2
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-cusparselt-cu12==0.6.2
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.4.127
overrides==7.7.0
packaging==24.1
pandas==2.2.3
pandocfilters==1.5.1
parso==0.8.4
pexpect==4.9.0
pickleshare==0.7.5
pillow==11.1.0
pipreqs==0.5.0
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1733344503739/work
platformdirs==4.3.6
prometheus_client==0.21.1
prompt_toolkit==3.0.48
propcache==0.3.0
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==19.0.1
pycparser==2.22
Pygments==2.18.0
pyparsing==3.2.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1733217236728/work
python-dateutil==2.9.0.post0
python-json-logger==3.2.1
pytz==2025.1
PyYAML==6.0.2
pyzmq==26.2.0
referencing==0.35.1
requests==2.32.3
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.22.3
ruamel.yaml==0.18.6
ruamel.yaml.clib==0.2.12
scikit-learn==1.5.2
scipy==1.13.1
Send2Trash==1.8.3
sentencepiece==0.2.0
six==1.17.0
sniffio==1.3.1
soundfile==0.12.1
soupsieve==2.6
sox==1.5.0
speechbrain==1.0.2
stack-data==0.6.3
sympy==1.13.1
terminado==0.18.1
threadpoolctl==3.5.0
tinycss2==1.4.0
tomli==2.2.1
torch==2.5.1
torchaudio==2.5.1
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
triton==3.1.0
types-python-dateutil==2.9.0.20241206
typing_extensions==4.12.2
typing_utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1733331286120/work
tzdata==2025.1
uri-template==1.3.0
urllib3==2.2.3
wcwidth==0.2.13
webcolors==24.11.1
webencodings==0.5.1
websocket-client==1.8.0
xxhash==3.5.0
yarg==0.1.9
yarl==1.18.3
zipp==3.21.0


2025-03-20 21:03:24,807 - speechbrain.utils.superpowers - DEBUG - 2512280


2025-03-20 21:03:24,964 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results_fold_1/conformer_small/7775/save.
2025-03-20 21:03:24,965 - speechbrain.utils.fetching - INFO - Fetch lm.ckpt: Using symlink found at '/scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt'
2025-03-20 21:03:24,965 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["lm"] = /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt
2025-03-20 21:03:24,983 - speechbrain.utils.fetching - INFO - Fetch tokenizer.ckpt: Using symlink found at '/scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/tokenizer.ckpt'
2025-03-20 21:03:24,983 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["tokenizer"] = /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/tokenizer.ckpt
2025-03-20 21:03:24,983 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2025-03-20 21:03:24,984 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): lm -> /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt
2025-03-20 21:03:24,984 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): tokenizer -> /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/tokenizer.ckpt
2025-03-20 21:03:25,521 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: positional_encoding.pe
2025-03-20 21:03:25,521 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_weight
2025-03-20 21:03:25,522 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_bias
2025-03-20 21:03:25,523 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.weight
2025-03-20 21:03:25,524 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.bias
2025-03-20 21:03:25,524 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.weight
2025-03-20 21:03:25,525 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.bias
2025-03-20 21:03:25,525 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.weight
2025-03-20 21:03:25,526 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.bias
2025-03-20 21:03:25,527 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.weight
2025-03-20 21:03:25,527 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.bias
2025-03-20 21:03:25,528 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.weight
2025-03-20 21:03:25,528 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.bias
2025-03-20 21:03:25,529 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_weight
2025-03-20 21:03:25,530 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_bias
2025-03-20 21:03:25,530 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.weight
2025-03-20 21:03:25,531 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.bias
2025-03-20 21:03:25,531 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.weight
2025-03-20 21:03:25,532 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.bias
2025-03-20 21:03:25,533 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.weight
2025-03-20 21:03:25,533 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.bias
2025-03-20 21:03:25,534 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.weight
2025-03-20 21:03:25,535 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.bias
2025-03-20 21:03:25,535 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.weight
2025-03-20 21:03:25,536 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.bias
2025-03-20 21:03:25,536 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_weight
2025-03-20 21:03:25,537 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_bias
2025-03-20 21:03:25,537 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.weight
2025-03-20 21:03:25,538 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.bias
2025-03-20 21:03:25,539 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.weight
2025-03-20 21:03:25,539 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.bias
2025-03-20 21:03:25,540 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.weight
2025-03-20 21:03:25,540 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.bias
2025-03-20 21:03:25,541 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.weight
2025-03-20 21:03:25,541 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.bias
2025-03-20 21:03:25,542 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.weight
2025-03-20 21:03:25,543 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.bias
2025-03-20 21:03:25,543 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_weight
2025-03-20 21:03:25,544 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_bias
2025-03-20 21:03:25,544 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.weight
2025-03-20 21:03:25,545 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.bias
2025-03-20 21:03:25,546 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.weight
2025-03-20 21:03:25,546 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.bias
2025-03-20 21:03:25,547 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.weight
2025-03-20 21:03:25,547 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.bias
2025-03-20 21:03:25,548 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.weight
2025-03-20 21:03:25,548 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.bias
2025-03-20 21:03:25,549 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.weight
2025-03-20 21:03:25,549 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.bias
2025-03-20 21:03:25,550 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_weight
2025-03-20 21:03:25,551 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_bias
2025-03-20 21:03:25,551 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.weight
2025-03-20 21:03:25,552 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.bias
2025-03-20 21:03:25,552 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.weight
2025-03-20 21:03:25,553 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.bias
2025-03-20 21:03:25,553 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.weight
2025-03-20 21:03:25,554 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.bias
2025-03-20 21:03:25,554 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.weight
2025-03-20 21:03:25,555 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.bias
2025-03-20 21:03:25,555 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.weight
2025-03-20 21:03:25,556 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.bias
2025-03-20 21:03:25,556 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_weight
2025-03-20 21:03:25,557 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_bias
2025-03-20 21:03:25,558 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.weight
2025-03-20 21:03:25,558 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.bias
2025-03-20 21:03:25,559 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.weight
2025-03-20 21:03:25,559 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.bias
2025-03-20 21:03:25,560 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.weight
2025-03-20 21:03:25,560 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.bias
2025-03-20 21:03:25,561 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.weight
2025-03-20 21:03:25,561 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.bias
2025-03-20 21:03:25,562 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.weight
2025-03-20 21:03:25,562 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.bias
2025-03-20 21:03:25,563 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_weight
2025-03-20 21:03:25,564 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_bias
2025-03-20 21:03:25,564 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.weight
2025-03-20 21:03:25,565 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.bias
2025-03-20 21:03:25,566 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.weight
2025-03-20 21:03:25,566 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.bias
2025-03-20 21:03:25,567 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.weight
2025-03-20 21:03:25,567 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.bias
2025-03-20 21:03:25,568 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.weight
2025-03-20 21:03:25,568 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.bias
2025-03-20 21:03:25,569 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.weight
2025-03-20 21:03:25,569 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.bias
2025-03-20 21:03:25,570 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_weight
2025-03-20 21:03:25,570 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_bias
2025-03-20 21:03:25,571 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.weight
2025-03-20 21:03:25,572 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.bias
2025-03-20 21:03:25,572 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.weight
2025-03-20 21:03:25,573 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.bias
2025-03-20 21:03:25,573 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.weight
2025-03-20 21:03:25,574 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.bias
2025-03-20 21:03:25,574 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.weight
2025-03-20 21:03:25,575 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.bias
2025-03-20 21:03:25,575 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.weight
2025-03-20 21:03:25,576 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.bias
2025-03-20 21:03:25,576 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_weight
2025-03-20 21:03:25,577 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_bias
2025-03-20 21:03:25,578 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.weight
2025-03-20 21:03:25,578 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.bias
2025-03-20 21:03:25,579 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.weight
2025-03-20 21:03:25,579 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.bias
2025-03-20 21:03:25,580 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.weight
2025-03-20 21:03:25,580 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.bias
2025-03-20 21:03:25,581 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.weight
2025-03-20 21:03:25,581 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.bias
2025-03-20 21:03:25,582 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.weight
2025-03-20 21:03:25,582 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.bias
2025-03-20 21:03:25,583 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_weight
2025-03-20 21:03:25,583 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_bias
2025-03-20 21:03:25,584 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.weight
2025-03-20 21:03:25,585 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.bias
2025-03-20 21:03:25,585 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.weight
2025-03-20 21:03:25,586 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.bias
2025-03-20 21:03:25,586 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.weight
2025-03-20 21:03:25,587 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.bias
2025-03-20 21:03:25,587 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.weight
2025-03-20 21:03:25,588 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.bias
2025-03-20 21:03:25,588 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.weight
2025-03-20 21:03:25,589 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.bias
2025-03-20 21:03:25,589 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_weight
2025-03-20 21:03:25,590 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_bias
2025-03-20 21:03:25,591 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.weight
2025-03-20 21:03:25,591 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.bias
2025-03-20 21:03:25,592 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.weight
2025-03-20 21:03:25,592 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.bias
2025-03-20 21:03:25,593 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.weight
2025-03-20 21:03:25,593 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.bias
2025-03-20 21:03:25,594 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.weight
2025-03-20 21:03:25,594 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.bias
2025-03-20 21:03:25,595 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.weight
2025-03-20 21:03:25,596 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.bias
2025-03-20 21:03:25,596 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_weight
2025-03-20 21:03:25,597 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_bias
2025-03-20 21:03:25,598 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.weight
2025-03-20 21:03:25,598 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.bias
2025-03-20 21:03:25,599 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.weight
2025-03-20 21:03:25,600 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.bias
2025-03-20 21:03:25,600 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.weight
2025-03-20 21:03:25,601 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.bias
2025-03-20 21:03:25,601 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.weight
2025-03-20 21:03:25,602 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.bias
2025-03-20 21:03:25,602 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.weight
2025-03-20 21:03:25,603 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.bias
2025-03-20 21:03:25,603 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.weight
2025-03-20 21:03:25,604 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.bias
2025-03-20 21:03:25,604 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: custom_src_module.emb.Embedding.weight
2025-03-20 21:03:25,605 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.weight
2025-03-20 21:03:25,606 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.bias
2025-03-20 21:03:25,606 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.weight
2025-03-20 21:03:25,607 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.bias
2025-03-20 21:03:25,607 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.weight
2025-03-20 21:03:25,608 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.bias
2025-03-20 21:03:25,608 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: embedding.Embedding.weight
2025-03-20 21:03:25,609 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l0
2025-03-20 21:03:25,609 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l0
2025-03-20 21:03:25,610 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l0
2025-03-20 21:03:25,610 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l0
2025-03-20 21:03:25,611 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l1
2025-03-20 21:03:25,611 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l1
2025-03-20 21:03:25,612 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l1
2025-03-20 21:03:25,612 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l1
2025-03-20 21:03:25,613 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.weight
2025-03-20 21:03:25,613 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.bias
2025-03-20 21:03:25,614 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.weight
2025-03-20 21:03:25,614 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.bias
2025-03-20 21:03:25,615 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.weight
2025-03-20 21:03:25,615 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.bias
2025-03-20 21:03:25,651 - speechbrain.core - INFO - Info: precision arg from hparam file is used
2025-03-20 21:03:25,651 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2025-03-20 21:03:25,651 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2025-03-20 21:03:25,913 - speechbrain.core - INFO - Gradscaler enabled: True. Using precision: fp16.
2025-03-20 21:03:25,914 - speechbrain.core - INFO - ASR Model Statistics:
* Total Number of Trainable Parameters: 13.3M
* Total Number of Parameters: 13.3M
* Trainable Parameters represent 100.0000% of the total size.
2025-03-20 21:03:33,484 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+18-17-54+00
2025-03-20 21:03:35,025 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+18-17-54+00
2025-03-20 21:11:26,220 - speechbrain.utils.train_logger - INFO - Epoch loaded: 10 - test loss: 7.23, test ACC: 6.83e-01, test WER: 1.07e+02
2025-03-20 21:11:26,838 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+21-11-26+00
2025-03-20 21:11:26,862 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_1/conformer_small/7775/save/CKPT+2025-03-20+18-17-54+00
