2025-03-20 19:47:16,350 - speechbrain.utils.quirks - INFO - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
2025-03-20 19:47:16,351 - speechbrain.utils.quirks - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
2025-03-20 19:47:16,351 - speechbrain.core - INFO - Beginning experiment!
2025-03-20 19:47:16,351 - speechbrain.core - INFO - Experiment folder: results_fold_4/conformer_small/7775
2025-03-20 19:47:27,028 - speechbrain.utils.superpowers - DEBUG - aiohappyeyeballs==2.6.1
aiohttp==3.11.13
aiosignal==1.3.2
anyio==4.8.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==3.0.0
async-lru==2.0.4
async-timeout==5.0.1
attrs==24.2.0
babel==2.17.0
backcall==0.2.0
beautifulsoup4==4.12.3
bleach==6.2.0
Brotli @ file:///home/conda/feedstock_root/build_artifacts/brotli-split_1648883617327/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2024.8.30
cffi==1.17.1
charset-normalizer==3.4.0
comm==0.2.2
contourpy==1.3.0
cycler==0.12.1
datasets==3.3.2
debugpy==1.8.13
decorator==5.1.1
defusedxml==0.7.1
dill==0.3.8
docopt==0.6.2
exceptiongroup==1.2.2
executing==2.1.0
fastjsonschema==2.21.1
filelock==3.16.1
fonttools==4.56.0
fqdn==1.5.1
frozenlist==1.5.0
fsspec==2024.10.0
h11==0.14.0
h2 @ file:///home/conda/feedstock_root/build_artifacts/h2_1738578511449/work
hpack @ file:///home/conda/feedstock_root/build_artifacts/hpack_1737618293087/work
httpcore==1.0.7
httpx==0.28.1
huggingface-hub==0.26.2
hyperframe @ file:///home/conda/feedstock_root/build_artifacts/hyperframe_1737618333194/work
HyperPyYAML==1.2.2
idna==3.10
importlib_metadata==8.5.0
importlib_resources==6.5.2
ipykernel==6.29.5
ipython==8.12.3
isoduration==20.11.0
jedi==0.19.2
Jinja2==3.1.4
joblib==1.4.2
json5==0.10.0
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
jupyter-events==0.12.0
jupyter-lsp==2.2.5
jupyter_client==8.6.3
jupyter_core==5.7.2
jupyter_server==2.15.0
jupyter_server_terminals==0.5.3
jupyterlab==4.3.5
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
kiwisolver==1.4.7
MarkupSafe==3.0.2
matplotlib==3.9.4
matplotlib-inline==0.1.7
mistune==3.0.2
mpmath==1.3.0
multidict==6.1.0
multiprocess==0.70.16
nbclient==0.10.1
nbconvert==7.16.4
nbformat==5.10.4
nest-asyncio==1.6.0
networkx==3.2.1
notebook_shim==0.2.4
numpy==2.0.2
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-cusparselt-cu12==0.6.2
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.4.127
overrides==7.7.0
packaging==24.1
pandas==2.2.3
pandocfilters==1.5.1
parso==0.8.4
pexpect==4.9.0
pickleshare==0.7.5
pillow==11.1.0
pipreqs==0.5.0
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1733344503739/work
platformdirs==4.3.6
prometheus_client==0.21.1
prompt_toolkit==3.0.48
propcache==0.3.0
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==19.0.1
pycparser==2.22
Pygments==2.18.0
pyparsing==3.2.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1733217236728/work
python-dateutil==2.9.0.post0
python-json-logger==3.2.1
pytz==2025.1
PyYAML==6.0.2
pyzmq==26.2.0
referencing==0.35.1
requests==2.32.3
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.22.3
ruamel.yaml==0.18.6
ruamel.yaml.clib==0.2.12
scikit-learn==1.5.2
scipy==1.13.1
Send2Trash==1.8.3
sentencepiece==0.2.0
six==1.17.0
sniffio==1.3.1
soundfile==0.12.1
soupsieve==2.6
sox==1.5.0
speechbrain==1.0.2
stack-data==0.6.3
sympy==1.13.1
terminado==0.18.1
threadpoolctl==3.5.0
tinycss2==1.4.0
tomli==2.2.1
torch==2.5.1
torchaudio==2.5.1
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
triton==3.1.0
types-python-dateutil==2.9.0.20241206
typing_extensions==4.12.2
typing_utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1733331286120/work
tzdata==2025.1
uri-template==1.3.0
urllib3==2.2.3
wcwidth==0.2.13
webcolors==24.11.1
webencodings==0.5.1
websocket-client==1.8.0
xxhash==3.5.0
yarg==0.1.9
yarl==1.18.3
zipp==3.21.0


2025-03-20 19:47:27,236 - speechbrain.utils.superpowers - DEBUG - 2512280


2025-03-20 19:47:27,803 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results_fold_4/conformer_small/7775/save.
2025-03-20 19:47:27,804 - speechbrain.utils.fetching - DEBUG - Fetch: Local file found, creating symlink '/scratch/flatala/speechbrain/transformer/LM_weights_13_epochs/lm.ckpt' -> '/scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt'
2025-03-20 19:47:27,826 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["lm"] = /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt
2025-03-20 19:47:27,827 - speechbrain.utils.fetching - DEBUG - Fetch: Local file found, creating symlink '/scratch/flatala/speechbrain/transformer/LM_weights_13_epochs/tokenizer.ckpt' -> '/scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/tokenizer.ckpt'
2025-03-20 19:47:27,827 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["tokenizer"] = /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/tokenizer.ckpt
2025-03-20 19:47:27,828 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2025-03-20 19:47:27,828 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): lm -> /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt
2025-03-20 19:47:27,828 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): tokenizer -> /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/tokenizer.ckpt
2025-03-20 19:47:28,429 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: positional_encoding.pe
2025-03-20 19:47:28,429 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_weight
2025-03-20 19:47:28,430 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_bias
2025-03-20 19:47:28,430 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.weight
2025-03-20 19:47:28,431 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.bias
2025-03-20 19:47:28,431 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.weight
2025-03-20 19:47:28,432 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.bias
2025-03-20 19:47:28,432 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.weight
2025-03-20 19:47:28,433 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.bias
2025-03-20 19:47:28,433 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.weight
2025-03-20 19:47:28,434 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.bias
2025-03-20 19:47:28,434 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.weight
2025-03-20 19:47:28,435 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.bias
2025-03-20 19:47:28,435 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_weight
2025-03-20 19:47:28,436 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_bias
2025-03-20 19:47:28,436 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.weight
2025-03-20 19:47:28,437 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.bias
2025-03-20 19:47:28,437 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.weight
2025-03-20 19:47:28,438 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.bias
2025-03-20 19:47:28,438 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.weight
2025-03-20 19:47:28,439 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.bias
2025-03-20 19:47:28,439 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.weight
2025-03-20 19:47:28,440 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.bias
2025-03-20 19:47:28,440 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.weight
2025-03-20 19:47:28,441 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.bias
2025-03-20 19:47:28,441 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_weight
2025-03-20 19:47:28,442 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_bias
2025-03-20 19:47:28,442 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.weight
2025-03-20 19:47:28,443 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.bias
2025-03-20 19:47:28,443 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.weight
2025-03-20 19:47:28,444 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.bias
2025-03-20 19:47:28,444 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.weight
2025-03-20 19:47:28,445 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.bias
2025-03-20 19:47:28,445 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.weight
2025-03-20 19:47:28,446 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.bias
2025-03-20 19:47:28,446 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.weight
2025-03-20 19:47:28,447 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.bias
2025-03-20 19:47:28,447 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_weight
2025-03-20 19:47:28,448 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_bias
2025-03-20 19:47:28,448 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.weight
2025-03-20 19:47:28,449 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.bias
2025-03-20 19:47:28,449 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.weight
2025-03-20 19:47:28,450 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.bias
2025-03-20 19:47:28,450 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.weight
2025-03-20 19:47:28,451 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.bias
2025-03-20 19:47:28,451 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.weight
2025-03-20 19:47:28,452 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.bias
2025-03-20 19:47:28,452 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.weight
2025-03-20 19:47:28,453 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.bias
2025-03-20 19:47:28,453 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_weight
2025-03-20 19:47:28,454 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_bias
2025-03-20 19:47:28,454 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.weight
2025-03-20 19:47:28,455 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.bias
2025-03-20 19:47:28,455 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.weight
2025-03-20 19:47:28,456 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.bias
2025-03-20 19:47:28,456 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.weight
2025-03-20 19:47:28,457 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.bias
2025-03-20 19:47:28,457 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.weight
2025-03-20 19:47:28,458 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.bias
2025-03-20 19:47:28,458 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.weight
2025-03-20 19:47:28,459 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.bias
2025-03-20 19:47:28,459 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_weight
2025-03-20 19:47:28,460 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_bias
2025-03-20 19:47:28,460 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.weight
2025-03-20 19:47:28,461 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.bias
2025-03-20 19:47:28,461 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.weight
2025-03-20 19:47:28,462 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.bias
2025-03-20 19:47:28,462 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.weight
2025-03-20 19:47:28,463 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.bias
2025-03-20 19:47:28,463 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.weight
2025-03-20 19:47:28,464 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.bias
2025-03-20 19:47:28,464 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.weight
2025-03-20 19:47:28,465 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.bias
2025-03-20 19:47:28,465 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_weight
2025-03-20 19:47:28,466 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_bias
2025-03-20 19:47:28,466 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.weight
2025-03-20 19:47:28,467 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.bias
2025-03-20 19:47:28,467 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.weight
2025-03-20 19:47:28,467 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.bias
2025-03-20 19:47:28,468 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.weight
2025-03-20 19:47:28,468 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.bias
2025-03-20 19:47:28,469 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.weight
2025-03-20 19:47:28,469 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.bias
2025-03-20 19:47:28,470 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.weight
2025-03-20 19:47:28,471 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.bias
2025-03-20 19:47:28,471 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_weight
2025-03-20 19:47:28,471 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_bias
2025-03-20 19:47:28,472 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.weight
2025-03-20 19:47:28,473 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.bias
2025-03-20 19:47:28,473 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.weight
2025-03-20 19:47:28,474 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.bias
2025-03-20 19:47:28,474 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.weight
2025-03-20 19:47:28,475 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.bias
2025-03-20 19:47:28,475 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.weight
2025-03-20 19:47:28,476 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.bias
2025-03-20 19:47:28,476 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.weight
2025-03-20 19:47:28,477 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.bias
2025-03-20 19:47:28,477 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_weight
2025-03-20 19:47:28,478 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_bias
2025-03-20 19:47:28,478 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.weight
2025-03-20 19:47:28,479 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.bias
2025-03-20 19:47:28,479 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.weight
2025-03-20 19:47:28,480 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.bias
2025-03-20 19:47:28,480 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.weight
2025-03-20 19:47:28,481 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.bias
2025-03-20 19:47:28,481 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.weight
2025-03-20 19:47:28,482 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.bias
2025-03-20 19:47:28,482 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.weight
2025-03-20 19:47:28,483 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.bias
2025-03-20 19:47:28,483 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_weight
2025-03-20 19:47:28,484 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_bias
2025-03-20 19:47:28,484 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.weight
2025-03-20 19:47:28,485 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.bias
2025-03-20 19:47:28,485 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.weight
2025-03-20 19:47:28,486 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.bias
2025-03-20 19:47:28,486 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.weight
2025-03-20 19:47:28,487 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.bias
2025-03-20 19:47:28,487 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.weight
2025-03-20 19:47:28,488 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.bias
2025-03-20 19:47:28,488 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.weight
2025-03-20 19:47:28,489 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.bias
2025-03-20 19:47:28,489 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_weight
2025-03-20 19:47:28,490 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_bias
2025-03-20 19:47:28,490 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.weight
2025-03-20 19:47:28,491 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.bias
2025-03-20 19:47:28,491 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.weight
2025-03-20 19:47:28,492 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.bias
2025-03-20 19:47:28,492 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.weight
2025-03-20 19:47:28,492 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.bias
2025-03-20 19:47:28,493 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.weight
2025-03-20 19:47:28,493 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.bias
2025-03-20 19:47:28,494 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.weight
2025-03-20 19:47:28,494 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.bias
2025-03-20 19:47:28,495 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_weight
2025-03-20 19:47:28,495 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_bias
2025-03-20 19:47:28,496 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.weight
2025-03-20 19:47:28,496 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.bias
2025-03-20 19:47:28,497 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.weight
2025-03-20 19:47:28,497 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.bias
2025-03-20 19:47:28,498 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.weight
2025-03-20 19:47:28,498 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.bias
2025-03-20 19:47:28,499 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.weight
2025-03-20 19:47:28,499 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.bias
2025-03-20 19:47:28,500 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.weight
2025-03-20 19:47:28,500 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.bias
2025-03-20 19:47:28,501 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.weight
2025-03-20 19:47:28,501 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.bias
2025-03-20 19:47:28,502 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: custom_src_module.emb.Embedding.weight
2025-03-20 19:47:28,502 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.weight
2025-03-20 19:47:28,503 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.bias
2025-03-20 19:47:28,503 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.weight
2025-03-20 19:47:28,503 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.bias
2025-03-20 19:47:28,504 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.weight
2025-03-20 19:47:28,504 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.bias
2025-03-20 19:47:28,505 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: embedding.Embedding.weight
2025-03-20 19:47:28,505 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l0
2025-03-20 19:47:28,506 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l0
2025-03-20 19:47:28,506 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l0
2025-03-20 19:47:28,507 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l0
2025-03-20 19:47:28,507 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l1
2025-03-20 19:47:28,508 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l1
2025-03-20 19:47:28,508 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l1
2025-03-20 19:47:28,509 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l1
2025-03-20 19:47:28,509 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.weight
2025-03-20 19:47:28,510 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.bias
2025-03-20 19:47:28,510 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.weight
2025-03-20 19:47:28,511 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.bias
2025-03-20 19:47:28,511 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.weight
2025-03-20 19:47:28,511 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.bias
2025-03-20 19:47:28,531 - speechbrain.core - INFO - Info: precision arg from hparam file is used
2025-03-20 19:47:28,531 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2025-03-20 19:47:28,531 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2025-03-20 19:47:28,720 - speechbrain.core - INFO - Gradscaler enabled: True. Using precision: fp16.
2025-03-20 19:47:28,722 - speechbrain.core - INFO - ASR Model Statistics:
* Total Number of Trainable Parameters: 13.3M
* Total Number of Parameters: 13.3M
* Trainable Parameters represent 100.0000% of the total size.
2025-03-20 19:47:34,730 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2025-03-20 19:47:34,730 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2025-03-20 19:50:32,453 - speechbrain.utils.train_logger - INFO - epoch: 1, lr: 3.48e-05, steps: 872, optimizer: Adam - train loss: 78.94 - valid loss: 33.29, valid ACC: 1.92e-01
2025-03-20 19:50:32,959 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+19-50-32+00
2025-03-20 19:50:32,966 - speechbrain.utils.epoch_loop - INFO - Going into epoch 2
2025-03-20 19:52:53,613 - speechbrain.utils.train_logger - INFO - epoch: 2, lr: 6.97e-05, steps: 1744, optimizer: Adam - train loss: 28.34 - valid loss: 24.41, valid ACC: 2.76e-01
2025-03-20 19:52:54,456 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+19-52-53+00
2025-03-20 19:52:54,470 - speechbrain.utils.epoch_loop - INFO - Going into epoch 3
2025-03-20 19:55:15,037 - speechbrain.utils.train_logger - INFO - epoch: 3, lr: 1.05e-04, steps: 2616, optimizer: Adam - train loss: 23.74 - valid loss: 19.60, valid ACC: 4.32e-01
2025-03-20 19:55:17,378 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+19-55-15+00
2025-03-20 19:55:17,401 - speechbrain.utils.epoch_loop - INFO - Going into epoch 4
2025-03-20 19:57:37,234 - speechbrain.utils.train_logger - INFO - epoch: 4, lr: 1.39e-04, steps: 3488, optimizer: Adam - train loss: 19.89 - valid loss: 15.31, valid ACC: 6.55e-01
2025-03-20 19:57:37,823 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+19-57-37+00
2025-03-20 19:57:37,859 - speechbrain.utils.epoch_loop - INFO - Going into epoch 5
2025-03-20 19:59:58,525 - speechbrain.utils.train_logger - INFO - epoch: 5, lr: 1.74e-04, steps: 4360, optimizer: Adam - train loss: 16.53 - valid loss: 12.83, valid ACC: 7.35e-01
2025-03-20 19:59:58,896 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+19-59-58+00
2025-03-20 19:59:58,946 - speechbrain.utils.epoch_loop - INFO - Going into epoch 6
2025-03-20 20:02:28,643 - speechbrain.utils.train_logger - INFO - epoch: 6, lr: 2.09e-04, steps: 5232, optimizer: Adam - train loss: 14.29 - valid loss: 11.97, valid ACC: 7.48e-01
2025-03-20 20:02:29,247 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+20-02-28+00
2025-03-20 20:02:29,312 - speechbrain.utils.epoch_loop - INFO - Going into epoch 7
2025-03-20 20:04:49,493 - speechbrain.utils.train_logger - INFO - epoch: 7, lr: 2.44e-04, steps: 6104, optimizer: Adam - train loss: 13.16 - valid loss: 11.60, valid ACC: 7.50e-01
2025-03-20 20:04:49,894 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+20-04-49+00
2025-03-20 20:04:49,979 - speechbrain.utils.epoch_loop - INFO - Going into epoch 8
2025-03-20 20:07:10,610 - speechbrain.utils.train_logger - INFO - epoch: 8, lr: 2.79e-04, steps: 6976, optimizer: Adam - train loss: 12.58 - valid loss: 11.41, valid ACC: 7.51e-01
2025-03-20 20:07:10,969 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+20-07-10+00
2025-03-20 20:07:11,090 - speechbrain.utils.epoch_loop - INFO - Going into epoch 9
2025-03-20 20:09:31,009 - speechbrain.utils.train_logger - INFO - epoch: 9, lr: 3.14e-04, steps: 7848, optimizer: Adam - train loss: 12.28 - valid loss: 11.38, valid ACC: 7.45e-01
2025-03-20 20:09:31,524 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+20-09-31+00
2025-03-20 20:09:31,657 - speechbrain.utils.epoch_loop - INFO - Going into epoch 10
2025-03-20 20:15:43,405 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/scratch/flatala/speechbrain/transformer/train.py", line 468, in <module>
    asr_brain.fit(
  File "/home/flatala/.local/lib/python3.9/site-packages/speechbrain/core.py", line 1620, in fit
    self._fit_valid(valid_set=valid_set, epoch=epoch, enable=enable)
  File "/home/flatala/.local/lib/python3.9/site-packages/speechbrain/core.py", line 1522, in _fit_valid
    loss = self.evaluate_batch(batch, stage=Stage.VALID)
  File "/home/flatala/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/flatala/.local/lib/python3.9/site-packages/speechbrain/core.py", line 1405, in evaluate_batch
    out = self.compute_forward(batch, stage=stage)
  File "/scratch/flatala/speechbrain/transformer/train.py", line 109, in compute_forward
    hyps, _, _, _ = self.hparams.valid_search(
  File "/home/flatala/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/flatala/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/flatala/.local/lib/python3.9/site-packages/speechbrain/decoders/seq2seq.py", line 1608, in forward
    ) = self.search_step(
  File "/home/flatala/.local/lib/python3.9/site-packages/speechbrain/decoders/seq2seq.py", line 1486, in search_step
    (log_probs, scorer_memory) = self._scorer_step(
  File "/home/flatala/.local/lib/python3.9/site-packages/speechbrain/decoders/seq2seq.py", line 900, in _scorer_step
    log_probs, scorer_memory = self.scorer.score(
  File "/home/flatala/.local/lib/python3.9/site-packages/speechbrain/decoders/scorer.py", line 1202, in score
    score, new_memory[k] = impl.score(inp_tokens, memory[k], None, attn)
  File "/home/flatala/.local/lib/python3.9/site-packages/speechbrain/decoders/scorer.py", line 205, in score
    scores, memory = self.ctc_score.forward_step(
  File "/home/flatala/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/flatala/.local/lib/python3.9/site-packages/speechbrain/decoders/ctc.py", line 227, in forward_step
    psi = torch.logsumexp(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.10 GiB. GPU 0 has a total capacity of 9.50 GiB of which 260.00 MiB is free. Including non-PyTorch memory, this process has 9.23 GiB memory in use. Of the allocated memory 8.39 GiB is allocated by PyTorch, and 710.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 21:32:51,977 - speechbrain.utils.quirks - INFO - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
2025-03-20 21:32:51,990 - speechbrain.utils.quirks - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
2025-03-20 21:32:51,990 - speechbrain.core - INFO - Beginning experiment!
2025-03-20 21:32:51,990 - speechbrain.core - INFO - Experiment folder: results_fold_4/conformer_small/7775
2025-03-20 21:32:54,700 - speechbrain.utils.superpowers - DEBUG - aiohappyeyeballs==2.6.1
aiohttp==3.11.13
aiosignal==1.3.2
anyio==4.8.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==3.0.0
async-lru==2.0.4
async-timeout==5.0.1
attrs==24.2.0
babel==2.17.0
backcall==0.2.0
beautifulsoup4==4.12.3
bleach==6.2.0
Brotli @ file:///home/conda/feedstock_root/build_artifacts/brotli-split_1648883617327/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2024.8.30
cffi==1.17.1
charset-normalizer==3.4.0
comm==0.2.2
contourpy==1.3.0
cycler==0.12.1
datasets==3.3.2
debugpy==1.8.13
decorator==5.1.1
defusedxml==0.7.1
dill==0.3.8
docopt==0.6.2
exceptiongroup==1.2.2
executing==2.1.0
fastjsonschema==2.21.1
filelock==3.16.1
fonttools==4.56.0
fqdn==1.5.1
frozenlist==1.5.0
fsspec==2024.10.0
h11==0.14.0
h2 @ file:///home/conda/feedstock_root/build_artifacts/h2_1738578511449/work
hpack @ file:///home/conda/feedstock_root/build_artifacts/hpack_1737618293087/work
httpcore==1.0.7
httpx==0.28.1
huggingface-hub==0.26.2
hyperframe @ file:///home/conda/feedstock_root/build_artifacts/hyperframe_1737618333194/work
HyperPyYAML==1.2.2
idna==3.10
importlib_metadata==8.5.0
importlib_resources==6.5.2
ipykernel==6.29.5
ipython==8.12.3
isoduration==20.11.0
jedi==0.19.2
Jinja2==3.1.4
joblib==1.4.2
json5==0.10.0
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
jupyter-events==0.12.0
jupyter-lsp==2.2.5
jupyter_client==8.6.3
jupyter_core==5.7.2
jupyter_server==2.15.0
jupyter_server_terminals==0.5.3
jupyterlab==4.3.5
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
kiwisolver==1.4.7
MarkupSafe==3.0.2
matplotlib==3.9.4
matplotlib-inline==0.1.7
mistune==3.0.2
mpmath==1.3.0
multidict==6.1.0
multiprocess==0.70.16
nbclient==0.10.1
nbconvert==7.16.4
nbformat==5.10.4
nest-asyncio==1.6.0
networkx==3.2.1
notebook_shim==0.2.4
numpy==2.0.2
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-cusparselt-cu12==0.6.2
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.4.127
overrides==7.7.0
packaging==24.1
pandas==2.2.3
pandocfilters==1.5.1
parso==0.8.4
pexpect==4.9.0
pickleshare==0.7.5
pillow==11.1.0
pipreqs==0.5.0
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1733344503739/work
platformdirs==4.3.6
prometheus_client==0.21.1
prompt_toolkit==3.0.48
propcache==0.3.0
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==19.0.1
pycparser==2.22
Pygments==2.18.0
pyparsing==3.2.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1733217236728/work
python-dateutil==2.9.0.post0
python-json-logger==3.2.1
pytz==2025.1
PyYAML==6.0.2
pyzmq==26.2.0
referencing==0.35.1
requests==2.32.3
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.22.3
ruamel.yaml==0.18.6
ruamel.yaml.clib==0.2.12
scikit-learn==1.5.2
scipy==1.13.1
Send2Trash==1.8.3
sentencepiece==0.2.0
six==1.17.0
sniffio==1.3.1
soundfile==0.12.1
soupsieve==2.6
sox==1.5.0
speechbrain==1.0.2
stack-data==0.6.3
sympy==1.13.1
terminado==0.18.1
threadpoolctl==3.5.0
tinycss2==1.4.0
tomli==2.2.1
torch==2.5.1
torchaudio==2.5.1
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
triton==3.1.0
types-python-dateutil==2.9.0.20241206
typing_extensions==4.12.2
typing_utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1733331286120/work
tzdata==2025.1
uri-template==1.3.0
urllib3==2.2.3
wcwidth==0.2.13
webcolors==24.11.1
webencodings==0.5.1
websocket-client==1.8.0
xxhash==3.5.0
yarg==0.1.9
yarl==1.18.3
zipp==3.21.0


2025-03-20 21:32:57,151 - speechbrain.utils.superpowers - DEBUG - 2512280


2025-03-20 21:32:57,885 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results_fold_4/conformer_small/7775/save.
2025-03-20 21:32:58,040 - speechbrain.utils.fetching - INFO - Fetch lm.ckpt: Using symlink found at '/scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt'
2025-03-20 21:32:58,076 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["lm"] = /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt
2025-03-20 21:32:58,120 - speechbrain.utils.fetching - INFO - Fetch tokenizer.ckpt: Using symlink found at '/scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/tokenizer.ckpt'
2025-03-20 21:32:58,291 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["tokenizer"] = /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/tokenizer.ckpt
2025-03-20 21:32:58,664 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2025-03-20 21:32:58,701 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): lm -> /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt
2025-03-20 21:32:58,745 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): tokenizer -> /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/tokenizer.ckpt
2025-03-20 21:32:59,092 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: positional_encoding.pe
2025-03-20 21:32:59,108 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_weight
2025-03-20 21:32:59,120 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_bias
2025-03-20 21:32:59,134 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.weight
2025-03-20 21:32:59,140 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.bias
2025-03-20 21:32:59,145 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.weight
2025-03-20 21:32:59,163 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.bias
2025-03-20 21:32:59,193 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.weight
2025-03-20 21:32:59,209 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.bias
2025-03-20 21:32:59,231 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.weight
2025-03-20 21:32:59,283 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.bias
2025-03-20 21:32:59,856 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.weight
2025-03-20 21:32:59,859 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.bias
2025-03-20 21:32:59,868 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_weight
2025-03-20 21:32:59,874 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_bias
2025-03-20 21:32:59,898 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.weight
2025-03-20 21:32:59,904 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.bias
2025-03-20 21:32:59,913 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.weight
2025-03-20 21:32:59,927 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.bias
2025-03-20 21:32:59,928 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.weight
2025-03-20 21:32:59,941 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.bias
2025-03-20 21:32:59,949 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.weight
2025-03-20 21:32:59,984 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.bias
2025-03-20 21:33:00,008 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.weight
2025-03-20 21:33:00,013 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.bias
2025-03-20 21:33:00,015 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_weight
2025-03-20 21:33:00,017 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_bias
2025-03-20 21:33:00,018 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.weight
2025-03-20 21:33:00,020 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.bias
2025-03-20 21:33:00,021 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.weight
2025-03-20 21:33:00,022 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.bias
2025-03-20 21:33:00,023 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.weight
2025-03-20 21:33:00,024 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.bias
2025-03-20 21:33:00,025 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.weight
2025-03-20 21:33:00,025 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.bias
2025-03-20 21:33:00,026 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.weight
2025-03-20 21:33:00,026 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.bias
2025-03-20 21:33:00,027 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_weight
2025-03-20 21:33:00,027 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_bias
2025-03-20 21:33:00,028 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.weight
2025-03-20 21:33:00,028 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.bias
2025-03-20 21:33:00,029 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.weight
2025-03-20 21:33:00,029 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.bias
2025-03-20 21:33:00,030 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.weight
2025-03-20 21:33:00,030 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.bias
2025-03-20 21:33:00,031 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.weight
2025-03-20 21:33:00,032 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.bias
2025-03-20 21:33:00,033 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.weight
2025-03-20 21:33:00,033 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.bias
2025-03-20 21:33:00,034 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_weight
2025-03-20 21:33:00,034 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_bias
2025-03-20 21:33:00,034 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.weight
2025-03-20 21:33:00,035 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.bias
2025-03-20 21:33:00,035 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.weight
2025-03-20 21:33:00,036 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.bias
2025-03-20 21:33:00,036 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.weight
2025-03-20 21:33:00,037 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.bias
2025-03-20 21:33:00,037 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.weight
2025-03-20 21:33:00,038 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.bias
2025-03-20 21:33:00,039 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.weight
2025-03-20 21:33:00,039 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.bias
2025-03-20 21:33:00,040 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_weight
2025-03-20 21:33:00,040 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_bias
2025-03-20 21:33:00,041 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.weight
2025-03-20 21:33:00,041 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.bias
2025-03-20 21:33:00,042 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.weight
2025-03-20 21:33:00,042 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.bias
2025-03-20 21:33:00,043 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.weight
2025-03-20 21:33:00,043 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.bias
2025-03-20 21:33:00,044 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.weight
2025-03-20 21:33:00,044 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.bias
2025-03-20 21:33:00,045 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.weight
2025-03-20 21:33:00,046 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.bias
2025-03-20 21:33:00,046 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_weight
2025-03-20 21:33:00,047 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_bias
2025-03-20 21:33:00,047 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.weight
2025-03-20 21:33:00,048 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.bias
2025-03-20 21:33:00,048 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.weight
2025-03-20 21:33:00,049 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.bias
2025-03-20 21:33:00,049 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.weight
2025-03-20 21:33:00,050 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.bias
2025-03-20 21:33:00,051 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.weight
2025-03-20 21:33:00,051 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.bias
2025-03-20 21:33:00,052 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.weight
2025-03-20 21:33:00,052 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.bias
2025-03-20 21:33:00,053 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_weight
2025-03-20 21:33:00,053 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_bias
2025-03-20 21:33:00,054 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.weight
2025-03-20 21:33:00,054 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.bias
2025-03-20 21:33:00,055 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.weight
2025-03-20 21:33:00,056 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.bias
2025-03-20 21:33:00,056 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.weight
2025-03-20 21:33:00,057 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.bias
2025-03-20 21:33:00,057 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.weight
2025-03-20 21:33:00,058 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.bias
2025-03-20 21:33:00,058 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.weight
2025-03-20 21:33:00,059 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.bias
2025-03-20 21:33:00,064 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_weight
2025-03-20 21:33:00,064 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_bias
2025-03-20 21:33:00,065 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.weight
2025-03-20 21:33:00,066 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.bias
2025-03-20 21:33:00,066 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.weight
2025-03-20 21:33:00,067 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.bias
2025-03-20 21:33:00,067 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.weight
2025-03-20 21:33:00,068 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.bias
2025-03-20 21:33:00,068 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.weight
2025-03-20 21:33:00,069 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.bias
2025-03-20 21:33:00,069 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.weight
2025-03-20 21:33:00,070 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.bias
2025-03-20 21:33:00,070 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_weight
2025-03-20 21:33:00,071 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_bias
2025-03-20 21:33:00,071 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.weight
2025-03-20 21:33:00,072 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.bias
2025-03-20 21:33:00,072 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.weight
2025-03-20 21:33:00,073 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.bias
2025-03-20 21:33:00,073 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.weight
2025-03-20 21:33:00,074 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.bias
2025-03-20 21:33:00,074 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.weight
2025-03-20 21:33:00,075 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.bias
2025-03-20 21:33:00,075 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.weight
2025-03-20 21:33:00,076 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.bias
2025-03-20 21:33:00,076 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_weight
2025-03-20 21:33:00,077 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_bias
2025-03-20 21:33:00,077 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.weight
2025-03-20 21:33:00,078 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.bias
2025-03-20 21:33:00,078 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.weight
2025-03-20 21:33:00,079 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.bias
2025-03-20 21:33:00,079 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.weight
2025-03-20 21:33:00,080 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.bias
2025-03-20 21:33:00,080 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.weight
2025-03-20 21:33:00,081 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.bias
2025-03-20 21:33:00,081 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.weight
2025-03-20 21:33:00,082 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.bias
2025-03-20 21:33:00,082 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_weight
2025-03-20 21:33:00,083 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_bias
2025-03-20 21:33:00,083 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.weight
2025-03-20 21:33:00,084 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.bias
2025-03-20 21:33:00,084 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.weight
2025-03-20 21:33:00,085 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.bias
2025-03-20 21:33:00,085 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.weight
2025-03-20 21:33:00,086 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.bias
2025-03-20 21:33:00,086 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.weight
2025-03-20 21:33:00,087 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.bias
2025-03-20 21:33:00,087 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.weight
2025-03-20 21:33:00,088 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.bias
2025-03-20 21:33:00,088 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.weight
2025-03-20 21:33:00,089 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.bias
2025-03-20 21:33:00,089 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: custom_src_module.emb.Embedding.weight
2025-03-20 21:33:00,090 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.weight
2025-03-20 21:33:00,090 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.bias
2025-03-20 21:33:00,091 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.weight
2025-03-20 21:33:00,091 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.bias
2025-03-20 21:33:00,092 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.weight
2025-03-20 21:33:00,092 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.bias
2025-03-20 21:33:00,093 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: embedding.Embedding.weight
2025-03-20 21:33:00,093 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l0
2025-03-20 21:33:00,094 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l0
2025-03-20 21:33:00,094 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l0
2025-03-20 21:33:00,095 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l0
2025-03-20 21:33:00,095 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l1
2025-03-20 21:33:00,096 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l1
2025-03-20 21:33:00,096 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l1
2025-03-20 21:33:00,097 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l1
2025-03-20 21:33:00,097 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.weight
2025-03-20 21:33:00,098 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.bias
2025-03-20 21:33:00,098 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.weight
2025-03-20 21:33:00,098 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.bias
2025-03-20 21:33:00,099 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.weight
2025-03-20 21:33:00,100 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_fold_4/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.bias
2025-03-20 21:33:00,122 - speechbrain.core - INFO - Info: precision arg from hparam file is used
2025-03-20 21:33:00,122 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2025-03-20 21:33:00,123 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2025-03-20 21:33:00,292 - speechbrain.core - INFO - Gradscaler enabled: True. Using precision: fp16.
2025-03-20 21:33:00,294 - speechbrain.core - INFO - ASR Model Statistics:
* Total Number of Trainable Parameters: 13.3M
* Total Number of Parameters: 13.3M
* Trainable Parameters represent 100.0000% of the total size.
2025-03-20 21:33:07,711 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+20-09-31+00
2025-03-20 21:33:18,434 - speechbrain.utils.epoch_loop - INFO - Going into epoch 10
2025-03-20 21:36:17,521 - speechbrain.utils.train_logger - INFO - epoch: 10, lr: 3.49e-04, steps: 8720, optimizer: Adam - train loss: 12.13 - valid loss: 11.26, valid ACC: 7.46e-01, valid WER: 1.22e+02
2025-03-20 21:36:17,906 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+21-36-17+00
2025-03-20 21:36:17,992 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+21-36-17+00
2025-03-20 21:36:18,057 - speechbrain.dataio.dataloader - DEBUG - SaveableDataLoader was requested to load a checkpoint, but the DataLoader has already been iterated. The DataLoader file will be ignored. This is normal in evaluation, when a checkpoint is loaded just to retrieve the best model.
2025-03-20 21:43:07,248 - speechbrain.utils.train_logger - INFO - Epoch loaded: 10 - test loss: 12.60, test ACC: 7.47e-01, test WER: 1.10e+02
2025-03-20 21:43:08,050 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+21-43-07+00
2025-03-20 21:43:08,227 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+20-02-28+00
2025-03-20 21:43:08,231 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+20-09-31+00
2025-03-20 21:43:08,232 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+19-59-58+00
2025-03-20 21:43:08,234 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+21-36-17+00
2025-03-20 21:43:08,235 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+20-07-10+00
2025-03-20 21:43:08,236 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+19-55-15+00
2025-03-20 21:43:08,237 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+19-50-32+00
2025-03-20 21:43:08,239 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+19-52-53+00
2025-03-20 21:43:08,255 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+19-57-37+00
2025-03-20 21:43:08,264 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_fold_4/conformer_small/7775/save/CKPT+2025-03-20+20-04-49+00
