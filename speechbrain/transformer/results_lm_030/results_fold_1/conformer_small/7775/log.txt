2025-03-21 09:32:34,730 - speechbrain.utils.quirks - INFO - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
2025-03-21 09:32:34,731 - speechbrain.utils.quirks - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
2025-03-21 09:32:34,732 - speechbrain.core - INFO - Beginning experiment!
2025-03-21 09:32:34,732 - speechbrain.core - INFO - Experiment folder: results_lm_030/results_fold_1/conformer_small/7775
2025-03-21 09:32:35,926 - speechbrain.utils.superpowers - DEBUG - aiohappyeyeballs==2.6.1
aiohttp==3.11.13
aiosignal==1.3.2
anyio==4.8.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==3.0.0
async-lru==2.0.4
async-timeout==5.0.1
attrs==24.2.0
babel==2.17.0
backcall==0.2.0
beautifulsoup4==4.12.3
bleach==6.2.0
Brotli @ file:///home/conda/feedstock_root/build_artifacts/brotli-split_1648883617327/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2024.8.30
cffi==1.17.1
charset-normalizer==3.4.0
comm==0.2.2
contourpy==1.3.0
cycler==0.12.1
datasets==3.3.2
debugpy==1.8.13
decorator==5.1.1
defusedxml==0.7.1
dill==0.3.8
docopt==0.6.2
exceptiongroup==1.2.2
executing==2.1.0
fastjsonschema==2.21.1
filelock==3.16.1
fonttools==4.56.0
fqdn==1.5.1
frozenlist==1.5.0
fsspec==2024.10.0
h11==0.14.0
h2 @ file:///home/conda/feedstock_root/build_artifacts/h2_1738578511449/work
hpack @ file:///home/conda/feedstock_root/build_artifacts/hpack_1737618293087/work
httpcore==1.0.7
httpx==0.28.1
huggingface-hub==0.26.2
hyperframe @ file:///home/conda/feedstock_root/build_artifacts/hyperframe_1737618333194/work
HyperPyYAML==1.2.2
idna==3.10
importlib_metadata==8.5.0
importlib_resources==6.5.2
ipykernel==6.29.5
ipython==8.12.3
isoduration==20.11.0
jedi==0.19.2
Jinja2==3.1.4
joblib==1.4.2
json5==0.10.0
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
jupyter-events==0.12.0
jupyter-lsp==2.2.5
jupyter_client==8.6.3
jupyter_core==5.7.2
jupyter_server==2.15.0
jupyter_server_terminals==0.5.3
jupyterlab==4.3.5
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
kiwisolver==1.4.7
MarkupSafe==3.0.2
matplotlib==3.9.4
matplotlib-inline==0.1.7
mistune==3.0.2
mpmath==1.3.0
multidict==6.1.0
multiprocess==0.70.16
nbclient==0.10.1
nbconvert==7.16.4
nbformat==5.10.4
nest-asyncio==1.6.0
networkx==3.2.1
notebook_shim==0.2.4
numpy==2.0.2
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-cusparselt-cu12==0.6.2
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.4.127
overrides==7.7.0
packaging==24.1
pandas==2.2.3
pandocfilters==1.5.1
parso==0.8.4
pexpect==4.9.0
pickleshare==0.7.5
pillow==11.1.0
pipreqs==0.5.0
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1733344503739/work
platformdirs==4.3.6
prometheus_client==0.21.1
prompt_toolkit==3.0.48
propcache==0.3.0
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==19.0.1
pycparser==2.22
Pygments==2.18.0
pyparsing==3.2.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1733217236728/work
python-dateutil==2.9.0.post0
python-json-logger==3.2.1
pytz==2025.1
PyYAML==6.0.2
pyzmq==26.2.0
referencing==0.35.1
requests==2.32.3
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.22.3
ruamel.yaml==0.18.6
ruamel.yaml.clib==0.2.12
scikit-learn==1.5.2
scipy==1.13.1
Send2Trash==1.8.3
sentencepiece==0.2.0
six==1.17.0
sniffio==1.3.1
soundfile==0.12.1
soupsieve==2.6
sox==1.5.0
speechbrain==1.0.2
stack-data==0.6.3
sympy==1.13.1
terminado==0.18.1
threadpoolctl==3.5.0
tinycss2==1.4.0
tomli==2.2.1
torch==2.5.1
torchaudio==2.5.1
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
triton==3.1.0
types-python-dateutil==2.9.0.20241206
typing_extensions==4.12.2
typing_utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1733331286120/work
tzdata==2025.1
uri-template==1.3.0
urllib3==2.2.3
wcwidth==0.2.13
webcolors==24.11.1
webencodings==0.5.1
websocket-client==1.8.0
xxhash==3.5.0
yarg==0.1.9
yarl==1.18.3
zipp==3.21.0


2025-03-21 09:32:35,961 - speechbrain.utils.superpowers - DEBUG - e36d022


2025-03-21 09:32:36,047 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results_lm_030/results_fold_1/conformer_small/7775/save.
2025-03-21 09:32:36,048 - speechbrain.utils.fetching - DEBUG - Fetch: Local file found, creating symlink '/scratch/flatala/speechbrain/transformer/LM_weights_13_epochs/lm.ckpt' -> '/scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt'
2025-03-21 09:32:36,079 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["lm"] = /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt
2025-03-21 09:32:36,080 - speechbrain.utils.fetching - DEBUG - Fetch: Local file found, creating symlink '/scratch/flatala/speechbrain/transformer/LM_weights_13_epochs/tokenizer.ckpt' -> '/scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/tokenizer.ckpt'
2025-03-21 09:32:36,081 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["tokenizer"] = /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/tokenizer.ckpt
2025-03-21 09:32:36,081 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2025-03-21 09:32:36,081 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): lm -> /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt
2025-03-21 09:32:36,081 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): tokenizer -> /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/tokenizer.ckpt
2025-03-21 09:32:36,147 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: positional_encoding.pe
2025-03-21 09:32:36,148 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_weight
2025-03-21 09:32:36,148 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_bias
2025-03-21 09:32:36,149 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.weight
2025-03-21 09:32:36,150 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.bias
2025-03-21 09:32:36,150 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.weight
2025-03-21 09:32:36,151 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.bias
2025-03-21 09:32:36,152 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.weight
2025-03-21 09:32:36,152 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.bias
2025-03-21 09:32:36,153 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.weight
2025-03-21 09:32:36,154 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.bias
2025-03-21 09:32:36,154 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.weight
2025-03-21 09:32:36,155 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.bias
2025-03-21 09:32:36,155 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_weight
2025-03-21 09:32:36,156 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_bias
2025-03-21 09:32:36,156 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.weight
2025-03-21 09:32:36,157 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.bias
2025-03-21 09:32:36,158 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.weight
2025-03-21 09:32:36,158 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.bias
2025-03-21 09:32:36,159 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.weight
2025-03-21 09:32:36,160 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.bias
2025-03-21 09:32:36,160 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.weight
2025-03-21 09:32:36,161 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.bias
2025-03-21 09:32:36,161 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.weight
2025-03-21 09:32:36,162 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.bias
2025-03-21 09:32:36,163 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_weight
2025-03-21 09:32:36,163 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_bias
2025-03-21 09:32:36,164 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.weight
2025-03-21 09:32:36,164 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.bias
2025-03-21 09:32:36,165 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.weight
2025-03-21 09:32:36,166 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.bias
2025-03-21 09:32:36,167 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.weight
2025-03-21 09:32:36,167 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.bias
2025-03-21 09:32:36,168 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.weight
2025-03-21 09:32:36,169 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.bias
2025-03-21 09:32:36,169 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.weight
2025-03-21 09:32:36,170 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.bias
2025-03-21 09:32:36,170 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_weight
2025-03-21 09:32:36,171 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_bias
2025-03-21 09:32:36,172 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.weight
2025-03-21 09:32:36,172 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.bias
2025-03-21 09:32:36,173 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.weight
2025-03-21 09:32:36,174 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.bias
2025-03-21 09:32:36,174 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.weight
2025-03-21 09:32:36,175 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.bias
2025-03-21 09:32:36,175 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.weight
2025-03-21 09:32:36,176 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.bias
2025-03-21 09:32:36,177 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.weight
2025-03-21 09:32:36,177 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.bias
2025-03-21 09:32:36,178 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_weight
2025-03-21 09:32:36,178 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_bias
2025-03-21 09:32:36,179 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.weight
2025-03-21 09:32:36,179 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.bias
2025-03-21 09:32:36,180 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.weight
2025-03-21 09:32:36,181 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.bias
2025-03-21 09:32:36,181 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.weight
2025-03-21 09:32:36,182 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.bias
2025-03-21 09:32:36,182 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.weight
2025-03-21 09:32:36,183 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.bias
2025-03-21 09:32:36,183 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.weight
2025-03-21 09:32:36,184 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.bias
2025-03-21 09:32:36,184 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_weight
2025-03-21 09:32:36,185 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_bias
2025-03-21 09:32:36,186 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.weight
2025-03-21 09:32:36,186 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.bias
2025-03-21 09:32:36,187 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.weight
2025-03-21 09:32:36,187 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.bias
2025-03-21 09:32:36,188 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.weight
2025-03-21 09:32:36,188 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.bias
2025-03-21 09:32:36,189 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.weight
2025-03-21 09:32:36,190 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.bias
2025-03-21 09:32:36,190 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.weight
2025-03-21 09:32:36,191 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.bias
2025-03-21 09:32:36,192 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_weight
2025-03-21 09:32:36,192 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_bias
2025-03-21 09:32:36,193 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.weight
2025-03-21 09:32:36,193 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.bias
2025-03-21 09:32:36,194 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.weight
2025-03-21 09:32:36,195 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.bias
2025-03-21 09:32:36,195 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.weight
2025-03-21 09:32:36,196 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.bias
2025-03-21 09:32:36,197 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.weight
2025-03-21 09:32:36,197 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.bias
2025-03-21 09:32:36,198 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.weight
2025-03-21 09:32:36,199 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.bias
2025-03-21 09:32:36,199 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_weight
2025-03-21 09:32:36,200 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_bias
2025-03-21 09:32:36,201 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.weight
2025-03-21 09:32:36,201 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.bias
2025-03-21 09:32:36,202 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.weight
2025-03-21 09:32:36,203 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.bias
2025-03-21 09:32:36,203 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.weight
2025-03-21 09:32:36,204 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.bias
2025-03-21 09:32:36,204 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.weight
2025-03-21 09:32:36,205 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.bias
2025-03-21 09:32:36,205 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.weight
2025-03-21 09:32:36,206 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.bias
2025-03-21 09:32:36,206 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_weight
2025-03-21 09:32:36,207 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_bias
2025-03-21 09:32:36,207 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.weight
2025-03-21 09:32:36,208 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.bias
2025-03-21 09:32:36,208 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.weight
2025-03-21 09:32:36,209 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.bias
2025-03-21 09:32:36,209 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.weight
2025-03-21 09:32:36,210 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.bias
2025-03-21 09:32:36,211 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.weight
2025-03-21 09:32:36,211 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.bias
2025-03-21 09:32:36,212 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.weight
2025-03-21 09:32:36,212 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.bias
2025-03-21 09:32:36,213 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_weight
2025-03-21 09:32:36,214 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_bias
2025-03-21 09:32:36,214 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.weight
2025-03-21 09:32:36,215 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.bias
2025-03-21 09:32:36,216 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.weight
2025-03-21 09:32:36,216 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.bias
2025-03-21 09:32:36,217 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.weight
2025-03-21 09:32:36,218 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.bias
2025-03-21 09:32:36,218 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.weight
2025-03-21 09:32:36,219 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.bias
2025-03-21 09:32:36,220 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.weight
2025-03-21 09:32:36,220 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.bias
2025-03-21 09:32:36,221 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_weight
2025-03-21 09:32:36,221 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_bias
2025-03-21 09:32:36,222 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.weight
2025-03-21 09:32:36,222 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.bias
2025-03-21 09:32:36,223 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.weight
2025-03-21 09:32:36,223 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.bias
2025-03-21 09:32:36,224 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.weight
2025-03-21 09:32:36,224 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.bias
2025-03-21 09:32:36,225 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.weight
2025-03-21 09:32:36,225 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.bias
2025-03-21 09:32:36,226 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.weight
2025-03-21 09:32:36,226 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.bias
2025-03-21 09:32:36,227 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_weight
2025-03-21 09:32:36,227 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_bias
2025-03-21 09:32:36,228 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.weight
2025-03-21 09:32:36,228 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.bias
2025-03-21 09:32:36,229 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.weight
2025-03-21 09:32:36,229 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.bias
2025-03-21 09:32:36,230 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.weight
2025-03-21 09:32:36,230 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.bias
2025-03-21 09:32:36,231 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.weight
2025-03-21 09:32:36,231 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.bias
2025-03-21 09:32:36,232 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.weight
2025-03-21 09:32:36,232 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.bias
2025-03-21 09:32:36,233 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.weight
2025-03-21 09:32:36,233 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.bias
2025-03-21 09:32:36,234 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: custom_src_module.emb.Embedding.weight
2025-03-21 09:32:36,234 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.weight
2025-03-21 09:32:36,235 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.bias
2025-03-21 09:32:36,235 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.weight
2025-03-21 09:32:36,236 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.bias
2025-03-21 09:32:36,236 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.weight
2025-03-21 09:32:36,237 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.bias
2025-03-21 09:32:36,237 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: embedding.Embedding.weight
2025-03-21 09:32:36,238 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l0
2025-03-21 09:32:36,238 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l0
2025-03-21 09:32:36,239 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l0
2025-03-21 09:32:36,239 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l0
2025-03-21 09:32:36,240 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l1
2025-03-21 09:32:36,240 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l1
2025-03-21 09:32:36,241 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l1
2025-03-21 09:32:36,241 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l1
2025-03-21 09:32:36,241 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.weight
2025-03-21 09:32:36,242 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.bias
2025-03-21 09:32:36,243 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.weight
2025-03-21 09:32:36,243 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.bias
2025-03-21 09:32:36,243 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.weight
2025-03-21 09:32:36,244 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/flatala/speechbrain/transformer/results_lm_030/results_fold_1/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.bias
2025-03-21 09:32:36,248 - speechbrain.core - INFO - Info: precision arg from hparam file is used
2025-03-21 09:32:36,249 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2025-03-21 09:32:36,249 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2025-03-21 09:32:36,419 - speechbrain.core - INFO - Gradscaler enabled: True. Using precision: fp16.
2025-03-21 09:32:36,421 - speechbrain.core - INFO - ASR Model Statistics:
* Total Number of Trainable Parameters: 13.3M
* Total Number of Parameters: 13.3M
* Trainable Parameters represent 100.0000% of the total size.
2025-03-21 09:32:37,594 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2025-03-21 09:32:37,594 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2025-03-21 09:33:24,402 - speechbrain.utils.train_logger - INFO - epoch: 1, lr: 8.32e-06, steps: 105, optimizer: Adam - train loss: 2.52e+02 - valid loss: 1.63e+02, valid ACC: 1.89e-01
2025-03-21 09:33:25,085 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-33-24+00
2025-03-21 09:33:25,094 - speechbrain.utils.epoch_loop - INFO - Going into epoch 2
2025-03-21 09:34:12,126 - speechbrain.utils.train_logger - INFO - epoch: 2, lr: 1.67e-05, steps: 210, optimizer: Adam - train loss: 1.23e+02 - valid loss: 1.15e+02, valid ACC: 1.89e-01
2025-03-21 09:34:12,666 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-34-12+00
2025-03-21 09:34:12,681 - speechbrain.utils.epoch_loop - INFO - Going into epoch 3
2025-03-21 09:34:58,781 - speechbrain.utils.train_logger - INFO - epoch: 3, lr: 2.51e-05, steps: 315, optimizer: Adam - train loss: 97.60 - valid loss: 93.89, valid ACC: 1.89e-01
2025-03-21 09:34:59,296 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-34-58+00
2025-03-21 09:34:59,319 - speechbrain.utils.epoch_loop - INFO - Going into epoch 4
2025-03-21 09:35:46,059 - speechbrain.utils.train_logger - INFO - epoch: 4, lr: 3.35e-05, steps: 420, optimizer: Adam - train loss: 75.22 - valid loss: 68.45, valid ACC: 1.89e-01
2025-03-21 09:35:46,591 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-35-46+00
2025-03-21 09:35:46,626 - speechbrain.utils.epoch_loop - INFO - Going into epoch 5
2025-03-21 09:36:33,625 - speechbrain.utils.train_logger - INFO - epoch: 5, lr: 4.19e-05, steps: 525, optimizer: Adam - train loss: 52.58 - valid loss: 45.89, valid ACC: 1.93e-01
2025-03-21 09:36:34,274 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-36-33+00
2025-03-21 09:36:34,327 - speechbrain.utils.epoch_loop - INFO - Going into epoch 6
2025-03-21 09:37:20,189 - speechbrain.utils.train_logger - INFO - epoch: 6, lr: 5.03e-05, steps: 630, optimizer: Adam - train loss: 36.60 - valid loss: 33.39, valid ACC: 2.13e-01
2025-03-21 09:37:20,945 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-37-20+00
2025-03-21 09:37:21,024 - speechbrain.utils.epoch_loop - INFO - Going into epoch 7
2025-03-21 09:38:08,385 - speechbrain.utils.train_logger - INFO - epoch: 7, lr: 5.87e-05, steps: 735, optimizer: Adam - train loss: 29.46 - valid loss: 28.97, valid ACC: 2.65e-01
2025-03-21 09:38:08,799 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-38-08+00
2025-03-21 09:38:08,893 - speechbrain.utils.epoch_loop - INFO - Going into epoch 8
2025-03-21 09:38:54,364 - speechbrain.utils.train_logger - INFO - epoch: 8, lr: 6.71e-05, steps: 840, optimizer: Adam - train loss: 26.91 - valid loss: 27.08, valid ACC: 3.16e-01
2025-03-21 09:38:55,112 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-38-54+00
2025-03-21 09:38:55,230 - speechbrain.utils.epoch_loop - INFO - Going into epoch 9
2025-03-21 09:39:42,302 - speechbrain.utils.train_logger - INFO - epoch: 9, lr: 7.55e-05, steps: 945, optimizer: Adam - train loss: 25.62 - valid loss: 25.92, valid ACC: 3.44e-01
2025-03-21 09:39:42,905 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-39-42+00
2025-03-21 09:39:43,651 - speechbrain.utils.epoch_loop - INFO - Going into epoch 10
2025-03-21 09:41:52,037 - speechbrain.utils.train_logger - INFO - epoch: 10, lr: 8.39e-05, steps: 1050, optimizer: Adam - train loss: 24.51 - valid loss: 24.74, valid ACC: 3.96e-01, valid WER: 1.06e+02
2025-03-21 09:41:52,379 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-41-52+00
2025-03-21 09:41:52,550 - speechbrain.utils.epoch_loop - INFO - Going into epoch 11
2025-03-21 09:42:38,672 - speechbrain.utils.train_logger - INFO - epoch: 11, lr: 9.23e-05, steps: 1155, optimizer: Adam - train loss: 23.37 - valid loss: 23.41, valid ACC: 4.56e-01
2025-03-21 09:42:38,930 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-42-38+00
2025-03-21 09:42:39,133 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-33-24+00
2025-03-21 09:42:39,134 - speechbrain.utils.epoch_loop - INFO - Going into epoch 12
2025-03-21 09:43:25,744 - speechbrain.utils.train_logger - INFO - epoch: 12, lr: 1.01e-04, steps: 1260, optimizer: Adam - train loss: 22.37 - valid loss: 22.37, valid ACC: 5.00e-01
2025-03-21 09:43:26,065 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-43-25+00
2025-03-21 09:43:26,282 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-34-12+00
2025-03-21 09:43:26,282 - speechbrain.utils.epoch_loop - INFO - Going into epoch 13
2025-03-21 09:44:13,625 - speechbrain.utils.train_logger - INFO - epoch: 13, lr: 1.09e-04, steps: 1365, optimizer: Adam - train loss: 21.37 - valid loss: 21.40, valid ACC: 5.63e-01
2025-03-21 09:44:13,897 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-44-13+00
2025-03-21 09:44:14,130 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-34-58+00
2025-03-21 09:44:14,130 - speechbrain.utils.epoch_loop - INFO - Going into epoch 14
2025-03-21 09:45:05,438 - speechbrain.utils.train_logger - INFO - epoch: 14, lr: 1.18e-04, steps: 1470, optimizer: Adam - train loss: 20.50 - valid loss: 20.50, valid ACC: 5.94e-01
2025-03-21 09:45:05,943 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-45-05+00
2025-03-21 09:45:06,188 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-35-46+00
2025-03-21 09:45:06,188 - speechbrain.utils.epoch_loop - INFO - Going into epoch 15
2025-03-21 09:45:52,886 - speechbrain.utils.train_logger - INFO - epoch: 15, lr: 1.26e-04, steps: 1575, optimizer: Adam - train loss: 19.73 - valid loss: 19.81, valid ACC: 6.22e-01
2025-03-21 09:45:53,727 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-45-52+00
2025-03-21 09:45:54,000 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-36-33+00
2025-03-21 09:45:54,000 - speechbrain.utils.epoch_loop - INFO - Going into epoch 16
2025-03-21 09:46:40,449 - speechbrain.utils.train_logger - INFO - epoch: 16, lr: 1.34e-04, steps: 1680, optimizer: Adam - train loss: 19.00 - valid loss: 19.10, valid ACC: 6.46e-01
2025-03-21 09:46:41,010 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-46-40+00
2025-03-21 09:46:41,295 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-37-20+00
2025-03-21 09:46:41,296 - speechbrain.utils.epoch_loop - INFO - Going into epoch 17
2025-03-21 09:47:27,911 - speechbrain.utils.train_logger - INFO - epoch: 17, lr: 1.43e-04, steps: 1785, optimizer: Adam - train loss: 18.28 - valid loss: 18.51, valid ACC: 6.65e-01
2025-03-21 09:47:28,543 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-47-27+00
2025-03-21 09:47:28,850 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-38-08+00
2025-03-21 09:47:28,850 - speechbrain.utils.epoch_loop - INFO - Going into epoch 18
2025-03-21 09:48:14,978 - speechbrain.utils.train_logger - INFO - epoch: 18, lr: 1.51e-04, steps: 1890, optimizer: Adam - train loss: 17.55 - valid loss: 17.82, valid ACC: 6.85e-01
2025-03-21 09:48:15,508 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-48-14+00
2025-03-21 09:48:15,830 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-38-54+00
2025-03-21 09:48:15,831 - speechbrain.utils.epoch_loop - INFO - Going into epoch 19
2025-03-21 09:49:02,182 - speechbrain.utils.train_logger - INFO - epoch: 19, lr: 1.60e-04, steps: 1995, optimizer: Adam - train loss: 17.01 - valid loss: 17.64, valid ACC: 7.00e-01
2025-03-21 09:49:02,672 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-49-02+00
2025-03-21 09:49:03,021 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-39-42+00
2025-03-21 09:49:03,022 - speechbrain.utils.epoch_loop - INFO - Going into epoch 20
2025-03-21 09:50:55,025 - speechbrain.utils.train_logger - INFO - epoch: 20, lr: 1.68e-04, steps: 2100, optimizer: Adam - train loss: 16.59 - valid loss: 17.23, valid ACC: 7.11e-01, valid WER: 1.11e+02
2025-03-21 09:50:55,748 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-50-55+00
2025-03-21 09:50:56,095 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-41-52+00
2025-03-21 09:50:56,096 - speechbrain.utils.epoch_loop - INFO - Going into epoch 21
2025-03-21 09:51:42,074 - speechbrain.utils.train_logger - INFO - epoch: 21, lr: 1.76e-04, steps: 2205, optimizer: Adam - train loss: 16.19 - valid loss: 17.01, valid ACC: 7.22e-01
2025-03-21 09:51:42,481 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-51-42+00
2025-03-21 09:51:42,837 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-42-38+00
2025-03-21 09:51:42,838 - speechbrain.utils.epoch_loop - INFO - Going into epoch 22
2025-03-21 09:52:28,753 - speechbrain.utils.train_logger - INFO - epoch: 22, lr: 1.85e-04, steps: 2310, optimizer: Adam - train loss: 15.85 - valid loss: 16.72, valid ACC: 7.29e-01
2025-03-21 09:52:29,159 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-52-28+00
2025-03-21 09:52:29,548 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-43-25+00
2025-03-21 09:52:29,548 - speechbrain.utils.epoch_loop - INFO - Going into epoch 23
2025-03-21 09:53:15,840 - speechbrain.utils.train_logger - INFO - epoch: 23, lr: 1.93e-04, steps: 2415, optimizer: Adam - train loss: 15.55 - valid loss: 16.64, valid ACC: 7.34e-01
2025-03-21 09:53:16,221 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-53-15+00
2025-03-21 09:53:16,612 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-44-13+00
2025-03-21 09:53:16,613 - speechbrain.utils.epoch_loop - INFO - Going into epoch 24
2025-03-21 09:54:03,285 - speechbrain.utils.train_logger - INFO - epoch: 24, lr: 2.02e-04, steps: 2520, optimizer: Adam - train loss: 15.31 - valid loss: 16.32, valid ACC: 7.40e-01
2025-03-21 09:54:03,677 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-54-03+00
2025-03-21 09:54:04,086 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-45-05+00
2025-03-21 09:54:04,087 - speechbrain.utils.epoch_loop - INFO - Going into epoch 25
2025-03-21 09:54:51,510 - speechbrain.utils.train_logger - INFO - epoch: 25, lr: 2.10e-04, steps: 2625, optimizer: Adam - train loss: 15.09 - valid loss: 16.18, valid ACC: 7.45e-01
2025-03-21 09:54:51,907 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-54-51+00
2025-03-21 09:54:52,328 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-45-52+00
2025-03-21 09:54:52,328 - speechbrain.utils.epoch_loop - INFO - Going into epoch 26
2025-03-21 09:55:38,572 - speechbrain.utils.train_logger - INFO - epoch: 26, lr: 2.18e-04, steps: 2730, optimizer: Adam - train loss: 14.85 - valid loss: 16.01, valid ACC: 7.44e-01
2025-03-21 09:55:38,934 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-55-38+00
2025-03-21 09:55:39,406 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-46-40+00
2025-03-21 09:55:39,406 - speechbrain.utils.epoch_loop - INFO - Going into epoch 27
2025-03-21 09:56:25,900 - speechbrain.utils.train_logger - INFO - epoch: 27, lr: 2.27e-04, steps: 2835, optimizer: Adam - train loss: 14.65 - valid loss: 15.67, valid ACC: 7.45e-01
2025-03-21 09:56:26,441 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-56-25+00
2025-03-21 09:56:26,902 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-47-27+00
2025-03-21 09:56:26,903 - speechbrain.utils.epoch_loop - INFO - Going into epoch 28
2025-03-21 09:57:12,868 - speechbrain.utils.train_logger - INFO - epoch: 28, lr: 2.35e-04, steps: 2940, optimizer: Adam - train loss: 14.41 - valid loss: 15.66, valid ACC: 7.46e-01
2025-03-21 09:57:13,219 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-57-12+00
2025-03-21 09:57:13,715 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-48-14+00
2025-03-21 09:57:13,715 - speechbrain.utils.epoch_loop - INFO - Going into epoch 29
2025-03-21 09:57:59,210 - speechbrain.utils.train_logger - INFO - epoch: 29, lr: 2.44e-04, steps: 3045, optimizer: Adam - train loss: 14.19 - valid loss: 15.20, valid ACC: 7.55e-01
2025-03-21 09:57:59,729 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-57-59+00
2025-03-21 09:58:00,360 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-49-02+00
2025-03-21 09:58:00,360 - speechbrain.utils.epoch_loop - INFO - Going into epoch 30
2025-03-21 10:00:07,972 - speechbrain.utils.train_logger - INFO - epoch: 30, lr: 2.52e-04, steps: 3150, optimizer: Adam - train loss: 13.97 - valid loss: 15.02, valid ACC: 7.55e-01, valid WER: 1.17e+02
2025-03-21 10:00:08,530 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-00-07+00
2025-03-21 10:00:09,032 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-50-55+00
2025-03-21 10:00:09,032 - speechbrain.utils.epoch_loop - INFO - Going into epoch 31
2025-03-21 10:00:54,847 - speechbrain.utils.train_logger - INFO - epoch: 31, lr: 2.60e-04, steps: 3255, optimizer: Adam - train loss: 13.80 - valid loss: 14.72, valid ACC: 7.58e-01
2025-03-21 10:00:55,333 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-00-54+00
2025-03-21 10:00:55,883 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-51-42+00
2025-03-21 10:00:55,883 - speechbrain.utils.epoch_loop - INFO - Going into epoch 32
2025-03-21 10:01:42,479 - speechbrain.utils.train_logger - INFO - epoch: 32, lr: 2.69e-04, steps: 3360, optimizer: Adam - train loss: 13.62 - valid loss: 14.77, valid ACC: 7.57e-01
2025-03-21 10:01:43,076 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-01-42+00
2025-03-21 10:01:43,631 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-52-28+00
2025-03-21 10:01:43,631 - speechbrain.utils.epoch_loop - INFO - Going into epoch 33
2025-03-21 10:02:29,828 - speechbrain.utils.train_logger - INFO - epoch: 33, lr: 2.77e-04, steps: 3465, optimizer: Adam - train loss: 13.41 - valid loss: 14.65, valid ACC: 7.62e-01
2025-03-21 10:02:30,504 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-02-29+00
2025-03-21 10:02:31,092 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-53-15+00
2025-03-21 10:02:31,092 - speechbrain.utils.epoch_loop - INFO - Going into epoch 34
2025-03-21 10:03:23,820 - speechbrain.utils.train_logger - INFO - epoch: 34, lr: 2.86e-04, steps: 3570, optimizer: Adam - train loss: 13.20 - valid loss: 14.35, valid ACC: 7.66e-01
2025-03-21 10:03:25,096 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-03-23+00
2025-03-21 10:03:25,969 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-54-03+00
2025-03-21 10:03:25,969 - speechbrain.utils.epoch_loop - INFO - Going into epoch 35
2025-03-21 10:04:14,807 - speechbrain.utils.train_logger - INFO - epoch: 35, lr: 2.94e-04, steps: 3675, optimizer: Adam - train loss: 12.99 - valid loss: 13.97, valid ACC: 7.70e-01
2025-03-21 10:04:15,583 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-04-14+00
2025-03-21 10:04:16,194 - speechbrain.utils.epoch_loop - INFO - Going into epoch 36
2025-03-21 10:05:02,494 - speechbrain.utils.train_logger - INFO - epoch: 36, lr: 3.02e-04, steps: 3780, optimizer: Adam - train loss: 12.70 - valid loss: 13.96, valid ACC: 7.72e-01
2025-03-21 10:05:02,836 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-05-02+00
2025-03-21 10:05:03,489 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-54-51+00
2025-03-21 10:05:03,491 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-55-38+00
2025-03-21 10:05:03,491 - speechbrain.utils.epoch_loop - INFO - Going into epoch 37
2025-03-21 10:05:48,878 - speechbrain.utils.train_logger - INFO - epoch: 37, lr: 3.11e-04, steps: 3885, optimizer: Adam - train loss: 12.50 - valid loss: 14.27, valid ACC: 7.65e-01
2025-03-21 10:05:49,343 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-05-48+00
2025-03-21 10:05:49,989 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-56-25+00
2025-03-21 10:05:49,990 - speechbrain.utils.epoch_loop - INFO - Going into epoch 38
2025-03-21 10:06:36,653 - speechbrain.utils.train_logger - INFO - epoch: 38, lr: 3.19e-04, steps: 3990, optimizer: Adam - train loss: 12.34 - valid loss: 13.92, valid ACC: 7.68e-01
2025-03-21 10:06:37,075 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-06-36+00
2025-03-21 10:06:37,725 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-57-12+00
2025-03-21 10:06:37,726 - speechbrain.utils.epoch_loop - INFO - Going into epoch 39
2025-03-21 10:07:24,797 - speechbrain.utils.train_logger - INFO - epoch: 39, lr: 3.28e-04, steps: 4095, optimizer: Adam - train loss: 12.13 - valid loss: 13.37, valid ACC: 7.72e-01
2025-03-21 10:07:25,192 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-07-24+00
2025-03-21 10:07:25,905 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+09-57-59+00
2025-03-21 10:07:25,906 - speechbrain.utils.epoch_loop - INFO - Going into epoch 40
2025-03-21 10:09:25,906 - speechbrain.utils.train_logger - INFO - epoch: 40, lr: 3.36e-04, steps: 4200, optimizer: Adam - train loss: 11.92 - valid loss: 13.39, valid ACC: 7.77e-01, valid WER: 1.03e+02
2025-03-21 10:09:26,862 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-09-25+00
2025-03-21 10:09:27,545 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-00-07+00
2025-03-21 10:09:27,545 - speechbrain.utils.epoch_loop - INFO - Going into epoch 41
2025-03-21 10:10:14,021 - speechbrain.utils.train_logger - INFO - epoch: 41, lr: 3.44e-04, steps: 4305, optimizer: Adam - train loss: 11.68 - valid loss: 13.34, valid ACC: 7.73e-01
2025-03-21 10:10:14,471 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-10-14+00
2025-03-21 10:10:15,153 - speechbrain.utils.epoch_loop - INFO - Going into epoch 42
2025-03-21 10:11:00,868 - speechbrain.utils.train_logger - INFO - epoch: 42, lr: 3.53e-04, steps: 4410, optimizer: Adam - train loss: 11.51 - valid loss: 13.15, valid ACC: 7.77e-01
2025-03-21 10:11:01,278 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-11-00+00
2025-03-21 10:11:02,068 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-00-54+00
2025-03-21 10:11:02,071 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-01-42+00
2025-03-21 10:11:02,071 - speechbrain.utils.epoch_loop - INFO - Going into epoch 43
2025-03-21 10:11:48,484 - speechbrain.utils.train_logger - INFO - epoch: 43, lr: 3.61e-04, steps: 4515, optimizer: Adam - train loss: 11.29 - valid loss: 13.35, valid ACC: 7.76e-01
2025-03-21 10:11:48,801 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-11-48+00
2025-03-21 10:11:49,525 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-02-29+00
2025-03-21 10:11:49,525 - speechbrain.utils.epoch_loop - INFO - Going into epoch 44
2025-03-21 10:12:35,720 - speechbrain.utils.train_logger - INFO - epoch: 44, lr: 3.70e-04, steps: 4620, optimizer: Adam - train loss: 11.02 - valid loss: 12.93, valid ACC: 7.87e-01
2025-03-21 10:12:36,020 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-12-35+00
2025-03-21 10:12:36,785 - speechbrain.utils.epoch_loop - INFO - Going into epoch 45
2025-03-21 10:13:23,952 - speechbrain.utils.train_logger - INFO - epoch: 45, lr: 3.78e-04, steps: 4725, optimizer: Adam - train loss: 10.70 - valid loss: 12.38, valid ACC: 7.89e-01
2025-03-21 10:13:24,274 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-13-23+00
2025-03-21 10:13:25,083 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-03-23+00
2025-03-21 10:13:25,083 - speechbrain.utils.epoch_loop - INFO - Going into epoch 46
2025-03-21 10:14:12,124 - speechbrain.utils.train_logger - INFO - epoch: 46, lr: 3.86e-04, steps: 4830, optimizer: Adam - train loss: 10.42 - valid loss: 12.57, valid ACC: 7.87e-01
2025-03-21 10:14:12,543 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-14-12+00
2025-03-21 10:14:13,398 - speechbrain.utils.epoch_loop - INFO - Going into epoch 47
2025-03-21 10:14:59,204 - speechbrain.utils.train_logger - INFO - epoch: 47, lr: 3.95e-04, steps: 4935, optimizer: Adam - train loss: 10.22 - valid loss: 12.32, valid ACC: 7.96e-01
2025-03-21 10:14:59,652 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-14-59+00
2025-03-21 10:15:00,601 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-05-48+00
2025-03-21 10:15:00,603 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-04-14+00
2025-03-21 10:15:00,603 - speechbrain.utils.epoch_loop - INFO - Going into epoch 48
2025-03-21 10:15:46,805 - speechbrain.utils.train_logger - INFO - epoch: 48, lr: 4.03e-04, steps: 5040, optimizer: Adam - train loss: 9.91 - valid loss: 12.94, valid ACC: 7.92e-01
2025-03-21 10:15:47,315 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-15-46+00
2025-03-21 10:15:48,162 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-06-36+00
2025-03-21 10:15:48,162 - speechbrain.utils.epoch_loop - INFO - Going into epoch 49
2025-03-21 10:16:35,008 - speechbrain.utils.train_logger - INFO - epoch: 49, lr: 4.12e-04, steps: 5145, optimizer: Adam - train loss: 9.59 - valid loss: 12.51, valid ACC: 7.96e-01
2025-03-21 10:16:35,534 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-16-35+00
2025-03-21 10:16:36,397 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-07-24+00
2025-03-21 10:16:36,399 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-05-02+00
2025-03-21 10:16:36,399 - speechbrain.utils.epoch_loop - INFO - Going into epoch 50
2025-03-21 10:18:33,733 - speechbrain.utils.train_logger - INFO - epoch: 50, lr: 4.20e-04, steps: 5250, optimizer: Adam - train loss: 9.25 - valid loss: 12.22, valid ACC: 8.01e-01, valid WER: 88.09
2025-03-21 10:18:34,049 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-18-33+00
2025-03-21 10:18:34,896 - speechbrain.utils.epoch_loop - INFO - Going into epoch 51
2025-03-21 10:19:20,874 - speechbrain.utils.train_logger - INFO - epoch: 51, lr: 4.28e-04, steps: 5355, optimizer: Adam - train loss: 8.92 - valid loss: 11.06, valid ACC: 8.09e-01
2025-03-21 10:19:21,163 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-19-20+00
2025-03-21 10:19:22,097 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-10-14+00
2025-03-21 10:19:22,097 - speechbrain.utils.epoch_loop - INFO - Going into epoch 52
2025-03-21 10:20:09,122 - speechbrain.utils.train_logger - INFO - epoch: 52, lr: 4.37e-04, steps: 5460, optimizer: Adam - train loss: 8.66 - valid loss: 10.92, valid ACC: 8.12e-01
2025-03-21 10:20:09,428 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-20-09+00
2025-03-21 10:20:10,350 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-09-25+00
2025-03-21 10:20:10,350 - speechbrain.utils.epoch_loop - INFO - Going into epoch 53
2025-03-21 10:21:04,065 - speechbrain.utils.train_logger - INFO - epoch: 53, lr: 4.45e-04, steps: 5565, optimizer: Adam - train loss: 8.27 - valid loss: 11.24, valid ACC: 8.06e-01
2025-03-21 10:21:04,624 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-21-04+00
2025-03-21 10:21:05,638 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-11-48+00
2025-03-21 10:21:05,640 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-11-00+00
2025-03-21 10:21:05,640 - speechbrain.utils.epoch_loop - INFO - Going into epoch 54
2025-03-21 10:21:51,930 - speechbrain.utils.train_logger - INFO - epoch: 54, lr: 4.54e-04, steps: 5670, optimizer: Adam - train loss: 8.11 - valid loss: 11.18, valid ACC: 8.13e-01
2025-03-21 10:21:52,462 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-21-51+00
2025-03-21 10:21:53,461 - speechbrain.utils.epoch_loop - INFO - Going into epoch 55
2025-03-21 10:22:39,511 - speechbrain.utils.train_logger - INFO - epoch: 55, lr: 4.62e-04, steps: 5775, optimizer: Adam - train loss: 7.75 - valid loss: 10.76, valid ACC: 8.13e-01
2025-03-21 10:22:40,073 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-22-39+00
2025-03-21 10:22:41,063 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-12-35+00
2025-03-21 10:22:41,063 - speechbrain.utils.epoch_loop - INFO - Going into epoch 56
2025-03-21 10:23:28,188 - speechbrain.utils.train_logger - INFO - epoch: 56, lr: 4.70e-04, steps: 5880, optimizer: Adam - train loss: 7.44 - valid loss: 11.16, valid ACC: 8.11e-01
2025-03-21 10:23:28,879 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-23-28+00
2025-03-21 10:23:29,952 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-13-23+00
2025-03-21 10:23:29,954 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-14-12+00
2025-03-21 10:23:29,954 - speechbrain.utils.epoch_loop - INFO - Going into epoch 57
2025-03-21 10:24:16,277 - speechbrain.utils.train_logger - INFO - epoch: 57, lr: 4.79e-04, steps: 5985, optimizer: Adam - train loss: 7.18 - valid loss: 9.99, valid ACC: 8.25e-01
2025-03-21 10:24:16,782 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-24-16+00
2025-03-21 10:24:17,734 - speechbrain.utils.epoch_loop - INFO - Going into epoch 58
2025-03-21 10:25:04,721 - speechbrain.utils.train_logger - INFO - epoch: 58, lr: 4.87e-04, steps: 6090, optimizer: Adam - train loss: 6.94 - valid loss: 10.31, valid ACC: 8.25e-01
2025-03-21 10:25:05,117 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-25-04+00
2025-03-21 10:25:06,262 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-14-59+00
2025-03-21 10:25:06,264 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-15-46+00
2025-03-21 10:25:06,264 - speechbrain.utils.epoch_loop - INFO - Going into epoch 59
2025-03-21 10:25:51,528 - speechbrain.utils.train_logger - INFO - epoch: 59, lr: 4.96e-04, steps: 6195, optimizer: Adam - train loss: 6.67 - valid loss: 10.14, valid ACC: 8.28e-01
2025-03-21 10:25:51,858 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-25-51+00
2025-03-21 10:25:52,907 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-16-35+00
2025-03-21 10:25:52,907 - speechbrain.utils.epoch_loop - INFO - Going into epoch 60
2025-03-21 10:27:50,525 - speechbrain.utils.train_logger - INFO - epoch: 60, lr: 5.04e-04, steps: 6300, optimizer: Adam - train loss: 6.45 - valid loss: 9.46, valid ACC: 8.32e-01, valid WER: 61.83
2025-03-21 10:27:50,761 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-27-50+00
2025-03-21 10:27:51,750 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-18-33+00
2025-03-21 10:27:51,750 - speechbrain.utils.epoch_loop - INFO - Going into epoch 61
2025-03-21 10:28:38,217 - speechbrain.utils.train_logger - INFO - epoch: 61, lr: 5.12e-04, steps: 6405, optimizer: Adam - train loss: 6.20 - valid loss: 9.96, valid ACC: 8.31e-01
2025-03-21 10:28:38,601 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-28-38+00
2025-03-21 10:28:39,589 - speechbrain.utils.epoch_loop - INFO - Going into epoch 62
2025-03-21 10:29:26,132 - speechbrain.utils.train_logger - INFO - epoch: 62, lr: 5.21e-04, steps: 6510, optimizer: Adam - train loss: 5.96 - valid loss: 9.17, valid ACC: 8.39e-01
2025-03-21 10:29:26,446 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-29-26+00
2025-03-21 10:29:27,538 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-19-20+00
2025-03-21 10:29:27,539 - speechbrain.utils.epoch_loop - INFO - Going into epoch 63
2025-03-21 10:30:12,714 - speechbrain.utils.train_logger - INFO - epoch: 63, lr: 5.29e-04, steps: 6615, optimizer: Adam - train loss: 5.75 - valid loss: 9.08, valid ACC: 8.39e-01
2025-03-21 10:30:12,994 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-30-12+00
2025-03-21 10:30:14,121 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-21-04+00
2025-03-21 10:30:14,122 - speechbrain.utils.epoch_loop - INFO - Going into epoch 64
2025-03-21 10:31:00,672 - speechbrain.utils.train_logger - INFO - epoch: 64, lr: 5.38e-04, steps: 6720, optimizer: Adam - train loss: 5.57 - valid loss: 9.58, valid ACC: 8.31e-01
2025-03-21 10:31:00,990 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-31-00+00
2025-03-21 10:31:02,119 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-20-09+00
2025-03-21 10:31:02,120 - speechbrain.utils.epoch_loop - INFO - Going into epoch 65
2025-03-21 10:31:49,005 - speechbrain.utils.train_logger - INFO - epoch: 65, lr: 5.46e-04, steps: 6825, optimizer: Adam - train loss: 5.37 - valid loss: 8.95, valid ACC: 8.50e-01
2025-03-21 10:31:49,249 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-31-49+00
2025-03-21 10:31:50,396 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-21-51+00
2025-03-21 10:31:50,396 - speechbrain.utils.epoch_loop - INFO - Going into epoch 66
2025-03-21 10:32:37,148 - speechbrain.utils.train_logger - INFO - epoch: 66, lr: 5.54e-04, steps: 6930, optimizer: Adam - train loss: 5.19 - valid loss: 9.32, valid ACC: 8.40e-01
2025-03-21 10:32:37,436 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-32-37+00
2025-03-21 10:32:38,594 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-23-28+00
2025-03-21 10:32:38,595 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-22-39+00
2025-03-21 10:32:38,596 - speechbrain.utils.epoch_loop - INFO - Going into epoch 67
2025-03-21 10:33:24,091 - speechbrain.utils.train_logger - INFO - epoch: 67, lr: 5.63e-04, steps: 7035, optimizer: Adam - train loss: 5.06 - valid loss: 8.48, valid ACC: 8.54e-01
2025-03-21 10:33:24,463 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-33-24+00
2025-03-21 10:33:25,612 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-24-16+00
2025-03-21 10:33:25,612 - speechbrain.utils.epoch_loop - INFO - Going into epoch 68
2025-03-21 10:34:12,677 - speechbrain.utils.train_logger - INFO - epoch: 68, lr: 5.71e-04, steps: 7140, optimizer: Adam - train loss: 4.92 - valid loss: 8.34, valid ACC: 8.54e-01
2025-03-21 10:34:12,965 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-34-12+00
2025-03-21 10:34:14,128 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-25-04+00
2025-03-21 10:34:14,128 - speechbrain.utils.epoch_loop - INFO - Going into epoch 69
2025-03-21 10:35:01,018 - speechbrain.utils.train_logger - INFO - epoch: 69, lr: 5.80e-04, steps: 7245, optimizer: Adam - train loss: 4.80 - valid loss: 8.69, valid ACC: 8.46e-01
2025-03-21 10:35:01,328 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-35-01+00
2025-03-21 10:35:02,560 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-25-51+00
2025-03-21 10:35:02,560 - speechbrain.utils.epoch_loop - INFO - Going into epoch 70
2025-03-21 10:37:01,787 - speechbrain.utils.train_logger - INFO - epoch: 70, lr: 5.88e-04, steps: 7350, optimizer: Adam - train loss: 4.68 - valid loss: 8.87, valid ACC: 8.47e-01, valid WER: 53.40
2025-03-21 10:37:02,758 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-37-01+00
2025-03-21 10:37:05,979 - speechbrain.utils.epoch_loop - INFO - Going into epoch 71
2025-03-21 10:37:52,745 - speechbrain.utils.train_logger - INFO - epoch: 71, lr: 5.96e-04, steps: 7455, optimizer: Adam - train loss: 4.57 - valid loss: 8.16, valid ACC: 8.54e-01
2025-03-21 10:37:53,108 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-37-52+00
2025-03-21 10:37:54,362 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-28-38+00
2025-03-21 10:37:54,362 - speechbrain.utils.epoch_loop - INFO - Going into epoch 72
2025-03-21 10:38:40,695 - speechbrain.utils.train_logger - INFO - epoch: 72, lr: 6.05e-04, steps: 7560, optimizer: Adam - train loss: 4.39 - valid loss: 8.48, valid ACC: 8.56e-01
2025-03-21 10:38:41,051 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-38-40+00
2025-03-21 10:38:42,394 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-27-50+00
2025-03-21 10:38:42,394 - speechbrain.utils.epoch_loop - INFO - Going into epoch 73
2025-03-21 10:39:29,051 - speechbrain.utils.train_logger - INFO - epoch: 73, lr: 6.13e-04, steps: 7665, optimizer: Adam - train loss: 4.41 - valid loss: 8.62, valid ACC: 8.49e-01
2025-03-21 10:39:29,427 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-39-29+00
2025-03-21 10:39:30,759 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-29-26+00
2025-03-21 10:39:30,759 - speechbrain.utils.epoch_loop - INFO - Going into epoch 74
2025-03-21 10:40:15,203 - speechbrain.utils.train_logger - INFO - epoch: 74, lr: 6.22e-04, steps: 7770, optimizer: Adam - train loss: 4.16 - valid loss: 8.32, valid ACC: 8.50e-01
2025-03-21 10:40:15,607 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-40-15+00
2025-03-21 10:40:16,918 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-30-12+00
2025-03-21 10:40:16,919 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-31-00+00
2025-03-21 10:40:16,920 - speechbrain.utils.epoch_loop - INFO - Going into epoch 75
2025-03-21 10:41:04,420 - speechbrain.utils.train_logger - INFO - epoch: 75, lr: 6.30e-04, steps: 7875, optimizer: Adam - train loss: 4.14 - valid loss: 8.56, valid ACC: 8.60e-01
2025-03-21 10:41:04,743 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-41-04+00
2025-03-21 10:41:06,032 - speechbrain.utils.epoch_loop - INFO - Going into epoch 76
2025-03-21 10:41:51,875 - speechbrain.utils.train_logger - INFO - epoch: 76, lr: 6.38e-04, steps: 7980, optimizer: Adam - train loss: 4.02 - valid loss: 7.90, valid ACC: 8.57e-01
2025-03-21 10:41:52,290 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-41-51+00
2025-03-21 10:41:53,634 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-32-37+00
2025-03-21 10:41:53,634 - speechbrain.utils.epoch_loop - INFO - Going into epoch 77
2025-03-21 10:42:39,541 - speechbrain.utils.train_logger - INFO - epoch: 77, lr: 6.47e-04, steps: 8085, optimizer: Adam - train loss: 4.02 - valid loss: 7.63, valid ACC: 8.62e-01
2025-03-21 10:42:39,946 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-42-39+00
2025-03-21 10:42:41,298 - speechbrain.utils.epoch_loop - INFO - Going into epoch 78
2025-03-21 10:43:27,740 - speechbrain.utils.train_logger - INFO - epoch: 78, lr: 6.55e-04, steps: 8190, optimizer: Adam - train loss: 3.83 - valid loss: 7.60, valid ACC: 8.61e-01
2025-03-21 10:43:27,997 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-43-27+00
2025-03-21 10:43:29,513 - speechbrain.utils.epoch_loop - INFO - Going into epoch 79
2025-03-21 10:44:35,704 - speechbrain.utils.train_logger - INFO - epoch: 79, lr: 6.64e-04, steps: 8295, optimizer: Adam - train loss: 3.80 - valid loss: 7.98, valid ACC: 8.47e-01
2025-03-21 10:44:36,006 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-44-35+00
2025-03-21 10:44:37,632 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-35-01+00
2025-03-21 10:44:37,632 - speechbrain.utils.epoch_loop - INFO - Going into epoch 80
2025-03-21 10:46:37,524 - speechbrain.utils.train_logger - INFO - epoch: 80, lr: 6.72e-04, steps: 8400, optimizer: Adam - train loss: 3.74 - valid loss: 7.78, valid ACC: 8.62e-01, valid WER: 48.04
2025-03-21 10:46:37,771 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-46-37+00
2025-03-21 10:46:39,382 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-37-01+00
2025-03-21 10:46:39,384 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-31-49+00
2025-03-21 10:46:39,394 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-46-37+00
2025-03-21 10:46:39,645 - speechbrain.dataio.dataloader - DEBUG - SaveableDataLoader was requested to load a checkpoint, but the DataLoader has already been iterated. The DataLoader file will be ignored. This is normal in evaluation, when a checkpoint is loaded just to retrieve the best model.
2025-03-21 10:53:17,683 - speechbrain.utils.train_logger - INFO - Epoch loaded: 80 - test loss: 6.79, test ACC: 8.77e-01, test WER: 49.07
2025-03-21 10:53:18,304 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-53-17+00
2025-03-21 10:53:19,969 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-33-24+00
2025-03-21 10:53:19,970 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-42-39+00
2025-03-21 10:53:19,972 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-34-12+00
2025-03-21 10:53:19,973 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-38-40+00
2025-03-21 10:53:19,974 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-40-15+00
2025-03-21 10:53:19,976 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-44-35+00
2025-03-21 10:53:19,977 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-43-27+00
2025-03-21 10:53:19,978 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-39-29+00
2025-03-21 10:53:19,980 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-41-04+00
2025-03-21 10:53:19,981 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-46-37+00
2025-03-21 10:53:19,982 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-41-51+00
2025-03-21 10:53:19,983 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_030/results_fold_1/conformer_small/7775/save/CKPT+2025-03-21+10-37-52+00
