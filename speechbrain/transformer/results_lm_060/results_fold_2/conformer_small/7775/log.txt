2025-03-21 17:13:07,810 - speechbrain.utils.quirks - INFO - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
2025-03-21 17:13:07,819 - speechbrain.utils.quirks - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
2025-03-21 17:13:07,819 - speechbrain.core - INFO - Beginning experiment!
2025-03-21 17:13:07,819 - speechbrain.core - INFO - Experiment folder: results_lm_060/results_fold_2/conformer_small/7775
2025-03-21 17:13:18,350 - speechbrain.utils.superpowers - DEBUG - aiohappyeyeballs==2.6.1
aiohttp==3.11.13
aiosignal==1.3.2
alembic==1.15.1
anyio==4.9.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==3.0.0
async-lru==2.0.5
async-timeout==5.0.1
attrs==25.3.0
babel==2.17.0
banal==1.0.6
beautifulsoup4==4.13.3
black==24.3.0
bleach==6.2.0
certifi==2025.1.31
cffi==1.17.1
cfgv==3.4.0
charset-normalizer==3.4.1
click==8.1.7
comm==0.2.2
contourpy==1.3.0
cycler==0.12.1
dataset==1.6.2
datasets==3.3.2
debugpy==1.8.13
decorator==5.2.1
defusedxml==0.7.1
dill==0.3.8
distlib==0.3.9
docstring_parser_fork==0.0.12
exceptiongroup==1.2.2
executing==2.2.0
fastjsonschema==2.21.1
filelock==3.17.0
flake8==7.0.0
fonttools==4.56.0
fqdn==1.5.1
frozenlist==1.5.0
fsspec==2024.12.0
greenlet==3.1.1
h11==0.14.0
httpcore==1.0.7
httpx==0.28.1
huggingface-hub==0.29.3
HyperPyYAML==1.2.2
identify==2.6.9
idna==3.10
importlib_metadata==8.6.1
importlib_resources==6.5.2
iniconfig==2.0.0
ipykernel==6.29.5
ipython==8.18.1
isoduration==20.11.0
isort==5.13.2
jedi==0.19.2
Jinja2==3.1.6
joblib==1.4.2
json5==0.10.0
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
jupyter-events==0.12.0
jupyter-lsp==2.2.5
jupyter_client==8.6.3
jupyter_core==5.7.2
jupyter_server==2.15.0
jupyter_server_terminals==0.5.3
jupyterlab==4.3.6
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
kiwisolver==1.4.7
Mako==1.3.9
MarkupSafe==3.0.2
matplotlib==3.9.4
matplotlib-inline==0.1.7
mccabe==0.7.0
mistune==3.1.2
mpmath==1.3.0
multidict==6.1.0
multiprocess==0.70.16
mypy-extensions==1.0.0
nbclient==0.10.2
nbconvert==7.16.6
nbformat==5.10.4
nest-asyncio==1.6.0
networkx==3.2.1
nodeenv==1.9.1
notebook_shim==0.2.4
numpy==1.26.4
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-cusparselt-cu12==0.6.2
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.4.127
overrides==7.7.0
packaging==24.2
pandas==2.2.3
pandocfilters==1.5.1
parso==0.8.4
pathspec==0.12.1
pexpect==4.9.0
pillow==11.1.0
platformdirs==4.3.6
pluggy==1.5.0
pre_commit==4.1.0
prometheus_client==0.21.1
prompt_toolkit==3.0.50
propcache==0.3.0
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==19.0.1
pycodestyle==2.11.0
pycparser==2.22
pydoclint==0.4.1
pyflakes==3.2.0
Pygments==2.19.1
pygtrie==2.5.0
pyparsing==3.2.1
pytest==7.4.0
python-dateutil==2.9.0.post0
python-json-logger==3.3.0
pytz==2025.1
PyYAML==6.0.2
pyzmq==26.3.0
referencing==0.36.2
regex==2024.11.6
requests==2.32.3
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.23.1
ruamel.yaml==0.18.10
ruamel.yaml.clib==0.2.12
safetensors==0.5.3
scipy==1.12.0
Send2Trash==1.8.3
sentencepiece==0.2.0
six==1.17.0
sniffio==1.3.1
soupsieve==2.6
speechbrain==1.0.2
SQLAlchemy==1.4.54
stack-data==0.6.3
sympy==1.13.1
terminado==0.18.1
tinycss2==1.4.0
tokenizers==0.21.0
tomli==2.2.1
torch==2.6.0
torchaudio==2.6.0
torchvision==0.21.0
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
transformers==4.49.0
triton==3.2.0
types-python-dateutil==2.9.0.20241206
typing_extensions==4.12.2
tzdata==2025.1
uri-template==1.3.0
urllib3==2.3.0
virtualenv==20.29.3
wcwidth==0.2.13
webcolors==24.11.1
webencodings==0.5.1
websocket-client==1.8.0
xxhash==3.5.0
yamllint==1.35.1
yarl==1.18.3
zipp==3.21.0


2025-03-21 17:13:19,026 - speechbrain.utils.superpowers - DEBUG - d8ae0d1


2025-03-21 17:13:19,384 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results_lm_060/results_fold_2/conformer_small/7775/save.
2025-03-21 17:13:19,385 - speechbrain.utils.fetching - DEBUG - Fetch: Local file found, creating symlink '/scratch/asasin/IST-ASR/speechbrain/transformer/LM_weights_13_epochs/lm.ckpt' -> '/scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt'
2025-03-21 17:13:19,412 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["lm"] = /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt
2025-03-21 17:13:19,413 - speechbrain.utils.fetching - DEBUG - Fetch: Local file found, creating symlink '/scratch/asasin/IST-ASR/speechbrain/transformer/LM_weights_13_epochs/tokenizer.ckpt' -> '/scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/tokenizer.ckpt'
2025-03-21 17:13:19,426 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["tokenizer"] = /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/tokenizer.ckpt
2025-03-21 17:13:19,427 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2025-03-21 17:13:19,427 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): lm -> /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt
2025-03-21 17:13:19,427 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): tokenizer -> /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/tokenizer.ckpt
2025-03-21 17:13:19,876 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: positional_encoding.pe
2025-03-21 17:13:19,877 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_weight
2025-03-21 17:13:19,877 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_bias
2025-03-21 17:13:19,878 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.weight
2025-03-21 17:13:19,878 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.bias
2025-03-21 17:13:19,879 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.weight
2025-03-21 17:13:19,879 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.bias
2025-03-21 17:13:19,880 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.weight
2025-03-21 17:13:19,881 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.bias
2025-03-21 17:13:19,881 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.weight
2025-03-21 17:13:19,882 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.bias
2025-03-21 17:13:19,882 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.weight
2025-03-21 17:13:19,883 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.bias
2025-03-21 17:13:19,883 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_weight
2025-03-21 17:13:19,884 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_bias
2025-03-21 17:13:19,884 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.weight
2025-03-21 17:13:19,885 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.bias
2025-03-21 17:13:19,885 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.weight
2025-03-21 17:13:19,886 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.bias
2025-03-21 17:13:19,887 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.weight
2025-03-21 17:13:19,887 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.bias
2025-03-21 17:13:19,888 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.weight
2025-03-21 17:13:19,888 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.bias
2025-03-21 17:13:19,889 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.weight
2025-03-21 17:13:19,889 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.bias
2025-03-21 17:13:19,890 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_weight
2025-03-21 17:13:19,890 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_bias
2025-03-21 17:13:19,891 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.weight
2025-03-21 17:13:19,891 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.bias
2025-03-21 17:13:19,892 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.weight
2025-03-21 17:13:19,893 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.bias
2025-03-21 17:13:19,893 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.weight
2025-03-21 17:13:19,894 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.bias
2025-03-21 17:13:19,894 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.weight
2025-03-21 17:13:19,895 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.bias
2025-03-21 17:13:19,895 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.weight
2025-03-21 17:13:19,896 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.bias
2025-03-21 17:13:19,896 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_weight
2025-03-21 17:13:19,897 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_bias
2025-03-21 17:13:19,897 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.weight
2025-03-21 17:13:19,898 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.bias
2025-03-21 17:13:19,898 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.weight
2025-03-21 17:13:19,899 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.bias
2025-03-21 17:13:19,899 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.weight
2025-03-21 17:13:19,900 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.bias
2025-03-21 17:13:19,901 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.weight
2025-03-21 17:13:19,901 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.bias
2025-03-21 17:13:19,902 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.weight
2025-03-21 17:13:19,902 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.bias
2025-03-21 17:13:19,903 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_weight
2025-03-21 17:13:19,903 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_bias
2025-03-21 17:13:19,904 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.weight
2025-03-21 17:13:19,904 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.bias
2025-03-21 17:13:19,905 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.weight
2025-03-21 17:13:19,905 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.bias
2025-03-21 17:13:19,906 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.weight
2025-03-21 17:13:19,906 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.bias
2025-03-21 17:13:19,907 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.weight
2025-03-21 17:13:19,908 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.bias
2025-03-21 17:13:19,908 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.weight
2025-03-21 17:13:19,909 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.bias
2025-03-21 17:13:19,909 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_weight
2025-03-21 17:13:19,910 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_bias
2025-03-21 17:13:19,910 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.weight
2025-03-21 17:13:19,911 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.bias
2025-03-21 17:13:19,911 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.weight
2025-03-21 17:13:19,912 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.bias
2025-03-21 17:13:19,912 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.weight
2025-03-21 17:13:19,913 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.bias
2025-03-21 17:13:19,913 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.weight
2025-03-21 17:13:19,914 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.bias
2025-03-21 17:13:19,914 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.weight
2025-03-21 17:13:19,915 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.bias
2025-03-21 17:13:19,916 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_weight
2025-03-21 17:13:19,916 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_bias
2025-03-21 17:13:19,917 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.weight
2025-03-21 17:13:19,917 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.bias
2025-03-21 17:13:19,918 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.weight
2025-03-21 17:13:19,918 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.bias
2025-03-21 17:13:19,919 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.weight
2025-03-21 17:13:19,919 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.bias
2025-03-21 17:13:19,920 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.weight
2025-03-21 17:13:19,921 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.bias
2025-03-21 17:13:19,921 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.weight
2025-03-21 17:13:19,922 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.bias
2025-03-21 17:13:19,922 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_weight
2025-03-21 17:13:19,923 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_bias
2025-03-21 17:13:19,923 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.weight
2025-03-21 17:13:19,924 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.bias
2025-03-21 17:13:19,924 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.weight
2025-03-21 17:13:19,925 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.bias
2025-03-21 17:13:19,925 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.weight
2025-03-21 17:13:19,926 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.bias
2025-03-21 17:13:19,926 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.weight
2025-03-21 17:13:19,927 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.bias
2025-03-21 17:13:19,928 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.weight
2025-03-21 17:13:19,928 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.bias
2025-03-21 17:13:19,929 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_weight
2025-03-21 17:13:19,929 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_bias
2025-03-21 17:13:19,930 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.weight
2025-03-21 17:13:19,930 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.bias
2025-03-21 17:13:19,931 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.weight
2025-03-21 17:13:19,931 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.bias
2025-03-21 17:13:19,932 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.weight
2025-03-21 17:13:19,933 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.bias
2025-03-21 17:13:19,933 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.weight
2025-03-21 17:13:19,934 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.bias
2025-03-21 17:13:19,934 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.weight
2025-03-21 17:13:19,935 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.bias
2025-03-21 17:13:19,935 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_weight
2025-03-21 17:13:19,936 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_bias
2025-03-21 17:13:19,936 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.weight
2025-03-21 17:13:19,937 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.bias
2025-03-21 17:13:19,937 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.weight
2025-03-21 17:13:19,938 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.bias
2025-03-21 17:13:19,938 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.weight
2025-03-21 17:13:19,939 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.bias
2025-03-21 17:13:19,939 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.weight
2025-03-21 17:13:19,940 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.bias
2025-03-21 17:13:19,941 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.weight
2025-03-21 17:13:19,941 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.bias
2025-03-21 17:13:19,942 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_weight
2025-03-21 17:13:19,942 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_bias
2025-03-21 17:13:19,943 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.weight
2025-03-21 17:13:19,943 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.bias
2025-03-21 17:13:19,944 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.weight
2025-03-21 17:13:19,944 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.bias
2025-03-21 17:13:19,945 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.weight
2025-03-21 17:13:19,945 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.bias
2025-03-21 17:13:19,946 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.weight
2025-03-21 17:13:19,946 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.bias
2025-03-21 17:13:19,947 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.weight
2025-03-21 17:13:19,947 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.bias
2025-03-21 17:13:19,948 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_weight
2025-03-21 17:13:19,949 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_bias
2025-03-21 17:13:19,949 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.weight
2025-03-21 17:13:19,950 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.bias
2025-03-21 17:13:19,950 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.weight
2025-03-21 17:13:19,951 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.bias
2025-03-21 17:13:19,951 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.weight
2025-03-21 17:13:19,952 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.bias
2025-03-21 17:13:19,952 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.weight
2025-03-21 17:13:19,953 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.bias
2025-03-21 17:13:19,953 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.weight
2025-03-21 17:13:19,954 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.bias
2025-03-21 17:13:19,955 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.weight
2025-03-21 17:13:19,955 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.bias
2025-03-21 17:13:19,956 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: custom_src_module.emb.Embedding.weight
2025-03-21 17:13:19,956 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.weight
2025-03-21 17:13:19,957 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.bias
2025-03-21 17:13:19,957 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.weight
2025-03-21 17:13:19,958 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.bias
2025-03-21 17:13:19,958 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.weight
2025-03-21 17:13:19,959 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.bias
2025-03-21 17:13:19,960 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: embedding.Embedding.weight
2025-03-21 17:13:19,960 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l0
2025-03-21 17:13:19,961 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l0
2025-03-21 17:13:19,961 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l0
2025-03-21 17:13:19,962 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l0
2025-03-21 17:13:19,963 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l1
2025-03-21 17:13:19,963 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l1
2025-03-21 17:13:19,964 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l1
2025-03-21 17:13:19,964 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l1
2025-03-21 17:13:19,965 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.weight
2025-03-21 17:13:19,965 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.bias
2025-03-21 17:13:19,966 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.weight
2025-03-21 17:13:19,967 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.bias
2025-03-21 17:13:19,967 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.weight
2025-03-21 17:13:19,968 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.bias
2025-03-21 17:13:20,014 - speechbrain.core - INFO - Info: precision arg from hparam file is used
2025-03-21 17:13:20,014 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2025-03-21 17:13:20,015 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2025-03-21 17:13:20,243 - speechbrain.core - INFO - Gradscaler enabled: True. Using precision: fp16.
2025-03-21 17:13:20,245 - speechbrain.core - INFO - ASR Model Statistics:
* Total Number of Trainable Parameters: 13.3M
* Total Number of Parameters: 13.3M
* Trainable Parameters represent 100.0000% of the total size.
2025-03-21 17:13:20,248 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2025-03-21 17:13:20,248 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2025-03-21 17:14:38,216 - speechbrain.utils.train_logger - INFO - epoch: 1, lr: 8.08e-06, steps: 102, optimizer: Adam - train loss: 2.61e+02 - valid loss: 1.48e+02, valid ACC: 1.86e-01
2025-03-21 17:14:38,684 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-14-38+00
2025-03-21 17:14:38,692 - speechbrain.utils.epoch_loop - INFO - Going into epoch 2
2025-03-21 17:14:53,074 - speechbrain.core - ERROR - Exception:
Traceback (most recent call last):
  File "/scratch/asasin/IST-ASR/speechbrain/transformer/train.py", line 468, in <module>
    asr_brain.fit(
  File "/home/asasin/.conda/envs/IST-ASR/lib/python3.9/site-packages/speechbrain/core.py", line 1619, in fit
    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
  File "/home/asasin/.conda/envs/IST-ASR/lib/python3.9/site-packages/speechbrain/core.py", line 1444, in _fit_train
    loss = self.fit_batch(batch)
  File "/home/asasin/.conda/envs/IST-ASR/lib/python3.9/site-packages/speechbrain/core.py", line 1231, in fit_batch
    loss = self.compute_objectives(
  File "/scratch/asasin/IST-ASR/speechbrain/transformer/train.py", line 151, in compute_objectives
    loss_ctc = self.hparams.ctc_cost(
  File "/home/asasin/.conda/envs/IST-ASR/lib/python3.9/site-packages/speechbrain/nnet/losses.py", line 280, in ctc_loss
    loss = torch.nn.functional.ctc_loss(
  File "/home/asasin/.conda/envs/IST-ASR/lib/python3.9/site-packages/torch/nn/functional.py", line 3079, in ctc_loss
    return torch.ctc_loss(
KeyboardInterrupt
2025-03-21 17:18:30,500 - speechbrain.utils.quirks - INFO - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
2025-03-21 17:18:30,500 - speechbrain.utils.quirks - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
2025-03-21 17:18:30,500 - speechbrain.core - INFO - Beginning experiment!
2025-03-21 17:18:30,501 - speechbrain.core - INFO - Experiment folder: results_lm_060/results_fold_2/conformer_small/7775
2025-03-21 17:18:31,667 - speechbrain.utils.superpowers - DEBUG - aiohappyeyeballs==2.6.1
aiohttp==3.11.13
aiosignal==1.3.2
alembic==1.15.1
anyio==4.9.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==3.0.0
async-lru==2.0.5
async-timeout==5.0.1
attrs==25.3.0
babel==2.17.0
banal==1.0.6
beautifulsoup4==4.13.3
black==24.3.0
bleach==6.2.0
certifi==2025.1.31
cffi==1.17.1
cfgv==3.4.0
charset-normalizer==3.4.1
click==8.1.7
comm==0.2.2
contourpy==1.3.0
cycler==0.12.1
dataset==1.6.2
datasets==3.3.2
debugpy==1.8.13
decorator==5.2.1
defusedxml==0.7.1
dill==0.3.8
distlib==0.3.9
docstring_parser_fork==0.0.12
exceptiongroup==1.2.2
executing==2.2.0
fastjsonschema==2.21.1
filelock==3.17.0
flake8==7.0.0
fonttools==4.56.0
fqdn==1.5.1
frozenlist==1.5.0
fsspec==2024.12.0
greenlet==3.1.1
h11==0.14.0
httpcore==1.0.7
httpx==0.28.1
huggingface-hub==0.29.3
HyperPyYAML==1.2.2
identify==2.6.9
idna==3.10
importlib_metadata==8.6.1
importlib_resources==6.5.2
iniconfig==2.0.0
ipykernel==6.29.5
ipython==8.18.1
isoduration==20.11.0
isort==5.13.2
jedi==0.19.2
Jinja2==3.1.6
joblib==1.4.2
json5==0.10.0
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
jupyter-events==0.12.0
jupyter-lsp==2.2.5
jupyter_client==8.6.3
jupyter_core==5.7.2
jupyter_server==2.15.0
jupyter_server_terminals==0.5.3
jupyterlab==4.3.6
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
kiwisolver==1.4.7
Mako==1.3.9
MarkupSafe==3.0.2
matplotlib==3.9.4
matplotlib-inline==0.1.7
mccabe==0.7.0
mistune==3.1.2
mpmath==1.3.0
multidict==6.1.0
multiprocess==0.70.16
mypy-extensions==1.0.0
nbclient==0.10.2
nbconvert==7.16.6
nbformat==5.10.4
nest-asyncio==1.6.0
networkx==3.2.1
nodeenv==1.9.1
notebook_shim==0.2.4
numpy==1.26.4
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-cusparselt-cu12==0.6.2
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.4.127
overrides==7.7.0
packaging==24.2
pandas==2.2.3
pandocfilters==1.5.1
parso==0.8.4
pathspec==0.12.1
pexpect==4.9.0
pillow==11.1.0
platformdirs==4.3.6
pluggy==1.5.0
pre_commit==4.1.0
prometheus_client==0.21.1
prompt_toolkit==3.0.50
propcache==0.3.0
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==19.0.1
pycodestyle==2.11.0
pycparser==2.22
pydoclint==0.4.1
pyflakes==3.2.0
Pygments==2.19.1
pygtrie==2.5.0
pyparsing==3.2.1
pytest==7.4.0
python-dateutil==2.9.0.post0
python-json-logger==3.3.0
pytz==2025.1
PyYAML==6.0.2
pyzmq==26.3.0
referencing==0.36.2
regex==2024.11.6
requests==2.32.3
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.23.1
ruamel.yaml==0.18.10
ruamel.yaml.clib==0.2.12
safetensors==0.5.3
scipy==1.12.0
Send2Trash==1.8.3
sentencepiece==0.2.0
six==1.17.0
sniffio==1.3.1
soupsieve==2.6
speechbrain==1.0.2
SQLAlchemy==1.4.54
stack-data==0.6.3
sympy==1.13.1
terminado==0.18.1
tinycss2==1.4.0
tokenizers==0.21.0
tomli==2.2.1
torch==2.6.0
torchaudio==2.6.0
torchvision==0.21.0
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
transformers==4.49.0
triton==3.2.0
types-python-dateutil==2.9.0.20241206
typing_extensions==4.12.2
tzdata==2025.1
uri-template==1.3.0
urllib3==2.3.0
virtualenv==20.29.3
wcwidth==0.2.13
webcolors==24.11.1
webencodings==0.5.1
websocket-client==1.8.0
xxhash==3.5.0
yamllint==1.35.1
yarl==1.18.3
zipp==3.21.0


2025-03-21 17:18:31,700 - speechbrain.utils.superpowers - DEBUG - d8ae0d1


2025-03-21 17:18:31,786 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results_lm_060/results_fold_2/conformer_small/7775/save.
2025-03-21 17:18:31,787 - speechbrain.utils.fetching - INFO - Fetch lm.ckpt: Using symlink found at '/scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt'
2025-03-21 17:18:31,787 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["lm"] = /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt
2025-03-21 17:18:31,788 - speechbrain.utils.fetching - INFO - Fetch tokenizer.ckpt: Using symlink found at '/scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/tokenizer.ckpt'
2025-03-21 17:18:31,789 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["tokenizer"] = /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/tokenizer.ckpt
2025-03-21 17:18:31,789 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2025-03-21 17:18:31,789 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): lm -> /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt
2025-03-21 17:18:31,789 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): tokenizer -> /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/tokenizer.ckpt
2025-03-21 17:18:31,849 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: positional_encoding.pe
2025-03-21 17:18:31,850 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_weight
2025-03-21 17:18:31,850 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_bias
2025-03-21 17:18:31,851 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.weight
2025-03-21 17:18:31,851 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.bias
2025-03-21 17:18:31,852 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.weight
2025-03-21 17:18:31,853 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.bias
2025-03-21 17:18:31,853 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.weight
2025-03-21 17:18:31,854 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.bias
2025-03-21 17:18:31,854 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.weight
2025-03-21 17:18:31,855 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.bias
2025-03-21 17:18:31,855 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.weight
2025-03-21 17:18:31,856 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.bias
2025-03-21 17:18:31,857 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_weight
2025-03-21 17:18:31,857 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_bias
2025-03-21 17:18:31,858 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.weight
2025-03-21 17:18:31,859 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.bias
2025-03-21 17:18:31,859 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.weight
2025-03-21 17:18:31,860 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.bias
2025-03-21 17:18:31,860 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.weight
2025-03-21 17:18:31,861 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.bias
2025-03-21 17:18:31,862 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.weight
2025-03-21 17:18:31,862 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.bias
2025-03-21 17:18:31,863 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.weight
2025-03-21 17:18:31,863 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.bias
2025-03-21 17:18:31,864 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_weight
2025-03-21 17:18:31,864 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_bias
2025-03-21 17:18:31,865 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.weight
2025-03-21 17:18:31,865 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.bias
2025-03-21 17:18:31,867 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.weight
2025-03-21 17:18:31,867 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.bias
2025-03-21 17:18:31,868 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.weight
2025-03-21 17:18:31,868 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.bias
2025-03-21 17:18:31,869 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.weight
2025-03-21 17:18:31,870 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.bias
2025-03-21 17:18:31,870 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.weight
2025-03-21 17:18:31,871 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.bias
2025-03-21 17:18:31,871 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_weight
2025-03-21 17:18:31,872 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_bias
2025-03-21 17:18:31,872 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.weight
2025-03-21 17:18:31,873 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.bias
2025-03-21 17:18:31,873 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.weight
2025-03-21 17:18:31,874 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.bias
2025-03-21 17:18:31,875 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.weight
2025-03-21 17:18:31,875 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.bias
2025-03-21 17:18:31,876 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.weight
2025-03-21 17:18:31,876 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.bias
2025-03-21 17:18:31,877 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.weight
2025-03-21 17:18:31,877 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.bias
2025-03-21 17:18:31,878 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_weight
2025-03-21 17:18:31,878 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_bias
2025-03-21 17:18:31,879 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.weight
2025-03-21 17:18:31,879 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.bias
2025-03-21 17:18:31,880 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.weight
2025-03-21 17:18:31,880 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.bias
2025-03-21 17:18:31,881 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.weight
2025-03-21 17:18:31,881 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.bias
2025-03-21 17:18:31,882 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.weight
2025-03-21 17:18:31,882 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.bias
2025-03-21 17:18:31,883 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.weight
2025-03-21 17:18:31,883 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.bias
2025-03-21 17:18:31,884 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_weight
2025-03-21 17:18:31,884 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_bias
2025-03-21 17:18:31,885 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.weight
2025-03-21 17:18:31,885 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.bias
2025-03-21 17:18:31,886 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.weight
2025-03-21 17:18:31,886 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.bias
2025-03-21 17:18:31,887 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.weight
2025-03-21 17:18:31,888 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.bias
2025-03-21 17:18:31,888 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.weight
2025-03-21 17:18:31,889 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.bias
2025-03-21 17:18:31,889 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.weight
2025-03-21 17:18:31,890 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.bias
2025-03-21 17:18:31,890 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_weight
2025-03-21 17:18:31,891 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_bias
2025-03-21 17:18:31,891 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.weight
2025-03-21 17:18:31,892 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.bias
2025-03-21 17:18:31,892 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.weight
2025-03-21 17:18:31,893 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.bias
2025-03-21 17:18:31,893 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.weight
2025-03-21 17:18:31,894 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.bias
2025-03-21 17:18:31,895 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.weight
2025-03-21 17:18:31,895 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.bias
2025-03-21 17:18:31,896 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.weight
2025-03-21 17:18:31,896 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.bias
2025-03-21 17:18:31,897 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_weight
2025-03-21 17:18:31,897 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_bias
2025-03-21 17:18:31,898 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.weight
2025-03-21 17:18:31,898 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.bias
2025-03-21 17:18:31,899 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.weight
2025-03-21 17:18:31,900 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.bias
2025-03-21 17:18:31,900 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.weight
2025-03-21 17:18:31,901 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.bias
2025-03-21 17:18:31,901 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.weight
2025-03-21 17:18:31,902 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.bias
2025-03-21 17:18:31,902 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.weight
2025-03-21 17:18:31,903 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.bias
2025-03-21 17:18:31,903 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_weight
2025-03-21 17:18:31,904 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_bias
2025-03-21 17:18:31,904 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.weight
2025-03-21 17:18:31,905 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.bias
2025-03-21 17:18:31,905 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.weight
2025-03-21 17:18:31,906 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.bias
2025-03-21 17:18:31,906 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.weight
2025-03-21 17:18:31,907 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.bias
2025-03-21 17:18:31,908 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.weight
2025-03-21 17:18:31,908 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.bias
2025-03-21 17:18:31,909 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.weight
2025-03-21 17:18:31,909 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.bias
2025-03-21 17:18:31,910 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_weight
2025-03-21 17:18:31,910 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_bias
2025-03-21 17:18:31,911 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.weight
2025-03-21 17:18:31,911 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.bias
2025-03-21 17:18:31,912 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.weight
2025-03-21 17:18:31,912 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.bias
2025-03-21 17:18:31,913 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.weight
2025-03-21 17:18:31,913 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.bias
2025-03-21 17:18:31,914 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.weight
2025-03-21 17:18:31,915 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.bias
2025-03-21 17:18:31,915 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.weight
2025-03-21 17:18:31,916 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.bias
2025-03-21 17:18:31,916 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_weight
2025-03-21 17:18:31,917 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_bias
2025-03-21 17:18:31,917 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.weight
2025-03-21 17:18:31,918 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.bias
2025-03-21 17:18:31,918 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.weight
2025-03-21 17:18:31,919 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.bias
2025-03-21 17:18:31,919 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.weight
2025-03-21 17:18:31,920 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.bias
2025-03-21 17:18:31,921 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.weight
2025-03-21 17:18:31,921 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.bias
2025-03-21 17:18:31,922 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.weight
2025-03-21 17:18:31,922 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.bias
2025-03-21 17:18:31,923 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_weight
2025-03-21 17:18:31,923 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_bias
2025-03-21 17:18:31,924 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.weight
2025-03-21 17:18:31,924 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.bias
2025-03-21 17:18:31,925 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.weight
2025-03-21 17:18:31,925 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.bias
2025-03-21 17:18:31,926 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.weight
2025-03-21 17:18:31,926 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.bias
2025-03-21 17:18:31,927 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.weight
2025-03-21 17:18:31,928 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.bias
2025-03-21 17:18:31,928 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.weight
2025-03-21 17:18:31,929 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.bias
2025-03-21 17:18:31,929 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.weight
2025-03-21 17:18:31,930 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.bias
2025-03-21 17:18:31,930 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: custom_src_module.emb.Embedding.weight
2025-03-21 17:18:31,931 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.weight
2025-03-21 17:18:31,931 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.bias
2025-03-21 17:18:31,932 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.weight
2025-03-21 17:18:31,932 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.bias
2025-03-21 17:18:31,933 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.weight
2025-03-21 17:18:31,933 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.bias
2025-03-21 17:18:31,934 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: embedding.Embedding.weight
2025-03-21 17:18:31,934 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l0
2025-03-21 17:18:31,935 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l0
2025-03-21 17:18:31,935 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l0
2025-03-21 17:18:31,936 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l0
2025-03-21 17:18:31,936 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l1
2025-03-21 17:18:31,937 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l1
2025-03-21 17:18:31,937 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l1
2025-03-21 17:18:31,938 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l1
2025-03-21 17:18:31,938 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.weight
2025-03-21 17:18:31,939 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.bias
2025-03-21 17:18:31,939 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.weight
2025-03-21 17:18:31,940 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.bias
2025-03-21 17:18:31,940 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.weight
2025-03-21 17:18:31,941 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_2/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.bias
2025-03-21 17:18:31,944 - speechbrain.core - INFO - Info: precision arg from hparam file is used
2025-03-21 17:18:31,945 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2025-03-21 17:18:31,945 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2025-03-21 17:18:32,121 - speechbrain.core - INFO - Gradscaler enabled: True. Using precision: fp16.
2025-03-21 17:18:32,122 - speechbrain.core - INFO - ASR Model Statistics:
* Total Number of Trainable Parameters: 13.3M
* Total Number of Parameters: 13.3M
* Trainable Parameters represent 100.0000% of the total size.
2025-03-21 17:18:32,126 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-14-38+00
2025-03-21 17:18:32,474 - speechbrain.utils.epoch_loop - INFO - Going into epoch 2
2025-03-21 17:19:47,020 - speechbrain.utils.train_logger - INFO - epoch: 2, lr: 1.62e-05, steps: 204, optimizer: Adam - train loss: 1.28e+02 - valid loss: 1.04e+02, valid ACC: 1.86e-01
2025-03-21 17:19:48,059 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-19-47+00
2025-03-21 17:19:48,068 - speechbrain.utils.epoch_loop - INFO - Going into epoch 3
2025-03-21 17:20:56,807 - speechbrain.utils.train_logger - INFO - epoch: 3, lr: 2.44e-05, steps: 306, optimizer: Adam - train loss: 1.01e+02 - valid loss: 85.98, valid ACC: 1.86e-01
2025-03-21 17:20:57,256 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-20-56+00
2025-03-21 17:20:57,278 - speechbrain.utils.epoch_loop - INFO - Going into epoch 4
2025-03-21 17:22:01,513 - speechbrain.utils.train_logger - INFO - epoch: 4, lr: 3.26e-05, steps: 408, optimizer: Adam - train loss: 78.74 - valid loss: 64.83, valid ACC: 1.86e-01
2025-03-21 17:22:01,951 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-22-01+00
2025-03-21 17:22:01,978 - speechbrain.utils.epoch_loop - INFO - Going into epoch 5
2025-03-21 17:23:09,316 - speechbrain.utils.train_logger - INFO - epoch: 5, lr: 4.07e-05, steps: 510, optimizer: Adam - train loss: 55.72 - valid loss: 45.24, valid ACC: 1.89e-01
2025-03-21 17:23:09,854 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-23-09+00
2025-03-21 17:23:09,898 - speechbrain.utils.epoch_loop - INFO - Going into epoch 6
2025-03-21 17:24:15,013 - speechbrain.utils.train_logger - INFO - epoch: 6, lr: 4.89e-05, steps: 612, optimizer: Adam - train loss: 38.43 - valid loss: 33.53, valid ACC: 2.00e-01
2025-03-21 17:24:15,336 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-24-15+00
2025-03-21 17:24:15,402 - speechbrain.utils.epoch_loop - INFO - Going into epoch 7
2025-03-21 17:25:21,520 - speechbrain.utils.train_logger - INFO - epoch: 7, lr: 5.70e-05, steps: 714, optimizer: Adam - train loss: 30.13 - valid loss: 28.93, valid ACC: 2.51e-01
2025-03-21 17:25:21,966 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-25-21+00
2025-03-21 17:25:22,046 - speechbrain.utils.epoch_loop - INFO - Going into epoch 8
2025-03-21 17:26:27,345 - speechbrain.utils.train_logger - INFO - epoch: 8, lr: 6.52e-05, steps: 816, optimizer: Adam - train loss: 27.07 - valid loss: 27.07, valid ACC: 2.90e-01
2025-03-21 17:26:27,732 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-26-27+00
2025-03-21 17:26:27,842 - speechbrain.utils.epoch_loop - INFO - Going into epoch 9
2025-03-21 17:27:32,716 - speechbrain.utils.train_logger - INFO - epoch: 9, lr: 7.34e-05, steps: 918, optimizer: Adam - train loss: 25.63 - valid loss: 25.91, valid ACC: 3.19e-01
2025-03-21 17:27:33,081 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-27-32+00
2025-03-21 17:27:33,203 - speechbrain.utils.epoch_loop - INFO - Going into epoch 10
2025-03-21 17:29:24,412 - speechbrain.utils.train_logger - INFO - epoch: 10, lr: 8.15e-05, steps: 1020, optimizer: Adam - train loss: 24.51 - valid loss: 24.85, valid ACC: 3.66e-01, valid WER: 98.89
2025-03-21 17:29:24,903 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-29-24+00
2025-03-21 17:29:25,049 - speechbrain.utils.epoch_loop - INFO - Going into epoch 11
2025-03-21 17:30:30,806 - speechbrain.utils.train_logger - INFO - epoch: 11, lr: 8.97e-05, steps: 1122, optimizer: Adam - train loss: 23.37 - valid loss: 24.04, valid ACC: 4.01e-01
2025-03-21 17:30:31,243 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-30-30+00
2025-03-21 17:30:31,427 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-14-38+00
2025-03-21 17:30:31,427 - speechbrain.utils.epoch_loop - INFO - Going into epoch 12
2025-03-21 17:31:35,744 - speechbrain.utils.train_logger - INFO - epoch: 12, lr: 9.78e-05, steps: 1224, optimizer: Adam - train loss: 22.42 - valid loss: 23.01, valid ACC: 4.48e-01
2025-03-21 17:31:36,405 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-31-35+00
2025-03-21 17:31:36,604 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-19-47+00
2025-03-21 17:31:36,605 - speechbrain.utils.epoch_loop - INFO - Going into epoch 13
2025-03-21 17:32:41,697 - speechbrain.utils.train_logger - INFO - epoch: 13, lr: 1.06e-04, steps: 1326, optimizer: Adam - train loss: 21.40 - valid loss: 22.15, valid ACC: 4.94e-01
2025-03-21 17:32:42,244 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-32-41+00
2025-03-21 17:32:42,451 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-20-56+00
2025-03-21 17:32:42,451 - speechbrain.utils.epoch_loop - INFO - Going into epoch 14
2025-03-21 17:33:47,136 - speechbrain.utils.train_logger - INFO - epoch: 14, lr: 1.14e-04, steps: 1428, optimizer: Adam - train loss: 20.44 - valid loss: 21.44, valid ACC: 5.40e-01
2025-03-21 17:33:47,535 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-33-47+00
2025-03-21 17:33:47,763 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-22-01+00
2025-03-21 17:33:47,763 - speechbrain.utils.epoch_loop - INFO - Going into epoch 15
2025-03-21 17:34:52,346 - speechbrain.utils.train_logger - INFO - epoch: 15, lr: 1.22e-04, steps: 1530, optimizer: Adam - train loss: 19.69 - valid loss: 20.72, valid ACC: 5.69e-01
2025-03-21 17:34:52,766 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-34-52+00
2025-03-21 17:34:53,023 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-23-09+00
2025-03-21 17:34:53,023 - speechbrain.utils.epoch_loop - INFO - Going into epoch 16
2025-03-21 17:36:22,092 - speechbrain.utils.train_logger - INFO - epoch: 16, lr: 1.30e-04, steps: 1632, optimizer: Adam - train loss: 18.93 - valid loss: 20.21, valid ACC: 5.94e-01
2025-03-21 17:36:22,549 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-36-22+00
2025-03-21 17:36:22,805 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-24-15+00
2025-03-21 17:36:22,805 - speechbrain.utils.epoch_loop - INFO - Going into epoch 17
2025-03-21 17:37:28,112 - speechbrain.utils.train_logger - INFO - epoch: 17, lr: 1.39e-04, steps: 1734, optimizer: Adam - train loss: 18.30 - valid loss: 19.66, valid ACC: 6.19e-01
2025-03-21 17:37:28,393 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-37-28+00
2025-03-21 17:37:28,677 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-25-21+00
2025-03-21 17:37:28,678 - speechbrain.utils.epoch_loop - INFO - Going into epoch 18
2025-03-21 17:38:34,996 - speechbrain.utils.train_logger - INFO - epoch: 18, lr: 1.47e-04, steps: 1836, optimizer: Adam - train loss: 17.52 - valid loss: 19.03, valid ACC: 6.34e-01
2025-03-21 17:38:35,416 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-38-34+00
2025-03-21 17:38:35,728 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-26-27+00
2025-03-21 17:38:35,728 - speechbrain.utils.epoch_loop - INFO - Going into epoch 19
2025-03-21 17:39:40,998 - speechbrain.utils.train_logger - INFO - epoch: 19, lr: 1.55e-04, steps: 1938, optimizer: Adam - train loss: 16.97 - valid loss: 18.61, valid ACC: 6.52e-01
2025-03-21 17:39:41,304 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-39-40+00
2025-03-21 17:39:41,626 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-27-32+00
2025-03-21 17:39:41,626 - speechbrain.utils.epoch_loop - INFO - Going into epoch 20
2025-03-21 17:42:27,367 - speechbrain.utils.train_logger - INFO - epoch: 20, lr: 1.63e-04, steps: 2040, optimizer: Adam - train loss: 16.45 - valid loss: 18.50, valid ACC: 6.63e-01, valid WER: 1.01e+02
2025-03-21 17:42:27,772 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-42-27+00
2025-03-21 17:42:28,097 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-29-24+00
2025-03-21 17:42:28,097 - speechbrain.utils.epoch_loop - INFO - Going into epoch 21
2025-03-21 17:43:34,971 - speechbrain.utils.train_logger - INFO - epoch: 21, lr: 1.71e-04, steps: 2142, optimizer: Adam - train loss: 16.08 - valid loss: 18.24, valid ACC: 6.72e-01
2025-03-21 17:43:35,226 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-43-34+00
2025-03-21 17:43:35,604 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-30-30+00
2025-03-21 17:43:35,604 - speechbrain.utils.epoch_loop - INFO - Going into epoch 22
2025-03-21 17:44:41,078 - speechbrain.utils.train_logger - INFO - epoch: 22, lr: 1.79e-04, steps: 2244, optimizer: Adam - train loss: 15.75 - valid loss: 17.99, valid ACC: 6.77e-01
2025-03-21 17:44:41,335 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-44-41+00
2025-03-21 17:44:41,778 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-31-35+00
2025-03-21 17:44:41,779 - speechbrain.utils.epoch_loop - INFO - Going into epoch 23
2025-03-21 17:45:46,952 - speechbrain.utils.train_logger - INFO - epoch: 23, lr: 1.88e-04, steps: 2346, optimizer: Adam - train loss: 15.43 - valid loss: 17.89, valid ACC: 6.83e-01
2025-03-21 17:45:47,276 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-45-46+00
2025-03-21 17:45:47,670 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-32-41+00
2025-03-21 17:45:47,671 - speechbrain.utils.epoch_loop - INFO - Going into epoch 24
2025-03-21 17:46:53,018 - speechbrain.utils.train_logger - INFO - epoch: 24, lr: 1.96e-04, steps: 2448, optimizer: Adam - train loss: 15.16 - valid loss: 17.81, valid ACC: 6.87e-01
2025-03-21 17:46:53,246 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-46-53+00
2025-03-21 17:46:53,650 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-33-47+00
2025-03-21 17:46:53,651 - speechbrain.utils.epoch_loop - INFO - Going into epoch 25
2025-03-21 17:47:57,882 - speechbrain.utils.train_logger - INFO - epoch: 25, lr: 2.04e-04, steps: 2550, optimizer: Adam - train loss: 14.94 - valid loss: 17.59, valid ACC: 6.91e-01
2025-03-21 17:47:58,139 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-47-57+00
2025-03-21 17:47:58,601 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-34-52+00
2025-03-21 17:47:58,601 - speechbrain.utils.epoch_loop - INFO - Going into epoch 26
2025-03-21 17:49:03,429 - speechbrain.utils.train_logger - INFO - epoch: 26, lr: 2.12e-04, steps: 2652, optimizer: Adam - train loss: 14.75 - valid loss: 17.36, valid ACC: 6.90e-01
2025-03-21 17:49:03,742 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-49-03+00
2025-03-21 17:49:04,211 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-36-22+00
2025-03-21 17:49:04,211 - speechbrain.utils.epoch_loop - INFO - Going into epoch 27
2025-03-21 17:50:09,934 - speechbrain.utils.train_logger - INFO - epoch: 27, lr: 2.20e-04, steps: 2754, optimizer: Adam - train loss: 14.51 - valid loss: 17.18, valid ACC: 6.95e-01
2025-03-21 17:50:10,359 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-50-09+00
2025-03-21 17:50:10,892 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-37-28+00
2025-03-21 17:50:10,892 - speechbrain.utils.epoch_loop - INFO - Going into epoch 28
2025-03-21 17:51:15,664 - speechbrain.utils.train_logger - INFO - epoch: 28, lr: 2.28e-04, steps: 2856, optimizer: Adam - train loss: 14.27 - valid loss: 17.06, valid ACC: 7.00e-01
2025-03-21 17:51:16,090 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-51-15+00
2025-03-21 17:51:16,596 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-38-34+00
2025-03-21 17:51:16,596 - speechbrain.utils.epoch_loop - INFO - Going into epoch 29
2025-03-21 17:52:21,029 - speechbrain.utils.train_logger - INFO - epoch: 29, lr: 2.37e-04, steps: 2958, optimizer: Adam - train loss: 14.11 - valid loss: 17.00, valid ACC: 6.99e-01
2025-03-21 17:52:21,310 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-52-21+00
2025-03-21 17:52:21,770 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-39-40+00
2025-03-21 17:52:21,771 - speechbrain.utils.epoch_loop - INFO - Going into epoch 30
2025-03-21 17:55:04,419 - speechbrain.utils.train_logger - INFO - epoch: 30, lr: 2.45e-04, steps: 3060, optimizer: Adam - train loss: 13.87 - valid loss: 16.80, valid ACC: 7.02e-01, valid WER: 1.17e+02
2025-03-21 17:55:04,933 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-55-04+00
2025-03-21 17:55:05,425 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-42-27+00
2025-03-21 17:55:05,425 - speechbrain.utils.epoch_loop - INFO - Going into epoch 31
2025-03-21 17:56:10,693 - speechbrain.utils.train_logger - INFO - epoch: 31, lr: 2.53e-04, steps: 3162, optimizer: Adam - train loss: 13.68 - valid loss: 16.78, valid ACC: 7.06e-01
2025-03-21 17:56:11,049 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-56-10+00
2025-03-21 17:56:11,576 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-43-34+00
2025-03-21 17:56:11,577 - speechbrain.utils.epoch_loop - INFO - Going into epoch 32
2025-03-21 17:57:15,744 - speechbrain.utils.train_logger - INFO - epoch: 32, lr: 2.61e-04, steps: 3264, optimizer: Adam - train loss: 13.53 - valid loss: 16.59, valid ACC: 7.03e-01
2025-03-21 17:57:16,204 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-57-15+00
2025-03-21 17:57:16,739 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-44-41+00
2025-03-21 17:57:16,740 - speechbrain.utils.epoch_loop - INFO - Going into epoch 33
2025-03-21 17:58:22,732 - speechbrain.utils.train_logger - INFO - epoch: 33, lr: 2.69e-04, steps: 3366, optimizer: Adam - train loss: 13.32 - valid loss: 16.31, valid ACC: 7.07e-01
2025-03-21 17:58:23,221 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-58-22+00
2025-03-21 17:58:23,777 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-45-46+00
2025-03-21 17:58:23,777 - speechbrain.utils.epoch_loop - INFO - Going into epoch 34
2025-03-21 17:59:28,805 - speechbrain.utils.train_logger - INFO - epoch: 34, lr: 2.77e-04, steps: 3468, optimizer: Adam - train loss: 13.14 - valid loss: 16.19, valid ACC: 7.14e-01
2025-03-21 17:59:29,184 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-59-28+00
2025-03-21 17:59:29,716 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-46-53+00
2025-03-21 17:59:29,716 - speechbrain.utils.epoch_loop - INFO - Going into epoch 35
2025-03-21 18:00:32,933 - speechbrain.utils.train_logger - INFO - epoch: 35, lr: 2.86e-04, steps: 3570, optimizer: Adam - train loss: 12.96 - valid loss: 15.89, valid ACC: 7.16e-01
2025-03-21 18:00:33,314 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-00-32+00
2025-03-21 18:00:33,911 - speechbrain.utils.epoch_loop - INFO - Going into epoch 36
2025-03-21 18:01:37,837 - speechbrain.utils.train_logger - INFO - epoch: 36, lr: 2.94e-04, steps: 3672, optimizer: Adam - train loss: 12.73 - valid loss: 15.98, valid ACC: 7.15e-01
2025-03-21 18:01:38,093 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-01-37+00
2025-03-21 18:01:38,754 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-47-57+00
2025-03-21 18:01:38,756 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-49-03+00
2025-03-21 18:01:38,756 - speechbrain.utils.epoch_loop - INFO - Going into epoch 37
2025-03-21 18:02:42,886 - speechbrain.utils.train_logger - INFO - epoch: 37, lr: 3.02e-04, steps: 3774, optimizer: Adam - train loss: 12.54 - valid loss: 15.72, valid ACC: 7.21e-01
2025-03-21 18:02:43,204 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-02-42+00
2025-03-21 18:02:43,795 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-50-09+00
2025-03-21 18:02:43,796 - speechbrain.utils.epoch_loop - INFO - Going into epoch 38
2025-03-21 18:03:48,912 - speechbrain.utils.train_logger - INFO - epoch: 38, lr: 3.10e-04, steps: 3876, optimizer: Adam - train loss: 12.28 - valid loss: 15.57, valid ACC: 7.20e-01
2025-03-21 18:03:49,354 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-03-48+00
2025-03-21 18:03:49,981 - speechbrain.utils.epoch_loop - INFO - Going into epoch 39
2025-03-21 18:04:53,813 - speechbrain.utils.train_logger - INFO - epoch: 39, lr: 3.18e-04, steps: 3978, optimizer: Adam - train loss: 12.04 - valid loss: 15.28, valid ACC: 7.24e-01
2025-03-21 18:04:54,312 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-04-53+00
2025-03-21 18:04:55,016 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-52-21+00
2025-03-21 18:04:55,017 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-51-15+00
2025-03-21 18:04:55,017 - speechbrain.utils.epoch_loop - INFO - Going into epoch 40
2025-03-21 18:07:10,449 - speechbrain.utils.train_logger - INFO - epoch: 40, lr: 3.26e-04, steps: 4080, optimizer: Adam - train loss: 11.83 - valid loss: 15.08, valid ACC: 7.27e-01, valid WER: 97.54
2025-03-21 18:07:10,741 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-07-10+00
2025-03-21 18:07:11,458 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-55-04+00
2025-03-21 18:07:11,458 - speechbrain.utils.epoch_loop - INFO - Going into epoch 41
2025-03-21 18:08:14,667 - speechbrain.utils.train_logger - INFO - epoch: 41, lr: 3.34e-04, steps: 4182, optimizer: Adam - train loss: 11.62 - valid loss: 15.04, valid ACC: 7.27e-01
2025-03-21 18:08:14,982 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-08-14+00
2025-03-21 18:08:15,656 - speechbrain.utils.epoch_loop - INFO - Going into epoch 42
2025-03-21 18:09:20,498 - speechbrain.utils.train_logger - INFO - epoch: 42, lr: 3.43e-04, steps: 4284, optimizer: Adam - train loss: 11.39 - valid loss: 14.93, valid ACC: 7.32e-01
2025-03-21 18:09:20,817 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-09-20+00
2025-03-21 18:09:21,564 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-56-10+00
2025-03-21 18:09:21,566 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-57-15+00
2025-03-21 18:09:21,566 - speechbrain.utils.epoch_loop - INFO - Going into epoch 43
2025-03-21 18:10:44,595 - speechbrain.utils.train_logger - INFO - epoch: 43, lr: 3.51e-04, steps: 4386, optimizer: Adam - train loss: 11.17 - valid loss: 14.67, valid ACC: 7.32e-01
2025-03-21 18:10:44,880 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-10-44+00
2025-03-21 18:10:45,588 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-58-22+00
2025-03-21 18:10:45,588 - speechbrain.utils.epoch_loop - INFO - Going into epoch 44
2025-03-21 18:11:49,371 - speechbrain.utils.train_logger - INFO - epoch: 44, lr: 3.59e-04, steps: 4488, optimizer: Adam - train loss: 10.97 - valid loss: 14.40, valid ACC: 7.35e-01
2025-03-21 18:11:49,640 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-11-49+00
2025-03-21 18:11:50,347 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+17-59-28+00
2025-03-21 18:11:50,348 - speechbrain.utils.epoch_loop - INFO - Going into epoch 45
2025-03-21 18:12:54,661 - speechbrain.utils.train_logger - INFO - epoch: 45, lr: 3.67e-04, steps: 4590, optimizer: Adam - train loss: 10.69 - valid loss: 14.22, valid ACC: 7.36e-01
2025-03-21 18:12:54,995 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-12-54+00
2025-03-21 18:12:55,730 - speechbrain.utils.epoch_loop - INFO - Going into epoch 46
2025-03-21 18:14:00,332 - speechbrain.utils.train_logger - INFO - epoch: 46, lr: 3.75e-04, steps: 4692, optimizer: Adam - train loss: 10.40 - valid loss: 14.18, valid ACC: 7.40e-01
2025-03-21 18:14:00,615 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-14-00+00
2025-03-21 18:14:01,515 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-01-37+00
2025-03-21 18:14:01,516 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-00-32+00
2025-03-21 18:14:01,517 - speechbrain.utils.epoch_loop - INFO - Going into epoch 47
2025-03-21 18:15:06,539 - speechbrain.utils.train_logger - INFO - epoch: 47, lr: 3.83e-04, steps: 4794, optimizer: Adam - train loss: 10.18 - valid loss: 13.61, valid ACC: 7.55e-01
2025-03-21 18:15:06,805 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-15-06+00
2025-03-21 18:15:07,600 - speechbrain.utils.epoch_loop - INFO - Going into epoch 48
2025-03-21 18:16:12,492 - speechbrain.utils.train_logger - INFO - epoch: 48, lr: 3.92e-04, steps: 4896, optimizer: Adam - train loss: 9.94 - valid loss: 13.54, valid ACC: 7.51e-01
2025-03-21 18:16:12,777 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-16-12+00
2025-03-21 18:16:13,634 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-03-48+00
2025-03-21 18:16:13,635 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-02-42+00
2025-03-21 18:16:13,636 - speechbrain.utils.epoch_loop - INFO - Going into epoch 49
2025-03-21 18:17:18,264 - speechbrain.utils.train_logger - INFO - epoch: 49, lr: 4.00e-04, steps: 4998, optimizer: Adam - train loss: 9.59 - valid loss: 13.23, valid ACC: 7.55e-01
2025-03-21 18:17:18,464 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-17-18+00
2025-03-21 18:17:19,264 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-04-53+00
2025-03-21 18:17:19,264 - speechbrain.utils.epoch_loop - INFO - Going into epoch 50
2025-03-21 18:19:38,118 - speechbrain.utils.train_logger - INFO - epoch: 50, lr: 4.08e-04, steps: 5100, optimizer: Adam - train loss: 9.34 - valid loss: 13.13, valid ACC: 7.61e-01, valid WER: 82.24
2025-03-21 18:19:38,493 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-19-38+00
2025-03-21 18:19:39,330 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-07-10+00
2025-03-21 18:19:39,331 - speechbrain.utils.epoch_loop - INFO - Going into epoch 51
2025-03-21 18:20:44,451 - speechbrain.utils.train_logger - INFO - epoch: 51, lr: 4.16e-04, steps: 5202, optimizer: Adam - train loss: 9.03 - valid loss: 12.76, valid ACC: 7.65e-01
2025-03-21 18:20:44,880 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-20-44+00
2025-03-21 18:20:45,765 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-08-14+00
2025-03-21 18:20:45,765 - speechbrain.utils.epoch_loop - INFO - Going into epoch 52
2025-03-21 18:21:49,700 - speechbrain.utils.train_logger - INFO - epoch: 52, lr: 4.24e-04, steps: 5304, optimizer: Adam - train loss: 8.78 - valid loss: 12.65, valid ACC: 7.64e-01
2025-03-21 18:21:50,207 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-21-49+00
2025-03-21 18:21:51,071 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-09-20+00
2025-03-21 18:21:51,071 - speechbrain.utils.epoch_loop - INFO - Going into epoch 53
2025-03-21 18:22:54,928 - speechbrain.utils.train_logger - INFO - epoch: 53, lr: 4.32e-04, steps: 5406, optimizer: Adam - train loss: 8.47 - valid loss: 12.26, valid ACC: 7.66e-01
2025-03-21 18:22:55,302 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-22-54+00
2025-03-21 18:22:56,220 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-10-44+00
2025-03-21 18:22:56,221 - speechbrain.utils.epoch_loop - INFO - Going into epoch 54
2025-03-21 18:24:00,952 - speechbrain.utils.train_logger - INFO - epoch: 54, lr: 4.41e-04, steps: 5508, optimizer: Adam - train loss: 8.15 - valid loss: 12.11, valid ACC: 7.71e-01
2025-03-21 18:24:01,323 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-24-00+00
2025-03-21 18:24:02,306 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-11-49+00
2025-03-21 18:24:02,307 - speechbrain.utils.epoch_loop - INFO - Going into epoch 55
2025-03-21 18:25:07,391 - speechbrain.utils.train_logger - INFO - epoch: 55, lr: 4.49e-04, steps: 5610, optimizer: Adam - train loss: 7.85 - valid loss: 11.93, valid ACC: 7.76e-01
2025-03-21 18:25:07,725 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-25-07+00
2025-03-21 18:25:08,720 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-12-54+00
2025-03-21 18:25:08,720 - speechbrain.utils.epoch_loop - INFO - Going into epoch 56
2025-03-21 18:26:13,878 - speechbrain.utils.train_logger - INFO - epoch: 56, lr: 4.57e-04, steps: 5712, optimizer: Adam - train loss: 7.63 - valid loss: 11.75, valid ACC: 7.73e-01
2025-03-21 18:26:14,407 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-26-13+00
2025-03-21 18:26:15,295 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-14-00+00
2025-03-21 18:26:15,295 - speechbrain.utils.epoch_loop - INFO - Going into epoch 57
2025-03-21 18:27:19,513 - speechbrain.utils.train_logger - INFO - epoch: 57, lr: 4.65e-04, steps: 5814, optimizer: Adam - train loss: 7.39 - valid loss: 11.29, valid ACC: 7.84e-01
2025-03-21 18:27:19,814 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-27-19+00
2025-03-21 18:27:20,865 - speechbrain.utils.epoch_loop - INFO - Going into epoch 58
2025-03-21 18:28:26,021 - speechbrain.utils.train_logger - INFO - epoch: 58, lr: 4.73e-04, steps: 5916, optimizer: Adam - train loss: 7.08 - valid loss: 12.04, valid ACC: 7.68e-01
2025-03-21 18:28:26,374 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-28-26+00
2025-03-21 18:28:27,496 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-16-12+00
2025-03-21 18:28:27,497 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-15-06+00
2025-03-21 18:28:27,498 - speechbrain.utils.epoch_loop - INFO - Going into epoch 59
2025-03-21 18:29:31,722 - speechbrain.utils.train_logger - INFO - epoch: 59, lr: 4.81e-04, steps: 6018, optimizer: Adam - train loss: 6.90 - valid loss: 11.58, valid ACC: 7.77e-01
2025-03-21 18:29:32,057 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-29-31+00
2025-03-21 18:29:32,986 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-17-18+00
2025-03-21 18:29:32,986 - speechbrain.utils.epoch_loop - INFO - Going into epoch 60
2025-03-21 18:31:52,676 - speechbrain.utils.train_logger - INFO - epoch: 60, lr: 4.90e-04, steps: 6120, optimizer: Adam - train loss: 6.61 - valid loss: 11.03, valid ACC: 7.87e-01, valid WER: 59.63
2025-03-21 18:31:53,005 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-31-52+00
2025-03-21 18:31:54,095 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-19-38+00
2025-03-21 18:31:54,095 - speechbrain.utils.epoch_loop - INFO - Going into epoch 61
2025-03-21 18:32:58,411 - speechbrain.utils.train_logger - INFO - epoch: 61, lr: 4.98e-04, steps: 6222, optimizer: Adam - train loss: 6.33 - valid loss: 10.83, valid ACC: 7.93e-01
2025-03-21 18:32:58,742 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-32-58+00
2025-03-21 18:32:59,710 - speechbrain.utils.epoch_loop - INFO - Going into epoch 62
2025-03-21 18:34:03,209 - speechbrain.utils.train_logger - INFO - epoch: 62, lr: 5.06e-04, steps: 6324, optimizer: Adam - train loss: 6.19 - valid loss: 10.17, valid ACC: 7.96e-01
2025-03-21 18:34:03,571 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-34-03+00
2025-03-21 18:34:04,708 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-21-49+00
2025-03-21 18:34:04,709 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-20-44+00
2025-03-21 18:34:04,709 - speechbrain.utils.epoch_loop - INFO - Going into epoch 63
2025-03-21 18:35:08,798 - speechbrain.utils.train_logger - INFO - epoch: 63, lr: 5.14e-04, steps: 6426, optimizer: Adam - train loss: 5.95 - valid loss: 10.41, valid ACC: 7.93e-01
2025-03-21 18:35:09,105 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-35-08+00
2025-03-21 18:35:10,226 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-22-54+00
2025-03-21 18:35:10,227 - speechbrain.utils.epoch_loop - INFO - Going into epoch 64
2025-03-21 18:36:13,986 - speechbrain.utils.train_logger - INFO - epoch: 64, lr: 5.22e-04, steps: 6528, optimizer: Adam - train loss: 5.78 - valid loss: 10.41, valid ACC: 7.95e-01
2025-03-21 18:36:14,214 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-36-13+00
2025-03-21 18:36:15,345 - speechbrain.utils.epoch_loop - INFO - Going into epoch 65
2025-03-21 18:37:18,851 - speechbrain.utils.train_logger - INFO - epoch: 65, lr: 5.30e-04, steps: 6630, optimizer: Adam - train loss: 5.57 - valid loss: 10.80, valid ACC: 7.90e-01
2025-03-21 18:37:19,140 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-37-18+00
2025-03-21 18:37:20,394 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-24-00+00
2025-03-21 18:37:20,395 - speechbrain.utils.epoch_loop - INFO - Going into epoch 66
2025-03-21 18:38:24,193 - speechbrain.utils.train_logger - INFO - epoch: 66, lr: 5.38e-04, steps: 6732, optimizer: Adam - train loss: 5.38 - valid loss: 9.95, valid ACC: 7.99e-01
2025-03-21 18:38:24,567 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-38-24+00
2025-03-21 18:38:25,832 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-26-13+00
2025-03-21 18:38:25,832 - speechbrain.utils.epoch_loop - INFO - Going into epoch 67
2025-03-21 18:39:29,263 - speechbrain.utils.train_logger - INFO - epoch: 67, lr: 5.47e-04, steps: 6834, optimizer: Adam - train loss: 5.22 - valid loss: 9.81, valid ACC: 8.05e-01
2025-03-21 18:39:29,504 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-39-29+00
2025-03-21 18:39:30,761 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-25-07+00
2025-03-21 18:39:30,761 - speechbrain.utils.epoch_loop - INFO - Going into epoch 68
2025-03-21 18:40:35,879 - speechbrain.utils.train_logger - INFO - epoch: 68, lr: 5.55e-04, steps: 6936, optimizer: Adam - train loss: 5.08 - valid loss: 9.70, valid ACC: 8.05e-01
2025-03-21 18:40:36,287 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-40-35+00
2025-03-21 18:40:37,485 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-28-26+00
2025-03-21 18:40:37,486 - speechbrain.utils.epoch_loop - INFO - Going into epoch 69
2025-03-21 18:41:42,378 - speechbrain.utils.train_logger - INFO - epoch: 69, lr: 5.63e-04, steps: 7038, optimizer: Adam - train loss: 4.89 - valid loss: 9.78, valid ACC: 8.08e-01
2025-03-21 18:41:42,717 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-41-42+00
2025-03-21 18:41:43,960 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-27-19+00
2025-03-21 18:41:43,961 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-29-31+00
2025-03-21 18:41:43,961 - speechbrain.utils.epoch_loop - INFO - Going into epoch 70
2025-03-21 18:44:03,911 - speechbrain.utils.train_logger - INFO - epoch: 70, lr: 5.71e-04, steps: 7140, optimizer: Adam - train loss: 4.73 - valid loss: 9.96, valid ACC: 8.00e-01, valid WER: 50.06
2025-03-21 18:44:04,365 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-44-03+00
2025-03-21 18:44:05,575 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-31-52+00
2025-03-21 18:44:05,575 - speechbrain.utils.epoch_loop - INFO - Going into epoch 71
2025-03-21 18:45:08,760 - speechbrain.utils.train_logger - INFO - epoch: 71, lr: 5.79e-04, steps: 7242, optimizer: Adam - train loss: 4.67 - valid loss: 9.87, valid ACC: 8.04e-01
2025-03-21 18:45:09,032 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-45-08+00
2025-03-21 18:45:10,195 - speechbrain.utils.epoch_loop - INFO - Going into epoch 72
2025-03-21 18:46:15,070 - speechbrain.utils.train_logger - INFO - epoch: 72, lr: 5.87e-04, steps: 7344, optimizer: Adam - train loss: 4.57 - valid loss: 9.77, valid ACC: 8.05e-01
2025-03-21 18:46:15,351 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-46-15+00
2025-03-21 18:46:16,834 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-32-58+00
2025-03-21 18:46:16,835 - speechbrain.utils.epoch_loop - INFO - Going into epoch 73
2025-03-21 18:47:20,255 - speechbrain.utils.train_logger - INFO - epoch: 73, lr: 5.96e-04, steps: 7446, optimizer: Adam - train loss: 4.40 - valid loss: 9.35, valid ACC: 8.05e-01
2025-03-21 18:47:20,542 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-47-20+00
2025-03-21 18:47:22,074 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-35-08+00
2025-03-21 18:47:22,074 - speechbrain.utils.epoch_loop - INFO - Going into epoch 74
2025-03-21 18:48:28,216 - speechbrain.utils.train_logger - INFO - epoch: 74, lr: 6.04e-04, steps: 7548, optimizer: Adam - train loss: 4.36 - valid loss: 9.65, valid ACC: 8.03e-01
2025-03-21 18:48:28,564 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-48-28+00
2025-03-21 18:48:29,936 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-36-13+00
2025-03-21 18:48:29,936 - speechbrain.utils.epoch_loop - INFO - Going into epoch 75
2025-03-21 18:49:33,895 - speechbrain.utils.train_logger - INFO - epoch: 75, lr: 6.12e-04, steps: 7650, optimizer: Adam - train loss: 4.22 - valid loss: 9.12, valid ACC: 8.13e-01
2025-03-21 18:49:34,217 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-49-33+00
2025-03-21 18:49:35,704 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-34-03+00
2025-03-21 18:49:35,706 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-37-18+00
2025-03-21 18:49:35,706 - speechbrain.utils.epoch_loop - INFO - Going into epoch 76
2025-03-21 18:50:40,305 - speechbrain.utils.train_logger - INFO - epoch: 76, lr: 6.20e-04, steps: 7752, optimizer: Adam - train loss: 4.12 - valid loss: 9.19, valid ACC: 8.11e-01
2025-03-21 18:50:40,606 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-50-40+00
2025-03-21 18:50:41,859 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-38-24+00
2025-03-21 18:50:41,860 - speechbrain.utils.epoch_loop - INFO - Going into epoch 77
2025-03-21 18:51:46,722 - speechbrain.utils.train_logger - INFO - epoch: 77, lr: 6.28e-04, steps: 7854, optimizer: Adam - train loss: 4.10 - valid loss: 9.32, valid ACC: 8.17e-01
2025-03-21 18:51:47,060 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-51-46+00
2025-03-21 18:51:48,390 - speechbrain.utils.epoch_loop - INFO - Going into epoch 78
2025-03-21 18:52:52,691 - speechbrain.utils.train_logger - INFO - epoch: 78, lr: 6.36e-04, steps: 7956, optimizer: Adam - train loss: 3.99 - valid loss: 9.02, valid ACC: 8.21e-01
2025-03-21 18:52:52,966 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-52-52+00
2025-03-21 18:52:54,480 - speechbrain.utils.epoch_loop - INFO - Going into epoch 79
2025-03-21 18:53:59,714 - speechbrain.utils.train_logger - INFO - epoch: 79, lr: 6.45e-04, steps: 8058, optimizer: Adam - train loss: 3.93 - valid loss: 9.15, valid ACC: 8.09e-01
2025-03-21 18:54:00,062 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-53-59+00
2025-03-21 18:54:01,807 - speechbrain.utils.epoch_loop - INFO - Going into epoch 80
2025-03-21 18:56:17,170 - speechbrain.utils.train_logger - INFO - epoch: 80, lr: 6.53e-04, steps: 8160, optimizer: Adam - train loss: 3.96 - valid loss: 9.06, valid ACC: 8.14e-01, valid WER: 39.73
2025-03-21 18:56:17,524 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-56-17+00
2025-03-21 18:56:19,386 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-44-03+00
2025-03-21 18:56:19,398 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-56-17+00
2025-03-21 18:56:19,475 - speechbrain.dataio.dataloader - DEBUG - SaveableDataLoader was requested to load a checkpoint, but the DataLoader has already been iterated. The DataLoader file will be ignored. This is normal in evaluation, when a checkpoint is loaded just to retrieve the best model.
2025-03-21 19:03:36,750 - speechbrain.utils.train_logger - INFO - Epoch loaded: 80 - test loss: 8.85, test ACC: 8.31e-01, test WER: 36.77
2025-03-21 19:03:37,332 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+19-03-37+00
2025-03-21 19:03:38,999 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-48-28+00
2025-03-21 19:03:39,000 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-51-46+00
2025-03-21 19:03:39,002 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-50-40+00
2025-03-21 19:03:39,003 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-47-20+00
2025-03-21 19:03:39,004 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-46-15+00
2025-03-21 19:03:39,005 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-56-17+00
2025-03-21 19:03:39,007 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-49-33+00
2025-03-21 19:03:39,008 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-39-29+00
2025-03-21 19:03:39,009 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-40-35+00
2025-03-21 19:03:39,010 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-45-08+00
2025-03-21 19:03:39,012 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-53-59+00
2025-03-21 19:03:39,013 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-41-42+00
2025-03-21 19:03:39,014 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_2/conformer_small/7775/save/CKPT+2025-03-21+18-52-52+00
