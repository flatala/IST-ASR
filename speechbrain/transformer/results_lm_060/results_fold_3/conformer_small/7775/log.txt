2025-03-21 19:04:28,669 - speechbrain.utils.quirks - INFO - Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
2025-03-21 19:04:28,670 - speechbrain.utils.quirks - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
2025-03-21 19:04:28,670 - speechbrain.core - INFO - Beginning experiment!
2025-03-21 19:04:28,670 - speechbrain.core - INFO - Experiment folder: results_lm_060/results_fold_3/conformer_small/7775
2025-03-21 19:04:35,703 - speechbrain.utils.superpowers - DEBUG - aiohappyeyeballs==2.6.1
aiohttp==3.11.13
aiosignal==1.3.2
alembic==1.15.1
anyio==4.9.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==3.0.0
async-lru==2.0.5
async-timeout==5.0.1
attrs==25.3.0
babel==2.17.0
banal==1.0.6
beautifulsoup4==4.13.3
black==24.3.0
bleach==6.2.0
certifi==2025.1.31
cffi==1.17.1
cfgv==3.4.0
charset-normalizer==3.4.1
click==8.1.7
comm==0.2.2
contourpy==1.3.0
cycler==0.12.1
dataset==1.6.2
datasets==3.3.2
debugpy==1.8.13
decorator==5.2.1
defusedxml==0.7.1
dill==0.3.8
distlib==0.3.9
docstring_parser_fork==0.0.12
exceptiongroup==1.2.2
executing==2.2.0
fastjsonschema==2.21.1
filelock==3.17.0
flake8==7.0.0
fonttools==4.56.0
fqdn==1.5.1
frozenlist==1.5.0
fsspec==2024.12.0
greenlet==3.1.1
h11==0.14.0
httpcore==1.0.7
httpx==0.28.1
huggingface-hub==0.29.3
HyperPyYAML==1.2.2
identify==2.6.9
idna==3.10
importlib_metadata==8.6.1
importlib_resources==6.5.2
iniconfig==2.0.0
ipykernel==6.29.5
ipython==8.18.1
isoduration==20.11.0
isort==5.13.2
jedi==0.19.2
Jinja2==3.1.6
joblib==1.4.2
json5==0.10.0
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
jupyter-events==0.12.0
jupyter-lsp==2.2.5
jupyter_client==8.6.3
jupyter_core==5.7.2
jupyter_server==2.15.0
jupyter_server_terminals==0.5.3
jupyterlab==4.3.6
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
kiwisolver==1.4.7
Mako==1.3.9
MarkupSafe==3.0.2
matplotlib==3.9.4
matplotlib-inline==0.1.7
mccabe==0.7.0
mistune==3.1.2
mpmath==1.3.0
multidict==6.1.0
multiprocess==0.70.16
mypy-extensions==1.0.0
nbclient==0.10.2
nbconvert==7.16.6
nbformat==5.10.4
nest-asyncio==1.6.0
networkx==3.2.1
nodeenv==1.9.1
notebook_shim==0.2.4
numpy==1.26.4
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-cusparselt-cu12==0.6.2
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.4.127
overrides==7.7.0
packaging==24.2
pandas==2.2.3
pandocfilters==1.5.1
parso==0.8.4
pathspec==0.12.1
pexpect==4.9.0
pillow==11.1.0
platformdirs==4.3.6
pluggy==1.5.0
pre_commit==4.1.0
prometheus_client==0.21.1
prompt_toolkit==3.0.50
propcache==0.3.0
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==19.0.1
pycodestyle==2.11.0
pycparser==2.22
pydoclint==0.4.1
pyflakes==3.2.0
Pygments==2.19.1
pygtrie==2.5.0
pyparsing==3.2.1
pytest==7.4.0
python-dateutil==2.9.0.post0
python-json-logger==3.3.0
pytz==2025.1
PyYAML==6.0.2
pyzmq==26.3.0
referencing==0.36.2
regex==2024.11.6
requests==2.32.3
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.23.1
ruamel.yaml==0.18.10
ruamel.yaml.clib==0.2.12
safetensors==0.5.3
scipy==1.12.0
Send2Trash==1.8.3
sentencepiece==0.2.0
six==1.17.0
sniffio==1.3.1
soupsieve==2.6
speechbrain==1.0.2
SQLAlchemy==1.4.54
stack-data==0.6.3
sympy==1.13.1
terminado==0.18.1
tinycss2==1.4.0
tokenizers==0.21.0
tomli==2.2.1
torch==2.6.0
torchaudio==2.6.0
torchvision==0.21.0
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
transformers==4.49.0
triton==3.2.0
types-python-dateutil==2.9.0.20241206
typing_extensions==4.12.2
tzdata==2025.1
uri-template==1.3.0
urllib3==2.3.0
virtualenv==20.29.3
wcwidth==0.2.13
webcolors==24.11.1
webencodings==0.5.1
websocket-client==1.8.0
xxhash==3.5.0
yamllint==1.35.1
yarl==1.18.3
zipp==3.21.0


2025-03-21 19:04:35,745 - speechbrain.utils.superpowers - DEBUG - d8ae0d1


2025-03-21 19:04:35,990 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results_lm_060/results_fold_3/conformer_small/7775/save.
2025-03-21 19:04:35,991 - speechbrain.utils.fetching - DEBUG - Fetch: Local file found, creating symlink '/scratch/asasin/IST-ASR/speechbrain/transformer/LM_weights_13_epochs/lm.ckpt' -> '/scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt'
2025-03-21 19:04:35,992 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["lm"] = /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt
2025-03-21 19:04:35,992 - speechbrain.utils.fetching - DEBUG - Fetch: Local file found, creating symlink '/scratch/asasin/IST-ASR/speechbrain/transformer/LM_weights_13_epochs/tokenizer.ckpt' -> '/scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/tokenizer.ckpt'
2025-03-21 19:04:35,993 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["tokenizer"] = /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/tokenizer.ckpt
2025-03-21 19:04:35,993 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2025-03-21 19:04:35,993 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): lm -> /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt
2025-03-21 19:04:35,994 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): tokenizer -> /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/tokenizer.ckpt
2025-03-21 19:04:36,416 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: positional_encoding.pe
2025-03-21 19:04:36,417 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_weight
2025-03-21 19:04:36,418 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_bias
2025-03-21 19:04:36,418 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.weight
2025-03-21 19:04:36,419 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.bias
2025-03-21 19:04:36,419 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.weight
2025-03-21 19:04:36,420 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.bias
2025-03-21 19:04:36,420 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.weight
2025-03-21 19:04:36,421 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.bias
2025-03-21 19:04:36,421 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.weight
2025-03-21 19:04:36,422 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.bias
2025-03-21 19:04:36,422 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.weight
2025-03-21 19:04:36,423 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.bias
2025-03-21 19:04:36,424 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_weight
2025-03-21 19:04:36,424 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_bias
2025-03-21 19:04:36,425 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.weight
2025-03-21 19:04:36,425 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.bias
2025-03-21 19:04:36,426 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.weight
2025-03-21 19:04:36,426 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.bias
2025-03-21 19:04:36,427 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.weight
2025-03-21 19:04:36,427 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.bias
2025-03-21 19:04:36,428 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.weight
2025-03-21 19:04:36,428 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.bias
2025-03-21 19:04:36,429 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.weight
2025-03-21 19:04:36,429 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.bias
2025-03-21 19:04:36,430 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_weight
2025-03-21 19:04:36,430 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_bias
2025-03-21 19:04:36,431 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.weight
2025-03-21 19:04:36,432 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.bias
2025-03-21 19:04:36,432 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.weight
2025-03-21 19:04:36,433 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.bias
2025-03-21 19:04:36,433 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.weight
2025-03-21 19:04:36,434 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.bias
2025-03-21 19:04:36,434 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.weight
2025-03-21 19:04:36,435 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.bias
2025-03-21 19:04:36,435 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.weight
2025-03-21 19:04:36,436 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.bias
2025-03-21 19:04:36,436 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_weight
2025-03-21 19:04:36,437 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_bias
2025-03-21 19:04:36,438 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.weight
2025-03-21 19:04:36,438 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.bias
2025-03-21 19:04:36,439 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.weight
2025-03-21 19:04:36,439 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.bias
2025-03-21 19:04:36,440 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.weight
2025-03-21 19:04:36,440 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.bias
2025-03-21 19:04:36,441 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.weight
2025-03-21 19:04:36,441 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.bias
2025-03-21 19:04:36,442 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.weight
2025-03-21 19:04:36,442 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.bias
2025-03-21 19:04:36,443 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_weight
2025-03-21 19:04:36,444 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_bias
2025-03-21 19:04:36,444 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.weight
2025-03-21 19:04:36,445 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.bias
2025-03-21 19:04:36,445 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.weight
2025-03-21 19:04:36,446 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.bias
2025-03-21 19:04:36,447 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.weight
2025-03-21 19:04:36,447 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.bias
2025-03-21 19:04:36,448 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.weight
2025-03-21 19:04:36,448 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.bias
2025-03-21 19:04:36,449 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.weight
2025-03-21 19:04:36,449 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.bias
2025-03-21 19:04:36,450 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_weight
2025-03-21 19:04:36,450 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_bias
2025-03-21 19:04:36,451 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.weight
2025-03-21 19:04:36,452 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.bias
2025-03-21 19:04:36,452 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.weight
2025-03-21 19:04:36,453 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.bias
2025-03-21 19:04:36,453 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.weight
2025-03-21 19:04:36,454 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.bias
2025-03-21 19:04:36,454 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.weight
2025-03-21 19:04:36,455 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.bias
2025-03-21 19:04:36,455 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.weight
2025-03-21 19:04:36,456 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.bias
2025-03-21 19:04:36,457 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_weight
2025-03-21 19:04:36,457 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_bias
2025-03-21 19:04:36,458 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.weight
2025-03-21 19:04:36,458 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.bias
2025-03-21 19:04:36,459 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.weight
2025-03-21 19:04:36,459 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.bias
2025-03-21 19:04:36,460 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.weight
2025-03-21 19:04:36,461 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.bias
2025-03-21 19:04:36,461 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.weight
2025-03-21 19:04:36,462 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.bias
2025-03-21 19:04:36,462 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.weight
2025-03-21 19:04:36,463 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.bias
2025-03-21 19:04:36,463 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_weight
2025-03-21 19:04:36,464 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_bias
2025-03-21 19:04:36,464 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.weight
2025-03-21 19:04:36,465 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.bias
2025-03-21 19:04:36,466 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.weight
2025-03-21 19:04:36,466 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.bias
2025-03-21 19:04:36,467 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.weight
2025-03-21 19:04:36,468 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.bias
2025-03-21 19:04:36,468 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.weight
2025-03-21 19:04:36,469 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.bias
2025-03-21 19:04:36,469 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.weight
2025-03-21 19:04:36,470 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.bias
2025-03-21 19:04:36,470 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_weight
2025-03-21 19:04:36,471 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_bias
2025-03-21 19:04:36,472 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.weight
2025-03-21 19:04:36,472 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.bias
2025-03-21 19:04:36,473 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.weight
2025-03-21 19:04:36,473 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.bias
2025-03-21 19:04:36,474 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.weight
2025-03-21 19:04:36,475 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.bias
2025-03-21 19:04:36,475 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.weight
2025-03-21 19:04:36,476 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.bias
2025-03-21 19:04:36,476 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.weight
2025-03-21 19:04:36,477 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.bias
2025-03-21 19:04:36,478 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_weight
2025-03-21 19:04:36,478 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_bias
2025-03-21 19:04:36,479 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.weight
2025-03-21 19:04:36,479 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.bias
2025-03-21 19:04:36,480 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.weight
2025-03-21 19:04:36,480 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.bias
2025-03-21 19:04:36,481 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.weight
2025-03-21 19:04:36,482 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.bias
2025-03-21 19:04:36,482 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.weight
2025-03-21 19:04:36,483 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.bias
2025-03-21 19:04:36,483 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.weight
2025-03-21 19:04:36,484 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.bias
2025-03-21 19:04:36,485 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_weight
2025-03-21 19:04:36,485 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_bias
2025-03-21 19:04:36,486 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.weight
2025-03-21 19:04:36,486 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.bias
2025-03-21 19:04:36,487 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.weight
2025-03-21 19:04:36,488 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.bias
2025-03-21 19:04:36,488 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.weight
2025-03-21 19:04:36,489 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.bias
2025-03-21 19:04:36,489 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.weight
2025-03-21 19:04:36,490 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.bias
2025-03-21 19:04:36,491 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.weight
2025-03-21 19:04:36,491 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.bias
2025-03-21 19:04:36,492 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_weight
2025-03-21 19:04:36,492 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_bias
2025-03-21 19:04:36,493 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.weight
2025-03-21 19:04:36,493 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.bias
2025-03-21 19:04:36,494 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.weight
2025-03-21 19:04:36,495 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.bias
2025-03-21 19:04:36,495 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.weight
2025-03-21 19:04:36,496 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.bias
2025-03-21 19:04:36,496 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.weight
2025-03-21 19:04:36,497 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.bias
2025-03-21 19:04:36,497 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.weight
2025-03-21 19:04:36,498 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.bias
2025-03-21 19:04:36,498 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.weight
2025-03-21 19:04:36,499 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.bias
2025-03-21 19:04:36,500 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: custom_src_module.emb.Embedding.weight
2025-03-21 19:04:36,500 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.weight
2025-03-21 19:04:36,501 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.bias
2025-03-21 19:04:36,501 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.weight
2025-03-21 19:04:36,502 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.bias
2025-03-21 19:04:36,502 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.weight
2025-03-21 19:04:36,503 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.bias
2025-03-21 19:04:36,504 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: embedding.Embedding.weight
2025-03-21 19:04:36,504 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l0
2025-03-21 19:04:36,505 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l0
2025-03-21 19:04:36,505 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l0
2025-03-21 19:04:36,506 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l0
2025-03-21 19:04:36,507 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l1
2025-03-21 19:04:36,507 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l1
2025-03-21 19:04:36,508 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l1
2025-03-21 19:04:36,509 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l1
2025-03-21 19:04:36,509 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.weight
2025-03-21 19:04:36,510 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.bias
2025-03-21 19:04:36,510 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.weight
2025-03-21 19:04:36,511 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.bias
2025-03-21 19:04:36,512 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.weight
2025-03-21 19:04:36,512 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.bias
2025-03-21 19:04:36,518 - speechbrain.core - INFO - Info: precision arg from hparam file is used
2025-03-21 19:04:36,518 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2025-03-21 19:04:36,519 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2025-03-21 19:04:36,695 - speechbrain.core - INFO - Gradscaler enabled: True. Using precision: fp16.
2025-03-21 19:04:36,697 - speechbrain.core - INFO - ASR Model Statistics:
* Total Number of Trainable Parameters: 13.3M
* Total Number of Parameters: 13.3M
* Trainable Parameters represent 100.0000% of the total size.
2025-03-21 19:04:36,700 - speechbrain.utils.checkpoints - INFO - Would load a checkpoint here, but none found yet.
2025-03-21 19:04:36,701 - speechbrain.utils.epoch_loop - INFO - Going into epoch 1
2025-03-21 19:05:41,328 - speechbrain.utils.train_logger - INFO - epoch: 1, lr: 8.16e-06, steps: 103, optimizer: Adam - train loss: 2.57e+02 - valid loss: 1.63e+02, valid ACC: 1.86e-01
2025-03-21 19:05:41,633 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-05-41+00
2025-03-21 19:05:41,641 - speechbrain.utils.epoch_loop - INFO - Going into epoch 2
2025-03-21 19:06:43,791 - speechbrain.utils.train_logger - INFO - epoch: 2, lr: 1.64e-05, steps: 206, optimizer: Adam - train loss: 1.26e+02 - valid loss: 1.14e+02, valid ACC: 1.86e-01
2025-03-21 19:06:44,121 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-06-43+00
2025-03-21 19:06:44,138 - speechbrain.utils.epoch_loop - INFO - Going into epoch 3
2025-03-21 19:07:46,052 - speechbrain.utils.train_logger - INFO - epoch: 3, lr: 2.46e-05, steps: 309, optimizer: Adam - train loss: 99.56 - valid loss: 94.29, valid ACC: 1.86e-01
2025-03-21 19:07:46,307 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-07-46+00
2025-03-21 19:07:46,330 - speechbrain.utils.epoch_loop - INFO - Going into epoch 4
2025-03-21 19:08:48,332 - speechbrain.utils.train_logger - INFO - epoch: 4, lr: 3.29e-05, steps: 412, optimizer: Adam - train loss: 77.32 - valid loss: 70.17, valid ACC: 1.86e-01
2025-03-21 19:08:48,567 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-08-48+00
2025-03-21 19:08:48,601 - speechbrain.utils.epoch_loop - INFO - Going into epoch 5
2025-03-21 19:09:50,676 - speechbrain.utils.train_logger - INFO - epoch: 5, lr: 4.11e-05, steps: 515, optimizer: Adam - train loss: 54.57 - valid loss: 48.06, valid ACC: 1.89e-01
2025-03-21 19:09:50,982 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-09-50+00
2025-03-21 19:09:51,030 - speechbrain.utils.epoch_loop - INFO - Going into epoch 6
2025-03-21 19:10:52,380 - speechbrain.utils.train_logger - INFO - epoch: 6, lr: 4.94e-05, steps: 618, optimizer: Adam - train loss: 37.79 - valid loss: 35.07, valid ACC: 2.03e-01
2025-03-21 19:10:52,613 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-10-52+00
2025-03-21 19:10:52,681 - speechbrain.utils.epoch_loop - INFO - Going into epoch 7
2025-03-21 19:11:54,374 - speechbrain.utils.train_logger - INFO - epoch: 7, lr: 5.76e-05, steps: 721, optimizer: Adam - train loss: 29.93 - valid loss: 30.00, valid ACC: 2.67e-01
2025-03-21 19:11:54,716 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-11-54+00
2025-03-21 19:11:54,796 - speechbrain.utils.epoch_loop - INFO - Going into epoch 8
2025-03-21 19:12:57,207 - speechbrain.utils.train_logger - INFO - epoch: 8, lr: 6.58e-05, steps: 824, optimizer: Adam - train loss: 27.14 - valid loss: 28.03, valid ACC: 3.11e-01
2025-03-21 19:12:57,486 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-12-57+00
2025-03-21 19:12:57,590 - speechbrain.utils.epoch_loop - INFO - Going into epoch 9
2025-03-21 19:13:58,259 - speechbrain.utils.train_logger - INFO - epoch: 9, lr: 7.41e-05, steps: 927, optimizer: Adam - train loss: 25.83 - valid loss: 26.67, valid ACC: 3.41e-01
2025-03-21 19:13:58,620 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-13-58+00
2025-03-21 19:13:58,744 - speechbrain.utils.epoch_loop - INFO - Going into epoch 10
2025-03-21 19:15:44,428 - speechbrain.utils.train_logger - INFO - epoch: 10, lr: 8.23e-05, steps: 1030, optimizer: Adam - train loss: 24.69 - valid loss: 25.33, valid ACC: 3.97e-01, valid WER: 99.73
2025-03-21 19:15:44,722 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-15-44+00
2025-03-21 19:15:44,874 - speechbrain.utils.epoch_loop - INFO - Going into epoch 11
2025-03-21 19:16:46,258 - speechbrain.utils.train_logger - INFO - epoch: 11, lr: 9.06e-05, steps: 1133, optimizer: Adam - train loss: 23.63 - valid loss: 24.22, valid ACC: 4.54e-01
2025-03-21 19:16:46,546 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-16-46+00
2025-03-21 19:16:46,723 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-05-41+00
2025-03-21 19:16:46,724 - speechbrain.utils.epoch_loop - INFO - Going into epoch 12
2025-03-21 19:17:48,479 - speechbrain.utils.train_logger - INFO - epoch: 12, lr: 9.88e-05, steps: 1236, optimizer: Adam - train loss: 22.62 - valid loss: 23.08, valid ACC: 5.01e-01
2025-03-21 19:17:48,920 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-17-48+00
2025-03-21 19:17:49,118 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-06-43+00
2025-03-21 19:17:49,119 - speechbrain.utils.epoch_loop - INFO - Going into epoch 13
2025-03-21 19:18:50,249 - speechbrain.utils.train_logger - INFO - epoch: 13, lr: 1.07e-04, steps: 1339, optimizer: Adam - train loss: 21.66 - valid loss: 22.01, valid ACC: 5.57e-01
2025-03-21 19:18:50,562 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-18-50+00
2025-03-21 19:18:50,769 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-07-46+00
2025-03-21 19:18:50,769 - speechbrain.utils.epoch_loop - INFO - Going into epoch 14
2025-03-21 19:19:51,796 - speechbrain.utils.train_logger - INFO - epoch: 14, lr: 1.15e-04, steps: 1442, optimizer: Adam - train loss: 20.81 - valid loss: 21.00, valid ACC: 6.01e-01
2025-03-21 19:19:52,258 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-19-51+00
2025-03-21 19:19:52,494 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-08-48+00
2025-03-21 19:19:52,494 - speechbrain.utils.epoch_loop - INFO - Going into epoch 15
2025-03-21 19:20:53,036 - speechbrain.utils.train_logger - INFO - epoch: 15, lr: 1.24e-04, steps: 1545, optimizer: Adam - train loss: 20.01 - valid loss: 20.42, valid ACC: 6.40e-01
2025-03-21 19:20:53,434 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-20-53+00
2025-03-21 19:20:53,679 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-09-50+00
2025-03-21 19:20:53,679 - speechbrain.utils.epoch_loop - INFO - Going into epoch 16
2025-03-21 19:21:55,461 - speechbrain.utils.train_logger - INFO - epoch: 16, lr: 1.32e-04, steps: 1648, optimizer: Adam - train loss: 19.29 - valid loss: 19.43, valid ACC: 6.75e-01
2025-03-21 19:21:55,754 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-21-55+00
2025-03-21 19:21:56,016 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-10-52+00
2025-03-21 19:21:56,017 - speechbrain.utils.epoch_loop - INFO - Going into epoch 17
2025-03-21 19:22:56,780 - speechbrain.utils.train_logger - INFO - epoch: 17, lr: 1.40e-04, steps: 1751, optimizer: Adam - train loss: 18.61 - valid loss: 18.62, valid ACC: 6.96e-01
2025-03-21 19:22:57,073 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-22-56+00
2025-03-21 19:22:57,345 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-11-54+00
2025-03-21 19:22:57,345 - speechbrain.utils.epoch_loop - INFO - Going into epoch 18
2025-03-21 19:23:57,654 - speechbrain.utils.train_logger - INFO - epoch: 18, lr: 1.48e-04, steps: 1854, optimizer: Adam - train loss: 17.84 - valid loss: 18.06, valid ACC: 7.12e-01
2025-03-21 19:23:57,940 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-23-57+00
2025-03-21 19:23:58,219 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-12-57+00
2025-03-21 19:23:58,220 - speechbrain.utils.epoch_loop - INFO - Going into epoch 19
2025-03-21 19:24:58,779 - speechbrain.utils.train_logger - INFO - epoch: 19, lr: 1.56e-04, steps: 1957, optimizer: Adam - train loss: 17.27 - valid loss: 17.64, valid ACC: 7.26e-01
2025-03-21 19:24:59,066 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-24-58+00
2025-03-21 19:24:59,353 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-13-58+00
2025-03-21 19:24:59,353 - speechbrain.utils.epoch_loop - INFO - Going into epoch 20
2025-03-21 19:26:51,320 - speechbrain.utils.train_logger - INFO - epoch: 20, lr: 1.65e-04, steps: 2060, optimizer: Adam - train loss: 16.81 - valid loss: 17.32, valid ACC: 7.37e-01, valid WER: 1.06e+02
2025-03-21 19:26:51,652 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-26-51+00
2025-03-21 19:26:51,969 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-15-44+00
2025-03-21 19:26:51,969 - speechbrain.utils.epoch_loop - INFO - Going into epoch 21
2025-03-21 19:27:54,475 - speechbrain.utils.train_logger - INFO - epoch: 21, lr: 1.73e-04, steps: 2163, optimizer: Adam - train loss: 16.39 - valid loss: 17.08, valid ACC: 7.40e-01
2025-03-21 19:27:54,868 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-27-54+00
2025-03-21 19:27:55,207 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-16-46+00
2025-03-21 19:27:55,208 - speechbrain.utils.epoch_loop - INFO - Going into epoch 22
2025-03-21 19:28:56,697 - speechbrain.utils.train_logger - INFO - epoch: 22, lr: 1.81e-04, steps: 2266, optimizer: Adam - train loss: 16.02 - valid loss: 16.85, valid ACC: 7.49e-01
2025-03-21 19:28:56,929 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-28-56+00
2025-03-21 19:28:57,294 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-17-48+00
2025-03-21 19:28:57,295 - speechbrain.utils.epoch_loop - INFO - Going into epoch 23
2025-03-21 19:29:59,046 - speechbrain.utils.train_logger - INFO - epoch: 23, lr: 1.89e-04, steps: 2369, optimizer: Adam - train loss: 15.71 - valid loss: 16.69, valid ACC: 7.50e-01
2025-03-21 19:29:59,370 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-29-59+00
2025-03-21 19:29:59,744 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-18-50+00
2025-03-21 19:29:59,745 - speechbrain.utils.epoch_loop - INFO - Going into epoch 24
2025-03-21 19:31:01,374 - speechbrain.utils.train_logger - INFO - epoch: 24, lr: 1.98e-04, steps: 2472, optimizer: Adam - train loss: 15.42 - valid loss: 16.63, valid ACC: 7.54e-01
2025-03-21 19:31:01,929 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-31-01+00
2025-03-21 19:31:02,541 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-19-51+00
2025-03-21 19:31:02,542 - speechbrain.utils.epoch_loop - INFO - Going into epoch 25
2025-03-21 19:32:05,034 - speechbrain.utils.train_logger - INFO - epoch: 25, lr: 2.06e-04, steps: 2575, optimizer: Adam - train loss: 15.18 - valid loss: 16.22, valid ACC: 7.56e-01
2025-03-21 19:32:05,442 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-32-05+00
2025-03-21 19:32:05,857 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-20-53+00
2025-03-21 19:32:05,858 - speechbrain.utils.epoch_loop - INFO - Going into epoch 26
2025-03-21 19:33:07,055 - speechbrain.utils.train_logger - INFO - epoch: 26, lr: 2.14e-04, steps: 2678, optimizer: Adam - train loss: 14.96 - valid loss: 16.06, valid ACC: 7.56e-01
2025-03-21 19:33:07,487 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-33-07+00
2025-03-21 19:33:07,934 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-21-55+00
2025-03-21 19:33:07,935 - speechbrain.utils.epoch_loop - INFO - Going into epoch 27
2025-03-21 19:34:10,228 - speechbrain.utils.train_logger - INFO - epoch: 27, lr: 2.22e-04, steps: 2781, optimizer: Adam - train loss: 14.72 - valid loss: 15.98, valid ACC: 7.56e-01
2025-03-21 19:34:10,583 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-34-10+00
2025-03-21 19:34:11,059 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-22-56+00
2025-03-21 19:34:11,059 - speechbrain.utils.epoch_loop - INFO - Going into epoch 28
2025-03-21 19:35:12,802 - speechbrain.utils.train_logger - INFO - epoch: 28, lr: 2.31e-04, steps: 2884, optimizer: Adam - train loss: 14.52 - valid loss: 15.62, valid ACC: 7.61e-01
2025-03-21 19:35:13,096 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-35-12+00
2025-03-21 19:35:13,565 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-23-57+00
2025-03-21 19:35:13,566 - speechbrain.utils.epoch_loop - INFO - Going into epoch 29
2025-03-21 19:36:14,732 - speechbrain.utils.train_logger - INFO - epoch: 29, lr: 2.39e-04, steps: 2987, optimizer: Adam - train loss: 14.31 - valid loss: 15.47, valid ACC: 7.61e-01
2025-03-21 19:36:15,127 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-36-14+00
2025-03-21 19:36:15,604 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-24-58+00
2025-03-21 19:36:15,605 - speechbrain.utils.epoch_loop - INFO - Going into epoch 30
2025-03-21 19:38:32,310 - speechbrain.utils.train_logger - INFO - epoch: 30, lr: 2.47e-04, steps: 3090, optimizer: Adam - train loss: 14.07 - valid loss: 15.28, valid ACC: 7.63e-01, valid WER: 1.16e+02
2025-03-21 19:38:32,659 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-38-32+00
2025-03-21 19:38:33,147 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-26-51+00
2025-03-21 19:38:33,148 - speechbrain.utils.epoch_loop - INFO - Going into epoch 31
2025-03-21 19:39:34,749 - speechbrain.utils.train_logger - INFO - epoch: 31, lr: 2.55e-04, steps: 3193, optimizer: Adam - train loss: 13.88 - valid loss: 14.85, valid ACC: 7.68e-01
2025-03-21 19:39:35,115 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-39-34+00
2025-03-21 19:39:35,614 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-27-54+00
2025-03-21 19:39:35,614 - speechbrain.utils.epoch_loop - INFO - Going into epoch 32
2025-03-21 19:40:37,312 - speechbrain.utils.train_logger - INFO - epoch: 32, lr: 2.64e-04, steps: 3296, optimizer: Adam - train loss: 13.66 - valid loss: 14.91, valid ACC: 7.67e-01
2025-03-21 19:40:37,612 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-40-37+00
2025-03-21 19:40:38,131 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-28-56+00
2025-03-21 19:40:38,132 - speechbrain.utils.epoch_loop - INFO - Going into epoch 33
2025-03-21 19:41:39,802 - speechbrain.utils.train_logger - INFO - epoch: 33, lr: 2.72e-04, steps: 3399, optimizer: Adam - train loss: 13.50 - valid loss: 15.05, valid ACC: 7.63e-01
2025-03-21 19:41:40,261 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-41-39+00
2025-03-21 19:41:40,799 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-29-59+00
2025-03-21 19:41:40,800 - speechbrain.utils.epoch_loop - INFO - Going into epoch 34
2025-03-21 19:42:41,982 - speechbrain.utils.train_logger - INFO - epoch: 34, lr: 2.80e-04, steps: 3502, optimizer: Adam - train loss: 13.28 - valid loss: 14.26, valid ACC: 7.73e-01
2025-03-21 19:42:42,377 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-42-41+00
2025-03-21 19:42:42,928 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-31-01+00
2025-03-21 19:42:42,928 - speechbrain.utils.epoch_loop - INFO - Going into epoch 35
2025-03-21 19:43:44,085 - speechbrain.utils.train_logger - INFO - epoch: 35, lr: 2.88e-04, steps: 3605, optimizer: Adam - train loss: 13.05 - valid loss: 14.38, valid ACC: 7.78e-01
2025-03-21 19:43:44,457 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-43-44+00
2025-03-21 19:43:45,108 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-32-05+00
2025-03-21 19:43:45,109 - speechbrain.utils.epoch_loop - INFO - Going into epoch 36
2025-03-21 19:44:47,173 - speechbrain.utils.train_logger - INFO - epoch: 36, lr: 2.97e-04, steps: 3708, optimizer: Adam - train loss: 12.81 - valid loss: 13.96, valid ACC: 7.80e-01
2025-03-21 19:44:47,604 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-44-47+00
2025-03-21 19:44:48,181 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-33-07+00
2025-03-21 19:44:48,182 - speechbrain.utils.epoch_loop - INFO - Going into epoch 37
2025-03-21 19:45:49,177 - speechbrain.utils.train_logger - INFO - epoch: 37, lr: 3.05e-04, steps: 3811, optimizer: Adam - train loss: 12.56 - valid loss: 13.62, valid ACC: 7.82e-01
2025-03-21 19:45:49,485 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-45-49+00
2025-03-21 19:45:50,066 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-34-10+00
2025-03-21 19:45:50,067 - speechbrain.utils.epoch_loop - INFO - Going into epoch 38
2025-03-21 19:46:51,937 - speechbrain.utils.train_logger - INFO - epoch: 38, lr: 3.13e-04, steps: 3914, optimizer: Adam - train loss: 12.31 - valid loss: 13.37, valid ACC: 7.82e-01
2025-03-21 19:46:52,264 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-46-51+00
2025-03-21 19:46:52,863 - speechbrain.utils.epoch_loop - INFO - Going into epoch 39
2025-03-21 19:47:54,378 - speechbrain.utils.train_logger - INFO - epoch: 39, lr: 3.21e-04, steps: 4017, optimizer: Adam - train loss: 12.09 - valid loss: 13.09, valid ACC: 7.87e-01
2025-03-21 19:47:54,694 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-47-54+00
2025-03-21 19:47:55,361 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-35-12+00
2025-03-21 19:47:55,362 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-36-14+00
2025-03-21 19:47:55,363 - speechbrain.utils.epoch_loop - INFO - Going into epoch 40
2025-03-21 19:50:12,235 - speechbrain.utils.train_logger - INFO - epoch: 40, lr: 3.30e-04, steps: 4120, optimizer: Adam - train loss: 11.87 - valid loss: 13.07, valid ACC: 7.83e-01, valid WER: 1.05e+02
2025-03-21 19:50:12,566 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-50-12+00
2025-03-21 19:50:13,209 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-38-32+00
2025-03-21 19:50:13,209 - speechbrain.utils.epoch_loop - INFO - Going into epoch 41
2025-03-21 19:51:14,208 - speechbrain.utils.train_logger - INFO - epoch: 41, lr: 3.38e-04, steps: 4223, optimizer: Adam - train loss: 11.61 - valid loss: 12.84, valid ACC: 7.90e-01
2025-03-21 19:51:14,490 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-51-14+00
2025-03-21 19:51:15,170 - speechbrain.utils.epoch_loop - INFO - Going into epoch 42
2025-03-21 19:52:16,915 - speechbrain.utils.train_logger - INFO - epoch: 42, lr: 3.46e-04, steps: 4326, optimizer: Adam - train loss: 11.34 - valid loss: 12.78, valid ACC: 7.92e-01
2025-03-21 19:52:17,237 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-52-16+00
2025-03-21 19:52:18,010 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-40-37+00
2025-03-21 19:52:18,011 - speechbrain.utils.epoch_loop - INFO - Going into epoch 43
2025-03-21 19:53:19,888 - speechbrain.utils.train_logger - INFO - epoch: 43, lr: 3.54e-04, steps: 4429, optimizer: Adam - train loss: 11.14 - valid loss: 12.39, valid ACC: 7.98e-01
2025-03-21 19:53:20,597 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-53-19+00
2025-03-21 19:53:21,394 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-41-39+00
2025-03-21 19:53:21,396 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-39-34+00
2025-03-21 19:53:21,396 - speechbrain.utils.epoch_loop - INFO - Going into epoch 44
2025-03-21 19:54:22,418 - speechbrain.utils.train_logger - INFO - epoch: 44, lr: 3.62e-04, steps: 4532, optimizer: Adam - train loss: 10.84 - valid loss: 11.80, valid ACC: 8.06e-01
2025-03-21 19:54:22,789 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-54-22+00
2025-03-21 19:54:23,532 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-42-41+00
2025-03-21 19:54:23,533 - speechbrain.utils.epoch_loop - INFO - Going into epoch 45
2025-03-21 19:55:24,946 - speechbrain.utils.train_logger - INFO - epoch: 45, lr: 3.71e-04, steps: 4635, optimizer: Adam - train loss: 10.60 - valid loss: 12.54, valid ACC: 7.94e-01
2025-03-21 19:55:25,309 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-55-24+00
2025-03-21 19:55:26,049 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-43-44+00
2025-03-21 19:55:26,050 - speechbrain.utils.epoch_loop - INFO - Going into epoch 46
2025-03-21 19:56:27,402 - speechbrain.utils.train_logger - INFO - epoch: 46, lr: 3.79e-04, steps: 4738, optimizer: Adam - train loss: 10.38 - valid loss: 11.48, valid ACC: 8.12e-01
2025-03-21 19:56:27,719 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-56-27+00
2025-03-21 19:56:28,512 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-44-47+00
2025-03-21 19:56:28,512 - speechbrain.utils.epoch_loop - INFO - Going into epoch 47
2025-03-21 19:57:30,090 - speechbrain.utils.train_logger - INFO - epoch: 47, lr: 3.87e-04, steps: 4841, optimizer: Adam - train loss: 10.06 - valid loss: 11.77, valid ACC: 8.04e-01
2025-03-21 19:57:30,466 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-57-30+00
2025-03-21 19:57:31,245 - speechbrain.utils.epoch_loop - INFO - Going into epoch 48
2025-03-21 19:58:34,101 - speechbrain.utils.train_logger - INFO - epoch: 48, lr: 3.95e-04, steps: 4944, optimizer: Adam - train loss: 9.79 - valid loss: 11.18, valid ACC: 8.11e-01
2025-03-21 19:58:34,514 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-58-34+00
2025-03-21 19:58:35,418 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-45-49+00
2025-03-21 19:58:35,419 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-46-51+00
2025-03-21 19:58:35,420 - speechbrain.utils.epoch_loop - INFO - Going into epoch 49
2025-03-21 19:59:37,290 - speechbrain.utils.train_logger - INFO - epoch: 49, lr: 4.04e-04, steps: 5047, optimizer: Adam - train loss: 9.55 - valid loss: 10.79, valid ACC: 8.16e-01
2025-03-21 19:59:37,576 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-59-37+00
2025-03-21 19:59:38,428 - speechbrain.utils.epoch_loop - INFO - Going into epoch 50
2025-03-21 20:01:56,654 - speechbrain.utils.train_logger - INFO - epoch: 50, lr: 4.12e-04, steps: 5150, optimizer: Adam - train loss: 9.23 - valid loss: 10.84, valid ACC: 8.11e-01, valid WER: 86.13
2025-03-21 20:01:57,065 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-01-56+00
2025-03-21 20:01:57,907 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-50-12+00
2025-03-21 20:01:57,909 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-47-54+00
2025-03-21 20:01:57,909 - speechbrain.utils.epoch_loop - INFO - Going into epoch 51
2025-03-21 20:02:59,405 - speechbrain.utils.train_logger - INFO - epoch: 51, lr: 4.20e-04, steps: 5253, optimizer: Adam - train loss: 8.97 - valid loss: 10.86, valid ACC: 8.18e-01
2025-03-21 20:02:59,957 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-02-59+00
2025-03-21 20:03:00,856 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-51-14+00
2025-03-21 20:03:00,856 - speechbrain.utils.epoch_loop - INFO - Going into epoch 52
2025-03-21 20:04:02,407 - speechbrain.utils.train_logger - INFO - epoch: 52, lr: 4.28e-04, steps: 5356, optimizer: Adam - train loss: 8.59 - valid loss: 10.40, valid ACC: 8.19e-01
2025-03-21 20:04:02,864 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-04-02+00
2025-03-21 20:04:03,766 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-52-16+00
2025-03-21 20:04:03,767 - speechbrain.utils.epoch_loop - INFO - Going into epoch 53
2025-03-21 20:05:05,446 - speechbrain.utils.train_logger - INFO - epoch: 53, lr: 4.37e-04, steps: 5459, optimizer: Adam - train loss: 8.30 - valid loss: 10.10, valid ACC: 8.28e-01
2025-03-21 20:05:05,827 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-05-05+00
2025-03-21 20:05:06,717 - speechbrain.utils.epoch_loop - INFO - Going into epoch 54
2025-03-21 20:06:08,823 - speechbrain.utils.train_logger - INFO - epoch: 54, lr: 4.45e-04, steps: 5562, optimizer: Adam - train loss: 8.08 - valid loss: 9.85, valid ACC: 8.29e-01
2025-03-21 20:06:09,288 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-06-08+00
2025-03-21 20:06:10,258 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-53-19+00
2025-03-21 20:06:10,259 - speechbrain.utils.epoch_loop - INFO - Going into epoch 55
2025-03-21 20:07:11,961 - speechbrain.utils.train_logger - INFO - epoch: 55, lr: 4.53e-04, steps: 5665, optimizer: Adam - train loss: 7.74 - valid loss: 9.37, valid ACC: 8.36e-01
2025-03-21 20:07:12,315 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-07-11+00
2025-03-21 20:07:13,333 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-55-24+00
2025-03-21 20:07:13,333 - speechbrain.utils.epoch_loop - INFO - Going into epoch 56
2025-03-21 20:08:14,900 - speechbrain.utils.train_logger - INFO - epoch: 56, lr: 4.61e-04, steps: 5768, optimizer: Adam - train loss: 7.50 - valid loss: 9.14, valid ACC: 8.43e-01
2025-03-21 20:08:15,418 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-08-14+00
2025-03-21 20:08:16,450 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-54-22+00
2025-03-21 20:08:16,450 - speechbrain.utils.epoch_loop - INFO - Going into epoch 57
2025-03-21 20:09:17,086 - speechbrain.utils.train_logger - INFO - epoch: 57, lr: 4.70e-04, steps: 5871, optimizer: Adam - train loss: 7.22 - valid loss: 9.46, valid ACC: 8.37e-01
2025-03-21 20:09:17,481 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-09-17+00
2025-03-21 20:09:18,512 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-57-30+00
2025-03-21 20:09:18,512 - speechbrain.utils.epoch_loop - INFO - Going into epoch 58
2025-03-21 20:10:20,472 - speechbrain.utils.train_logger - INFO - epoch: 58, lr: 4.78e-04, steps: 5974, optimizer: Adam - train loss: 6.99 - valid loss: 8.83, valid ACC: 8.42e-01
2025-03-21 20:10:20,858 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-10-20+00
2025-03-21 20:10:21,868 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-58-34+00
2025-03-21 20:10:21,868 - speechbrain.utils.epoch_loop - INFO - Going into epoch 59
2025-03-21 20:11:23,193 - speechbrain.utils.train_logger - INFO - epoch: 59, lr: 4.86e-04, steps: 6077, optimizer: Adam - train loss: 6.82 - valid loss: 8.70, valid ACC: 8.43e-01
2025-03-21 20:11:23,502 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-11-23+00
2025-03-21 20:11:24,594 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-56-27+00
2025-03-21 20:11:24,594 - speechbrain.utils.epoch_loop - INFO - Going into epoch 60
2025-03-21 20:28:05,312 - speechbrain.utils.quirks - INFO - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
2025-03-21 20:28:05,313 - speechbrain.utils.quirks - INFO - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
2025-03-21 20:28:05,313 - speechbrain.core - INFO - Beginning experiment!
2025-03-21 20:28:05,313 - speechbrain.core - INFO - Experiment folder: results_lm_060/results_fold_3/conformer_small/7775
2025-03-21 20:28:08,617 - speechbrain.utils.superpowers - DEBUG - aiohappyeyeballs==2.6.1
aiohttp==3.11.13
aiosignal==1.3.2
alembic==1.15.1
anyio==4.9.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==3.0.0
async-lru==2.0.5
async-timeout==5.0.1
attrs==25.3.0
babel==2.17.0
banal==1.0.6
beautifulsoup4==4.13.3
black==24.3.0
bleach==6.2.0
certifi==2025.1.31
cffi==1.17.1
cfgv==3.4.0
charset-normalizer==3.4.1
click==8.1.7
comm==0.2.2
contourpy==1.3.0
cycler==0.12.1
dataset==1.6.2
datasets==3.3.2
debugpy==1.8.13
decorator==5.2.1
defusedxml==0.7.1
dill==0.3.8
distlib==0.3.9
docstring_parser_fork==0.0.12
exceptiongroup==1.2.2
executing==2.2.0
fastjsonschema==2.21.1
filelock==3.17.0
flake8==7.0.0
fonttools==4.56.0
fqdn==1.5.1
frozenlist==1.5.0
fsspec==2024.12.0
greenlet==3.1.1
h11==0.14.0
httpcore==1.0.7
httpx==0.28.1
huggingface-hub==0.29.3
HyperPyYAML==1.2.2
identify==2.6.9
idna==3.10
importlib_metadata==8.6.1
importlib_resources==6.5.2
iniconfig==2.0.0
ipykernel==6.29.5
ipython==8.18.1
isoduration==20.11.0
isort==5.13.2
jedi==0.19.2
Jinja2==3.1.6
joblib==1.4.2
json5==0.10.0
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
jupyter-events==0.12.0
jupyter-lsp==2.2.5
jupyter_client==8.6.3
jupyter_core==5.7.2
jupyter_server==2.15.0
jupyter_server_terminals==0.5.3
jupyterlab==4.3.6
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
kiwisolver==1.4.7
Mako==1.3.9
MarkupSafe==3.0.2
matplotlib==3.9.4
matplotlib-inline==0.1.7
mccabe==0.7.0
mistune==3.1.2
mpmath==1.3.0
multidict==6.1.0
multiprocess==0.70.16
mypy-extensions==1.0.0
nbclient==0.10.2
nbconvert==7.16.6
nbformat==5.10.4
nest-asyncio==1.6.0
networkx==3.2.1
nodeenv==1.9.1
notebook_shim==0.2.4
numpy==1.26.4
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-cusparselt-cu12==0.6.2
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.4.127
overrides==7.7.0
packaging==24.2
pandas==2.2.3
pandocfilters==1.5.1
parso==0.8.4
pathspec==0.12.1
pexpect==4.9.0
pillow==11.1.0
platformdirs==4.3.6
pluggy==1.5.0
pre_commit==4.1.0
prometheus_client==0.21.1
prompt_toolkit==3.0.50
propcache==0.3.0
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==19.0.1
pycodestyle==2.11.0
pycparser==2.22
pydoclint==0.4.1
pyflakes==3.2.0
Pygments==2.19.1
pygtrie==2.5.0
pyparsing==3.2.1
pytest==7.4.0
python-dateutil==2.9.0.post0
python-json-logger==3.3.0
pytz==2025.1
PyYAML==6.0.2
pyzmq==26.3.0
referencing==0.36.2
regex==2024.11.6
requests==2.32.3
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.23.1
ruamel.yaml==0.18.10
ruamel.yaml.clib==0.2.12
safetensors==0.5.3
scipy==1.12.0
Send2Trash==1.8.3
sentencepiece==0.2.0
six==1.17.0
sniffio==1.3.1
soupsieve==2.6
speechbrain==1.0.2
SQLAlchemy==1.4.54
stack-data==0.6.3
sympy==1.13.1
terminado==0.18.1
tinycss2==1.4.0
tokenizers==0.21.0
tomli==2.2.1
torch==2.6.0
torchaudio==2.6.0
torchvision==0.21.0
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
transformers==4.49.0
triton==3.2.0
types-python-dateutil==2.9.0.20241206
typing_extensions==4.12.2
tzdata==2025.1
uri-template==1.3.0
urllib3==2.3.0
virtualenv==20.29.3
wcwidth==0.2.13
webcolors==24.11.1
webencodings==0.5.1
websocket-client==1.8.0
xxhash==3.5.0
yamllint==1.35.1
yarl==1.18.3
zipp==3.21.0


2025-03-21 20:28:08,654 - speechbrain.utils.superpowers - DEBUG - d8ae0d1


2025-03-21 20:28:08,759 - speechbrain.utils.parameter_transfer - DEBUG - Collecting files (or symlinks) for pretraining in results_lm_060/results_fold_3/conformer_small/7775/save.
2025-03-21 20:28:08,760 - speechbrain.utils.fetching - INFO - Fetch lm.ckpt: Using symlink found at '/scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt'
2025-03-21 20:28:08,760 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["lm"] = /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt
2025-03-21 20:28:08,761 - speechbrain.utils.fetching - INFO - Fetch tokenizer.ckpt: Using symlink found at '/scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/tokenizer.ckpt'
2025-03-21 20:28:08,761 - speechbrain.utils.parameter_transfer - DEBUG - Set local path in self.paths["tokenizer"] = /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/tokenizer.ckpt
2025-03-21 20:28:08,761 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: lm, tokenizer
2025-03-21 20:28:08,762 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): lm -> /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt
2025-03-21 20:28:08,762 - speechbrain.utils.parameter_transfer - DEBUG - Redirecting (loading from local path): tokenizer -> /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/tokenizer.ckpt
2025-03-21 20:28:08,900 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: positional_encoding.pe
2025-03-21 20:28:08,900 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_weight
2025-03-21 20:28:08,901 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.in_proj_bias
2025-03-21 20:28:08,902 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.weight
2025-03-21 20:28:08,902 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.self_att.att.out_proj.bias
2025-03-21 20:28:08,903 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.weight
2025-03-21 20:28:08,904 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.0.bias
2025-03-21 20:28:08,904 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.weight
2025-03-21 20:28:08,905 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.pos_ffn.ffn.3.bias
2025-03-21 20:28:08,905 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.weight
2025-03-21 20:28:08,906 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm1.norm.bias
2025-03-21 20:28:08,907 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.weight
2025-03-21 20:28:08,907 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.0.norm2.norm.bias
2025-03-21 20:28:08,908 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_weight
2025-03-21 20:28:08,908 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.in_proj_bias
2025-03-21 20:28:08,909 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.weight
2025-03-21 20:28:08,910 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.self_att.att.out_proj.bias
2025-03-21 20:28:08,910 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.weight
2025-03-21 20:28:08,911 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.0.bias
2025-03-21 20:28:08,911 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.weight
2025-03-21 20:28:08,912 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.pos_ffn.ffn.3.bias
2025-03-21 20:28:08,912 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.weight
2025-03-21 20:28:08,913 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm1.norm.bias
2025-03-21 20:28:08,914 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.weight
2025-03-21 20:28:08,914 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.1.norm2.norm.bias
2025-03-21 20:28:08,915 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_weight
2025-03-21 20:28:08,915 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.in_proj_bias
2025-03-21 20:28:08,916 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.weight
2025-03-21 20:28:08,916 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.self_att.att.out_proj.bias
2025-03-21 20:28:08,917 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.weight
2025-03-21 20:28:08,917 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.0.bias
2025-03-21 20:28:08,918 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.weight
2025-03-21 20:28:08,919 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.pos_ffn.ffn.3.bias
2025-03-21 20:28:08,919 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.weight
2025-03-21 20:28:08,920 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm1.norm.bias
2025-03-21 20:28:08,920 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.weight
2025-03-21 20:28:08,921 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.2.norm2.norm.bias
2025-03-21 20:28:08,921 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_weight
2025-03-21 20:28:08,922 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.in_proj_bias
2025-03-21 20:28:08,923 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.weight
2025-03-21 20:28:08,923 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.self_att.att.out_proj.bias
2025-03-21 20:28:08,924 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.weight
2025-03-21 20:28:08,924 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.0.bias
2025-03-21 20:28:08,925 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.weight
2025-03-21 20:28:08,926 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.pos_ffn.ffn.3.bias
2025-03-21 20:28:08,926 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.weight
2025-03-21 20:28:08,927 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm1.norm.bias
2025-03-21 20:28:08,927 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.weight
2025-03-21 20:28:08,928 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.3.norm2.norm.bias
2025-03-21 20:28:08,929 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_weight
2025-03-21 20:28:08,929 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.in_proj_bias
2025-03-21 20:28:08,930 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.weight
2025-03-21 20:28:08,930 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.self_att.att.out_proj.bias
2025-03-21 20:28:08,931 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.weight
2025-03-21 20:28:08,931 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.0.bias
2025-03-21 20:28:08,932 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.weight
2025-03-21 20:28:08,932 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.pos_ffn.ffn.3.bias
2025-03-21 20:28:08,933 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.weight
2025-03-21 20:28:08,934 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm1.norm.bias
2025-03-21 20:28:08,934 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.weight
2025-03-21 20:28:08,935 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.4.norm2.norm.bias
2025-03-21 20:28:08,935 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_weight
2025-03-21 20:28:08,936 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.in_proj_bias
2025-03-21 20:28:08,936 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.weight
2025-03-21 20:28:08,937 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.self_att.att.out_proj.bias
2025-03-21 20:28:08,937 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.weight
2025-03-21 20:28:08,938 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.0.bias
2025-03-21 20:28:08,939 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.weight
2025-03-21 20:28:08,939 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.pos_ffn.ffn.3.bias
2025-03-21 20:28:08,940 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.weight
2025-03-21 20:28:08,940 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm1.norm.bias
2025-03-21 20:28:08,941 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.weight
2025-03-21 20:28:08,941 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.5.norm2.norm.bias
2025-03-21 20:28:08,942 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_weight
2025-03-21 20:28:08,942 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.in_proj_bias
2025-03-21 20:28:08,943 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.weight
2025-03-21 20:28:08,944 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.self_att.att.out_proj.bias
2025-03-21 20:28:08,944 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.weight
2025-03-21 20:28:08,945 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.0.bias
2025-03-21 20:28:08,945 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.weight
2025-03-21 20:28:09,058 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.pos_ffn.ffn.3.bias
2025-03-21 20:28:09,059 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.weight
2025-03-21 20:28:09,060 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm1.norm.bias
2025-03-21 20:28:09,060 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.weight
2025-03-21 20:28:09,061 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.6.norm2.norm.bias
2025-03-21 20:28:09,062 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_weight
2025-03-21 20:28:09,062 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.in_proj_bias
2025-03-21 20:28:09,063 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.weight
2025-03-21 20:28:09,063 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.self_att.att.out_proj.bias
2025-03-21 20:28:09,064 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.weight
2025-03-21 20:28:09,065 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.0.bias
2025-03-21 20:28:09,065 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.weight
2025-03-21 20:28:09,066 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.pos_ffn.ffn.3.bias
2025-03-21 20:28:09,067 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.weight
2025-03-21 20:28:09,067 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm1.norm.bias
2025-03-21 20:28:09,068 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.weight
2025-03-21 20:28:09,068 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.7.norm2.norm.bias
2025-03-21 20:28:09,069 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_weight
2025-03-21 20:28:09,070 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.in_proj_bias
2025-03-21 20:28:09,070 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.weight
2025-03-21 20:28:09,071 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.self_att.att.out_proj.bias
2025-03-21 20:28:09,072 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.weight
2025-03-21 20:28:09,072 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.0.bias
2025-03-21 20:28:09,073 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.weight
2025-03-21 20:28:09,073 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.pos_ffn.ffn.3.bias
2025-03-21 20:28:09,074 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.weight
2025-03-21 20:28:09,075 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm1.norm.bias
2025-03-21 20:28:09,075 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.weight
2025-03-21 20:28:09,076 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.8.norm2.norm.bias
2025-03-21 20:28:09,076 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_weight
2025-03-21 20:28:09,077 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.in_proj_bias
2025-03-21 20:28:09,078 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.weight
2025-03-21 20:28:09,078 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.self_att.att.out_proj.bias
2025-03-21 20:28:09,079 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.weight
2025-03-21 20:28:09,079 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.0.bias
2025-03-21 20:28:09,080 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.weight
2025-03-21 20:28:09,081 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.pos_ffn.ffn.3.bias
2025-03-21 20:28:09,081 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.weight
2025-03-21 20:28:09,082 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm1.norm.bias
2025-03-21 20:28:09,082 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.weight
2025-03-21 20:28:09,083 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.9.norm2.norm.bias
2025-03-21 20:28:09,083 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_weight
2025-03-21 20:28:09,084 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.in_proj_bias
2025-03-21 20:28:09,085 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.weight
2025-03-21 20:28:09,085 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.self_att.att.out_proj.bias
2025-03-21 20:28:09,086 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.weight
2025-03-21 20:28:09,086 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.0.bias
2025-03-21 20:28:09,087 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.weight
2025-03-21 20:28:09,088 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.pos_ffn.ffn.3.bias
2025-03-21 20:28:09,088 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.weight
2025-03-21 20:28:09,089 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm1.norm.bias
2025-03-21 20:28:09,089 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.weight
2025-03-21 20:28:09,090 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.10.norm2.norm.bias
2025-03-21 20:28:09,091 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_weight
2025-03-21 20:28:09,091 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.in_proj_bias
2025-03-21 20:28:09,092 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.weight
2025-03-21 20:28:09,092 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.self_att.att.out_proj.bias
2025-03-21 20:28:09,093 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.weight
2025-03-21 20:28:09,094 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.0.bias
2025-03-21 20:28:09,094 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.weight
2025-03-21 20:28:09,095 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.pos_ffn.ffn.3.bias
2025-03-21 20:28:09,095 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.weight
2025-03-21 20:28:09,096 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm1.norm.bias
2025-03-21 20:28:09,097 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.weight
2025-03-21 20:28:09,097 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.layers.11.norm2.norm.bias
2025-03-21 20:28:09,098 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.weight
2025-03-21 20:28:09,099 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: encoder.norm.norm.bias
2025-03-21 20:28:09,099 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: custom_src_module.emb.Embedding.weight
2025-03-21 20:28:09,100 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.weight
2025-03-21 20:28:09,100 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.0.w.bias
2025-03-21 20:28:09,101 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.weight
2025-03-21 20:28:09,102 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.1.norm.bias
2025-03-21 20:28:09,102 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.weight
2025-03-21 20:28:09,103 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the transferred parameters did not have parameters for the key: output_proj.layers.2.w.bias
2025-03-21 20:28:09,103 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: embedding.Embedding.weight
2025-03-21 20:28:09,104 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l0
2025-03-21 20:28:09,105 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l0
2025-03-21 20:28:09,105 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l0
2025-03-21 20:28:09,106 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l0
2025-03-21 20:28:09,106 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_ih_l1
2025-03-21 20:28:09,107 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.weight_hh_l1
2025-03-21 20:28:09,108 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_ih_l1
2025-03-21 20:28:09,108 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: rnn.rnn.bias_hh_l1
2025-03-21 20:28:09,109 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.weight
2025-03-21 20:28:09,109 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.linear.w.bias
2025-03-21 20:28:09,110 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.weight
2025-03-21 20:28:09,110 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: dnn.norm.norm.bias
2025-03-21 20:28:09,111 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.weight
2025-03-21 20:28:09,112 - speechbrain.utils.checkpoints - WARNING - During parameter transfer to TransformerLM(
  (positional_encoding): PositionalEncoding()
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_att): MultiheadAttention(
          (att): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
        )
        (pos_ffn): PositionalwiseFeedForward(
          (ffn): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (norm1): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (norm2): LayerNorm(
          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm(
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    )
  )
  (custom_src_module): NormalizedEmbedding(
    (emb): Embedding(
      (Embedding): Embedding(5000, 768)
    )
  )
  (output_proj): ModuleList(
    (layers): ModuleList(
      (0): Linear(
        (w): Linear(in_features=768, out_features=768, bias=True)
      )
      (1): LayerNorm(
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      )
      (2): Linear(
        (w): Linear(in_features=768, out_features=5000, bias=True)
      )
    )
  )
) loading from /scratch/asasin/IST-ASR/speechbrain/transformer/results_lm_060/results_fold_3/conformer_small/7775/save/lm.ckpt, the object could not use the parameters loaded with the key: out.w.bias
2025-03-21 20:28:09,116 - speechbrain.core - INFO - Info: precision arg from hparam file is used
2025-03-21 20:28:09,116 - speechbrain.core - INFO - Info: max_grad_norm arg from hparam file is used
2025-03-21 20:28:09,116 - speechbrain.core - INFO - Info: grad_accumulation_factor arg from hparam file is used
2025-03-21 20:28:09,313 - speechbrain.core - INFO - Gradscaler enabled: True. Using precision: fp16.
2025-03-21 20:28:09,315 - speechbrain.core - INFO - ASR Model Statistics:
* Total Number of Trainable Parameters: 13.3M
* Total Number of Parameters: 13.3M
* Trainable Parameters represent 100.0000% of the total size.
2025-03-21 20:28:09,358 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-11-23+00
2025-03-21 20:28:09,622 - speechbrain.utils.epoch_loop - INFO - Going into epoch 60
2025-03-21 20:30:43,998 - speechbrain.utils.train_logger - INFO - epoch: 60, lr: 4.94e-04, steps: 6180, optimizer: Adam - train loss: 6.50 - valid loss: 8.27, valid ACC: 8.47e-01, valid WER: 60.60
2025-03-21 20:30:44,417 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-30-43+00
2025-03-21 20:30:44,459 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+19-59-37+00
2025-03-21 20:30:44,460 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-01-56+00
2025-03-21 20:30:44,461 - speechbrain.utils.epoch_loop - INFO - Going into epoch 61
2025-03-21 20:31:49,320 - speechbrain.utils.train_logger - INFO - epoch: 61, lr: 5.03e-04, steps: 6283, optimizer: Adam - train loss: 6.25 - valid loss: 8.59, valid ACC: 8.49e-01
2025-03-21 20:31:49,559 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-31-49+00
2025-03-21 20:31:49,610 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-02-59+00
2025-03-21 20:31:49,610 - speechbrain.utils.epoch_loop - INFO - Going into epoch 62
2025-03-21 20:32:51,341 - speechbrain.utils.train_logger - INFO - epoch: 62, lr: 5.11e-04, steps: 6386, optimizer: Adam - train loss: 6.06 - valid loss: 7.96, valid ACC: 8.52e-01
2025-03-21 20:32:51,563 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-32-51+00
2025-03-21 20:32:51,628 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-04-02+00
2025-03-21 20:32:51,628 - speechbrain.utils.epoch_loop - INFO - Going into epoch 63
2025-03-21 20:33:53,529 - speechbrain.utils.train_logger - INFO - epoch: 63, lr: 5.19e-04, steps: 6489, optimizer: Adam - train loss: 5.81 - valid loss: 7.40, valid ACC: 8.58e-01
2025-03-21 20:33:53,764 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-33-53+00
2025-03-21 20:33:53,854 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-05-05+00
2025-03-21 20:33:53,855 - speechbrain.utils.epoch_loop - INFO - Going into epoch 64
2025-03-21 20:34:56,226 - speechbrain.utils.train_logger - INFO - epoch: 64, lr: 5.27e-04, steps: 6592, optimizer: Adam - train loss: 5.59 - valid loss: 7.53, valid ACC: 8.59e-01
2025-03-21 20:34:56,445 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-34-56+00
2025-03-21 20:34:56,540 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-06-08+00
2025-03-21 20:34:56,541 - speechbrain.utils.epoch_loop - INFO - Going into epoch 65
2025-03-21 20:35:58,380 - speechbrain.utils.train_logger - INFO - epoch: 65, lr: 5.36e-04, steps: 6695, optimizer: Adam - train loss: 5.47 - valid loss: 7.32, valid ACC: 8.62e-01
2025-03-21 20:35:58,686 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-35-58+00
2025-03-21 20:35:58,802 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-07-11+00
2025-03-21 20:35:58,802 - speechbrain.utils.epoch_loop - INFO - Going into epoch 66
2025-03-21 20:37:01,045 - speechbrain.utils.train_logger - INFO - epoch: 66, lr: 5.44e-04, steps: 6798, optimizer: Adam - train loss: 5.31 - valid loss: 7.89, valid ACC: 8.51e-01
2025-03-21 20:37:01,345 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-37-01+00
2025-03-21 20:37:01,494 - speechbrain.utils.epoch_loop - INFO - Going into epoch 67
2025-03-21 20:38:03,933 - speechbrain.utils.train_logger - INFO - epoch: 67, lr: 5.52e-04, steps: 6901, optimizer: Adam - train loss: 5.13 - valid loss: 7.11, valid ACC: 8.68e-01
2025-03-21 20:38:04,186 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-38-03+00
2025-03-21 20:38:04,364 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-09-17+00
2025-03-21 20:38:04,364 - speechbrain.utils.epoch_loop - INFO - Going into epoch 68
2025-03-21 20:39:06,530 - speechbrain.utils.train_logger - INFO - epoch: 68, lr: 5.60e-04, steps: 7004, optimizer: Adam - train loss: 5.02 - valid loss: 7.63, valid ACC: 8.56e-01
2025-03-21 20:39:07,231 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-39-06+00
2025-03-21 20:39:07,459 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-10-20+00
2025-03-21 20:39:07,460 - speechbrain.utils.epoch_loop - INFO - Going into epoch 69
2025-03-21 20:40:10,745 - speechbrain.utils.train_logger - INFO - epoch: 69, lr: 5.68e-04, steps: 7107, optimizer: Adam - train loss: 4.87 - valid loss: 7.12, valid ACC: 8.68e-01
2025-03-21 20:40:11,351 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-40-10+00
2025-03-21 20:40:11,573 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-08-14+00
2025-03-21 20:40:11,575 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-11-23+00
2025-03-21 20:40:11,575 - speechbrain.utils.epoch_loop - INFO - Going into epoch 70
2025-03-21 20:42:29,520 - speechbrain.utils.train_logger - INFO - epoch: 70, lr: 5.77e-04, steps: 7210, optimizer: Adam - train loss: 4.75 - valid loss: 6.95, valid ACC: 8.68e-01, valid WER: 43.21
2025-03-21 20:42:29,947 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-42-29+00
2025-03-21 20:42:30,194 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-30-43+00
2025-03-21 20:42:30,195 - speechbrain.utils.epoch_loop - INFO - Going into epoch 71
2025-03-21 20:43:34,018 - speechbrain.utils.train_logger - INFO - epoch: 71, lr: 5.85e-04, steps: 7313, optimizer: Adam - train loss: 4.55 - valid loss: 6.86, valid ACC: 8.68e-01
2025-03-21 20:43:34,541 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-43-34+00
2025-03-21 20:43:34,813 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-31-49+00
2025-03-21 20:43:34,814 - speechbrain.utils.epoch_loop - INFO - Going into epoch 72
2025-03-21 20:44:38,801 - speechbrain.utils.train_logger - INFO - epoch: 72, lr: 5.93e-04, steps: 7416, optimizer: Adam - train loss: 4.45 - valid loss: 7.25, valid ACC: 8.69e-01
2025-03-21 20:44:39,359 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-44-38+00
2025-03-21 20:44:39,610 - speechbrain.utils.epoch_loop - INFO - Going into epoch 73
2025-03-21 20:45:41,932 - speechbrain.utils.train_logger - INFO - epoch: 73, lr: 6.01e-04, steps: 7519, optimizer: Adam - train loss: 4.39 - valid loss: 7.04, valid ACC: 8.63e-01
2025-03-21 20:45:42,478 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-45-41+00
2025-03-21 20:45:42,790 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-32-51+00
2025-03-21 20:45:42,790 - speechbrain.utils.epoch_loop - INFO - Going into epoch 74
2025-03-21 20:46:45,683 - speechbrain.utils.train_logger - INFO - epoch: 74, lr: 6.10e-04, steps: 7622, optimizer: Adam - train loss: 4.32 - valid loss: 7.52, valid ACC: 8.63e-01
2025-03-21 20:46:46,148 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-46-45+00
2025-03-21 20:46:46,493 - speechbrain.utils.epoch_loop - INFO - Going into epoch 75
2025-03-21 20:47:50,305 - speechbrain.utils.train_logger - INFO - epoch: 75, lr: 6.18e-04, steps: 7725, optimizer: Adam - train loss: 4.11 - valid loss: 6.55, valid ACC: 8.75e-01
2025-03-21 20:47:50,939 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-47-50+00
2025-03-21 20:47:51,290 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-33-53+00
2025-03-21 20:47:51,290 - speechbrain.utils.epoch_loop - INFO - Going into epoch 76
2025-03-21 20:48:54,930 - speechbrain.utils.train_logger - INFO - epoch: 76, lr: 6.26e-04, steps: 7828, optimizer: Adam - train loss: 4.07 - valid loss: 6.61, valid ACC: 8.70e-01
2025-03-21 20:48:55,410 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-48-54+00
2025-03-21 20:48:55,778 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-37-01+00
2025-03-21 20:48:55,779 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-34-56+00
2025-03-21 20:48:55,779 - speechbrain.utils.epoch_loop - INFO - Going into epoch 77
2025-03-21 20:49:57,746 - speechbrain.utils.train_logger - INFO - epoch: 77, lr: 6.34e-04, steps: 7931, optimizer: Adam - train loss: 4.03 - valid loss: 7.22, valid ACC: 8.70e-01
2025-03-21 20:49:58,301 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-49-57+00
2025-03-21 20:49:58,655 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-35-58+00
2025-03-21 20:49:58,656 - speechbrain.utils.epoch_loop - INFO - Going into epoch 78
2025-03-21 20:51:02,531 - speechbrain.utils.train_logger - INFO - epoch: 78, lr: 6.43e-04, steps: 8034, optimizer: Adam - train loss: 3.88 - valid loss: 6.01, valid ACC: 8.78e-01
2025-03-21 20:51:03,155 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-51-02+00
2025-03-21 20:51:03,582 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-39-06+00
2025-03-21 20:51:03,582 - speechbrain.utils.epoch_loop - INFO - Going into epoch 79
2025-03-21 20:52:05,387 - speechbrain.utils.train_logger - INFO - epoch: 79, lr: 6.51e-04, steps: 8137, optimizer: Adam - train loss: 3.95 - valid loss: 6.08, valid ACC: 8.81e-01
2025-03-21 20:52:05,985 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-52-05+00
2025-03-21 20:52:06,778 - speechbrain.utils.epoch_loop - INFO - Going into epoch 80
2025-03-21 20:54:23,918 - speechbrain.utils.train_logger - INFO - epoch: 80, lr: 6.59e-04, steps: 8240, optimizer: Adam - train loss: 3.74 - valid loss: 6.30, valid ACC: 8.81e-01, valid WER: 37.62
2025-03-21 20:54:24,422 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-54-23+00
2025-03-21 20:54:24,993 - speechbrain.utils.checkpoints - INFO - Loading a checkpoint from results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-54-23+00
2025-03-21 20:54:25,070 - speechbrain.dataio.dataloader - DEBUG - SaveableDataLoader was requested to load a checkpoint, but the DataLoader has already been iterated. The DataLoader file will be ignored. This is normal in evaluation, when a checkpoint is loaded just to retrieve the best model.
2025-03-21 21:01:52,328 - speechbrain.utils.train_logger - INFO - Epoch loaded: 80 - test loss: 5.67, test ACC: 8.91e-01, test WER: 34.71
2025-03-21 21:01:52,937 - speechbrain.utils.checkpoints - INFO - Saved an end-of-epoch checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+21-01-52+00
2025-03-21 21:01:53,444 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-48-54+00
2025-03-21 21:01:53,445 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-49-57+00
2025-03-21 21:01:53,446 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-45-41+00
2025-03-21 21:01:53,448 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-44-38+00
2025-03-21 21:01:53,449 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-42-29+00
2025-03-21 21:01:53,450 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-46-45+00
2025-03-21 21:01:53,452 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-51-02+00
2025-03-21 21:01:53,453 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-52-05+00
2025-03-21 21:01:53,454 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-54-23+00
2025-03-21 21:01:53,455 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-43-34+00
2025-03-21 21:01:53,457 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-38-03+00
2025-03-21 21:01:53,461 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-40-10+00
2025-03-21 21:01:53,467 - speechbrain.utils.checkpoints - INFO - Deleted checkpoint in results_lm_060/results_fold_3/conformer_small/7775/save/CKPT+2025-03-21+20-47-50+00
